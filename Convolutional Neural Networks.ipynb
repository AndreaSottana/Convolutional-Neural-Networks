{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "__Prerequisites__\n",
    "\n",
    "- [Neural Networks](https://github.com/AI-Core/Neural-Networks/blob/master/Neural%20Networks.ipynb)\n",
    "\n",
    "## What's wrong with how neural networks process images?\n",
    "\n",
    "The fully connected neural network we looked at in the previous lesson takes in a vector as input. So we flattened our images by stacking the rows so that it could be passed in as input and used for classification problems successfully. \n",
    "\n",
    "#### Spacially structured data\n",
    "\n",
    "For some problems, the order of the features in each example does not matter (e.g. age, height, hair length). But this isn't the case for images. If we randomly reorder the pixels in an image, then it will likely be unrecognisable. Most of the useful information in images comes not from the values of the features (pixels), but from their relative positions. The same is true for processing any other **spacially structured** data such as videos, soundwaves, medical scans, 3D-models etc. \n",
    "\n",
    "The spatial relationships between the different pixels is information that is crucial to our understanding of an image. When we flatten the image, we lose this information.\n",
    "\n",
    "#### Weight sharing across space\n",
    "\n",
    "Regardless of where I see something interesting in my field of view, it can often be processed in the same way. \n",
    "\n",
    "Neural networks have individual weights for each input feature because they expect each feature to represent a totally different thing (e.g. age, height, weight). In other domains like computer vision however, different features (pixels) can represent the same thing just in different locations (e.g. money on my left, money on my right).\n",
    "\n",
    "Instead of learning to look for the same features of an image with different weights for each position that that feature might be in, we should try to share the same learnt weights over all positions of the input. This will save us both time and memory in computing and storing these duplicate weights. \n",
    "\n",
    "#### So what\n",
    "\n",
    "Using our prior understanding of how image data should be processed spacially, we'd like to find some kind of model that can retain the spacial structure of an input, and look for the same features over the whole of this space. This is what we will use convolutional neural networks for.\n",
    "\n",
    "## Images as data\n",
    "\n",
    "Images are not naturally vectors. They obviously have a height and a width rather than just a length - so they need to at least be a matrix. \n",
    "\n",
    "#### Channels\n",
    "\n",
    "Any non-black color can be made by combining 3 primary colors.\n",
    "As such, as well as height and width, color images have another axis called the **channels**, which specifies the intensity (contribution) of each of these primary colors.\n",
    "Red, green and blue are the (arbitrary) standard primary colors. \n",
    "So most images that we will work with have a red channel, a green channel and a blue channel.\n",
    "This is illustrated below.\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "Some images can also have transparent backgrounds, in which case they might have a fourth channel to represent the opacity at each pixel location.\n",
    "\n",
    "## How was computer vision done before deep learning?\n",
    "\n",
    "In the past, people would try to draw patterns that they thought would appear in images and be useful for the problem that they were trying to solve. This was a painstakingly long process, and was obviously susceptible to a lot of bias by these feature designers.\n",
    "\n",
    "## Filters/Kernels\n",
    "These supposedly useful patterns mentioned above are known as **filters** or **kernels**. \n",
    "Each filter looks for a particular pattern.\n",
    "E.g. a filter that looks for circles would have high values in a circle and low values in other locations.\n",
    "\n",
    "![title](images/kernels.jpg)\n",
    "\n",
    "Filters *look* for the patterns they represent by seeing how similar the pixels at any particular location match the values that they contain. A mathematically convenient way to do this is by taking a **dot product** between the filter's values and the input values which it covers - an element wise multiplication and sum of the results. **This produces a single value** which should be larger when the input better matches the feature that the filter looks for.\n",
    "\n",
    "It is standard for filters to always convolve through the full depth of the input. So if we have an input with 3 channels (e.g. a color image), our kernel will also have a depth of 3 - where each channel of the filter is what it looks for from that corresponding color channel. If our input has 54 channels, then so will our filter. \n",
    "\n",
    "The width and height of our kernels is up to us (they are hyperparameters). It's standard to have kernels with equal width and height.\n",
    "\n",
    "## The convolution operation\n",
    "\n",
    "In machine learning, convolution is the proccess of moving a filter across every possible position of the input and computing a value for how well it is matched at each location. \n",
    "\n",
    "This pattern matching over the spacially structured input produces a similar spacially structured output. We call this output an **activation map** or a **feature map** because it represents the activations in the next layer that should represent some higher level (more complex) features than the feature maps in the input.\n",
    "\n",
    "The animation below shows how a 1x3x3 filter is applied to a 1x5x5 image (for simplicity, input channels = 1). \n",
    "On the left is the filter that we will convolve over the input. In the centre is the input being convolved over. On the right is the output activation map produced by convolving this filter over this input.\n",
    "\n",
    "Notice how the output has high  values when the filter is passed over locations where there is an X shape in the input image. This is because the values of the filter are such that it is performing pattern matching for the X shape.\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "The convolution operation has a few possible parameters:\n",
    "\n",
    "### Stride\n",
    "The stride is the number of pixels we shift our kernel along by to compute the next value in the output activation map. Increased stride means less values are computed for the output activation map, which means that we have to do less computation and store less output information, decreasing computing time and cost and reducing memory requirements but reducing output resolution.\n",
    "\n",
    "### Padding\n",
    "We can *pad* the input with a border of extra pixels around the edge. Why might we want to do this?\n",
    "\n",
    "##### Model depth limitations\n",
    "\n",
    "When we use a kernel size larger than one, each single output value is a function of many input values (all the pixels which the filter covers). This means that the size of the convolution output is smaller than the input. As such, there is a limit to the number of successive convolutions that we can apply because eventually the input gets so small that there is only one location of the input that the filter can be placed on the input and the output will then have a height and width of 1 and cannot be convolved over (convolution with a 1x1 filter is equivalent to multiplication).\n",
    "\n",
    "##### Equal input from each pixel\n",
    "\n",
    "When we use a kernel size larger than one, the corner pixels will only contribute to a single output value because they only enter the kernel at it's very extreme positions. As such they contribute less to the final predictions than the other pixels. The same is true for pixels near the edge, but to a lesser extent.\n",
    "\n",
    "#### Different padding modes\n",
    "\n",
    "We can use different \"padding modes\" to specify what we pad the image with. Options include padding it with zeros, continuing the last color outwards, reflecting the inwards colors. See options provided by PyTorch [here](\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "For convolution, each computed value in the output feature map is a linear function of the pixels in a local region of the input as opposed to fully connected nets where each computed feature is a linear function of all the values in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional layer\n",
    "\n",
    "In practice, we want to look for more than just one feature in any input. When we used a neural network, each layer had multiple outputs corresponding to different learnt features. Similarly, instead of convolving just a single filter over the input to produce a single activation map, we convolve many filters over the input to produce many activation maps. This produces a stack of activation maps as the output. The output then has an extra dimension, in addition to the spacial ones, which corresponds to which output activation map you're looking at. This dimension is the convolutional analogy to the number of outputs from a linear layer.\n",
    "\n",
    "Also just like linear layers, convolutional layers apply a simple linear transformation to their input and can be applied successively with activation functions to represent very complex non-linear transformations. Models with such layers are **convolutional neural networks**. These are appropriate for tackling problems like object detection and image segmentation. These convolutional layers have values for each weight within each filter and also include biases to shift each output feature. \n",
    "Just like before, each successive layer in the network learns successively higher level abstract features from the inputs.\n",
    "\n",
    "These convolutional layers are also provided by PyTorch. In this notebook we will use `torch.nn.Conv2D` to convolve over our input in 2 directions (width and height).\n",
    "\n",
    "![image](images/CNN_FNN_comparison.JPG)\n",
    "\n",
    "## What does each filter look for?\n",
    "Engineers used to have to tune filter values manually. Now, just like the weights and biases in linear layers of neural networks, they can be learnt automatically by backpropagation and gradient descent.\n",
    "\n",
    "## Pooling layers\n",
    "Immediately after a convolutional layer, it is common to apply some form of **pooling**. Pooling is a technique that summarises/downsamples the values in a local region of its input. This reduces the number of values in its output, therefore reducing the number of parameters that need to be learned for a succeeding parameterised operation such as a further convolutional or linear layer.\n",
    "\n",
    "Because pooling summarises values in local spacial regions it can help models to be robust under translation of the input, making them more **translation invariant**.\n",
    "\n",
    "Pooling layers also slide kernels over their input, and reduce the values within that grid location to a single value. But they perform different operations than a linear combination like in convolution (see below).\n",
    "\n",
    "**Max pooling** replaces the values at each grid location with their maximum.\n",
    "\n",
    "**Average Pooling** replaces the values at each grid location with their average.\n",
    "\n",
    "See the PyTorch [docs](https://pytorch.org/docs/stable/nn.html#pooling-layers) for more pooling layers\n",
    "\n",
    "## The output of convolutional neural networks\n",
    "\n",
    "Unless we keep applying convolutional layers to our data until it is reduced to a height and width of 1, the output will still retain some spacial dimensions. This means that as well as our input, our output can also be an image for example. This can be useful for problems such as image segmentation, where the output is a pixelwise classification mask of everything in the scene. In this case the output is the same shape as the input image, but with each pixel location taking the value of a class label (e.g. all pixels of cars in the image have value=1, all roads have value=2 etc).\n",
    "\n",
    "In our case though, we want to perform image classification for 10 classes. It is common practice to flatten the output of the convolutional layers of a network into a vector, and then transform them into a vector of the desired output shape by applying a final linear layer. This is what we do below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement a convolutional neural network\n",
    "\n",
    "The first cell is just the same boilerplate we've used before. Make sure you understand it and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter  # displays our loss curve as we go\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# PIL is Python Image Library, which we need to transform into a Tensor\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1725, 0.4118, 0.4118,\n",
      "         0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.3412, 0.8196, 0.8980, 0.9922, 0.9922,\n",
      "         0.9098, 0.8196, 0.8196, 0.8196, 0.2667, 0.0549, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0078, 0.0588, 0.7373, 0.9922, 0.9922, 0.9961, 0.9922, 0.9922,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.2314, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.4000, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961, 0.9922, 0.7137,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.2314, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7804, 0.9608, 0.9922, 0.9922, 0.9412, 0.3529, 0.1137, 0.1137, 0.0980,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.5059, 0.0275, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4314, 0.9569, 0.9294, 0.6667, 0.2392, 0.0000, 0.0000, 0.2392, 0.7216,\n",
      "         0.9922, 0.9922, 0.9765, 0.5608, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.3333, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.8196, 0.9922,\n",
      "         0.9922, 0.8353, 0.4118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.5961, 0.9843, 0.9922,\n",
      "         0.8235, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0627, 0.2824, 0.8980, 0.9961, 0.9490, 0.7804,\n",
      "         0.1216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6980, 0.9922, 0.9922, 0.9961, 0.4118, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0510, 0.7216, 0.9961, 0.9961, 0.7529, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3843, 0.8431, 0.9922, 0.9098, 0.3098, 0.0706, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.3412,\n",
      "         0.9529, 0.9922, 0.9608, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3412, 0.9922,\n",
      "         0.9922, 0.9373, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3843, 0.9529, 0.9922,\n",
      "         0.7294, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0353, 0.2941, 0.2941, 0.2941, 0.2941, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9412, 0.9922, 0.9922,\n",
      "         0.2196, 0.0510, 0.1176, 0.1176, 0.1176, 0.1176, 0.1216, 0.1176, 0.1176,\n",
      "         0.1176, 0.6392, 0.7373, 0.9922, 0.9922, 0.9922, 0.9216, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7216, 0.9922, 0.9922, 0.9922,\n",
      "         0.9922, 0.7176, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961, 0.9922, 0.9922,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.5843, 0.1922, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9922, 0.9922, 0.9922,\n",
      "         0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922, 0.9922,\n",
      "         0.9922, 0.8863, 0.6392, 0.3294, 0.0549, 0.0157, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.9922, 0.9922,\n",
      "         0.9922, 0.9922, 0.9922, 0.9294, 0.7098, 0.5765, 0.8196, 0.8157, 0.7804,\n",
      "         0.2314, 0.1647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.4078, 0.4078, 0.4078,\n",
      "         0.4078, 0.4078, 0.4078, 0.2627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "example = train_data[np.random.randint(0, 300)][0]    # get a random example\n",
    "x = example[0]\n",
    "print(x) # the data\n",
    "#y = example[1]\n",
    "#print(y) # the label\n",
    "plt.imshow(x.numpy(),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# The Data Loaders are batching the data for us and are also shuffling them.\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential\n",
    "            # torch.nn.Sequential takes a tuple of callable objects \n",
    "            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1),  # choosing 64 kernels\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3),\n",
    "            torch.nn.ReLU()\n",
    "            \n",
    "            # first conv layer\n",
    "            # activation function\n",
    "            # second conv layer\n",
    "            # activation function\n",
    "        )\n",
    "\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "             # put your linear architecture here using torch.nn.Sequential \n",
    "            torch.nn.Linear(73728, 32),\n",
    "            torch.nn.Linear(32, 10)  # inputs is output of last convolution flattened, 10 output classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(-1, np.prod(x.shape[1:]))# flatten output ready for fully connected layer\n",
    "        # x.shape[1:] takes all dimensions but the first one\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x) # softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_cnn = ConvNet() # this is callable\n",
    "# my_cnn(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "\n",
    "cnn = ConvNet().to(device) #.to(device)#instantiate model\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: 2.3017172813415527\n",
      "Epoch: 0 \tBatch: 1 \tLoss: 2.3295199871063232\n",
      "Epoch: 0 \tBatch: 2 \tLoss: 2.2656445503234863\n",
      "Epoch: 0 \tBatch: 3 \tLoss: 2.3148691654205322\n",
      "Epoch: 0 \tBatch: 4 \tLoss: 2.2669339179992676\n",
      "Epoch: 0 \tBatch: 5 \tLoss: 2.2320775985717773\n",
      "Epoch: 0 \tBatch: 6 \tLoss: 2.2287752628326416\n",
      "Epoch: 0 \tBatch: 7 \tLoss: 2.2080023288726807\n",
      "Epoch: 0 \tBatch: 8 \tLoss: 2.0927157402038574\n",
      "Epoch: 0 \tBatch: 9 \tLoss: 2.0747525691986084\n",
      "Epoch: 0 \tBatch: 10 \tLoss: 1.9890869855880737\n",
      "Epoch: 0 \tBatch: 11 \tLoss: 1.9745988845825195\n",
      "Epoch: 0 \tBatch: 12 \tLoss: 1.9161632061004639\n",
      "Epoch: 0 \tBatch: 13 \tLoss: 1.922075629234314\n",
      "Epoch: 0 \tBatch: 14 \tLoss: 1.8465347290039062\n",
      "Epoch: 0 \tBatch: 15 \tLoss: 1.8804634809494019\n",
      "Epoch: 0 \tBatch: 16 \tLoss: 1.8911025524139404\n",
      "Epoch: 0 \tBatch: 17 \tLoss: 1.8284443616867065\n",
      "Epoch: 0 \tBatch: 18 \tLoss: 1.913866400718689\n",
      "Epoch: 0 \tBatch: 19 \tLoss: 1.8223391771316528\n",
      "Epoch: 0 \tBatch: 20 \tLoss: 1.8104779720306396\n",
      "Epoch: 0 \tBatch: 21 \tLoss: 1.762607216835022\n",
      "Epoch: 0 \tBatch: 22 \tLoss: 1.7699779272079468\n",
      "Epoch: 0 \tBatch: 23 \tLoss: 1.766541600227356\n",
      "Epoch: 0 \tBatch: 24 \tLoss: 1.743332028388977\n",
      "Epoch: 0 \tBatch: 25 \tLoss: 1.7996777296066284\n",
      "Epoch: 0 \tBatch: 26 \tLoss: 1.7767612934112549\n",
      "Epoch: 0 \tBatch: 27 \tLoss: 1.8556619882583618\n",
      "Epoch: 0 \tBatch: 28 \tLoss: 1.8449411392211914\n",
      "Epoch: 0 \tBatch: 29 \tLoss: 1.6849486827850342\n",
      "Epoch: 0 \tBatch: 30 \tLoss: 1.7600722312927246\n",
      "Epoch: 0 \tBatch: 31 \tLoss: 1.7927336692810059\n",
      "Epoch: 0 \tBatch: 32 \tLoss: 1.7533743381500244\n",
      "Epoch: 0 \tBatch: 33 \tLoss: 1.7972760200500488\n",
      "Epoch: 0 \tBatch: 34 \tLoss: 1.7757607698440552\n",
      "Epoch: 0 \tBatch: 35 \tLoss: 1.7466078996658325\n",
      "Epoch: 0 \tBatch: 36 \tLoss: 1.7589476108551025\n",
      "Epoch: 0 \tBatch: 37 \tLoss: 1.7271413803100586\n",
      "Epoch: 0 \tBatch: 38 \tLoss: 1.7764735221862793\n",
      "Epoch: 0 \tBatch: 39 \tLoss: 1.7794849872589111\n",
      "Epoch: 0 \tBatch: 40 \tLoss: 1.700382947921753\n",
      "Epoch: 0 \tBatch: 41 \tLoss: 1.7806764841079712\n",
      "Epoch: 0 \tBatch: 42 \tLoss: 1.7584172487258911\n",
      "Epoch: 0 \tBatch: 43 \tLoss: 1.7996159791946411\n",
      "Epoch: 0 \tBatch: 44 \tLoss: 1.7248750925064087\n",
      "Epoch: 0 \tBatch: 45 \tLoss: 1.7069246768951416\n",
      "Epoch: 0 \tBatch: 46 \tLoss: 1.6697661876678467\n",
      "Epoch: 0 \tBatch: 47 \tLoss: 1.7176822423934937\n",
      "Epoch: 0 \tBatch: 48 \tLoss: 1.7221593856811523\n",
      "Epoch: 0 \tBatch: 49 \tLoss: 1.6986238956451416\n",
      "Epoch: 0 \tBatch: 50 \tLoss: 1.704654335975647\n",
      "Epoch: 0 \tBatch: 51 \tLoss: 1.6806279420852661\n",
      "Epoch: 0 \tBatch: 52 \tLoss: 1.772202491760254\n",
      "Epoch: 0 \tBatch: 53 \tLoss: 1.7677252292633057\n",
      "Epoch: 0 \tBatch: 54 \tLoss: 1.7726117372512817\n",
      "Epoch: 0 \tBatch: 55 \tLoss: 1.7697964906692505\n",
      "Epoch: 0 \tBatch: 56 \tLoss: 1.758562445640564\n",
      "Epoch: 0 \tBatch: 57 \tLoss: 1.6850780248641968\n",
      "Epoch: 0 \tBatch: 58 \tLoss: 1.7362738847732544\n",
      "Epoch: 0 \tBatch: 59 \tLoss: 1.7485301494598389\n",
      "Epoch: 0 \tBatch: 60 \tLoss: 1.715329647064209\n",
      "Epoch: 0 \tBatch: 61 \tLoss: 1.694687843322754\n",
      "Epoch: 0 \tBatch: 62 \tLoss: 1.8065565824508667\n",
      "Epoch: 0 \tBatch: 63 \tLoss: 1.7170411348342896\n",
      "Epoch: 0 \tBatch: 64 \tLoss: 1.7424414157867432\n",
      "Epoch: 0 \tBatch: 65 \tLoss: 1.7237439155578613\n",
      "Epoch: 0 \tBatch: 66 \tLoss: 1.718543291091919\n",
      "Epoch: 0 \tBatch: 67 \tLoss: 1.717886209487915\n",
      "Epoch: 0 \tBatch: 68 \tLoss: 1.6754270792007446\n",
      "Epoch: 0 \tBatch: 69 \tLoss: 1.7728317975997925\n",
      "Epoch: 0 \tBatch: 70 \tLoss: 1.6884799003601074\n",
      "Epoch: 0 \tBatch: 71 \tLoss: 1.7630189657211304\n",
      "Epoch: 0 \tBatch: 72 \tLoss: 1.7687370777130127\n",
      "Epoch: 0 \tBatch: 73 \tLoss: 1.661242961883545\n",
      "Epoch: 0 \tBatch: 74 \tLoss: 1.7666476964950562\n",
      "Epoch: 0 \tBatch: 75 \tLoss: 1.6968153715133667\n",
      "Epoch: 0 \tBatch: 76 \tLoss: 1.6615707874298096\n",
      "Epoch: 0 \tBatch: 77 \tLoss: 1.679425835609436\n",
      "Epoch: 0 \tBatch: 78 \tLoss: 1.7662755250930786\n",
      "Epoch: 0 \tBatch: 79 \tLoss: 1.782528281211853\n",
      "Epoch: 0 \tBatch: 80 \tLoss: 1.7471020221710205\n",
      "Epoch: 0 \tBatch: 81 \tLoss: 1.7151031494140625\n",
      "Epoch: 0 \tBatch: 82 \tLoss: 1.7003304958343506\n",
      "Epoch: 0 \tBatch: 83 \tLoss: 1.7111726999282837\n",
      "Epoch: 0 \tBatch: 84 \tLoss: 1.7724146842956543\n",
      "Epoch: 0 \tBatch: 85 \tLoss: 1.7828946113586426\n",
      "Epoch: 0 \tBatch: 86 \tLoss: 1.71431303024292\n",
      "Epoch: 0 \tBatch: 87 \tLoss: 1.6804893016815186\n",
      "Epoch: 0 \tBatch: 88 \tLoss: 1.7991888523101807\n",
      "Epoch: 0 \tBatch: 89 \tLoss: 1.7199572324752808\n",
      "Epoch: 0 \tBatch: 90 \tLoss: 1.7059149742126465\n",
      "Epoch: 0 \tBatch: 91 \tLoss: 1.7439409494400024\n",
      "Epoch: 0 \tBatch: 92 \tLoss: 1.742709994316101\n",
      "Epoch: 0 \tBatch: 93 \tLoss: 1.6856884956359863\n",
      "Epoch: 0 \tBatch: 94 \tLoss: 1.7202316522598267\n",
      "Epoch: 0 \tBatch: 95 \tLoss: 1.7322139739990234\n",
      "Epoch: 0 \tBatch: 96 \tLoss: 1.687326431274414\n",
      "Epoch: 0 \tBatch: 97 \tLoss: 1.6836901903152466\n",
      "Epoch: 0 \tBatch: 98 \tLoss: 1.6897107362747192\n",
      "Epoch: 0 \tBatch: 99 \tLoss: 1.6649680137634277\n",
      "Epoch: 0 \tBatch: 100 \tLoss: 1.6318565607070923\n",
      "Epoch: 0 \tBatch: 101 \tLoss: 1.638874888420105\n",
      "Epoch: 0 \tBatch: 102 \tLoss: 1.646992564201355\n",
      "Epoch: 0 \tBatch: 103 \tLoss: 1.6512004137039185\n",
      "Epoch: 0 \tBatch: 104 \tLoss: 1.6701419353485107\n",
      "Epoch: 0 \tBatch: 105 \tLoss: 1.6079384088516235\n",
      "Epoch: 0 \tBatch: 106 \tLoss: 1.6631842851638794\n",
      "Epoch: 0 \tBatch: 107 \tLoss: 1.6096646785736084\n",
      "Epoch: 0 \tBatch: 108 \tLoss: 1.607123851776123\n",
      "Epoch: 0 \tBatch: 109 \tLoss: 1.6265811920166016\n",
      "Epoch: 0 \tBatch: 110 \tLoss: 1.6250265836715698\n",
      "Epoch: 0 \tBatch: 111 \tLoss: 1.6597312688827515\n",
      "Epoch: 0 \tBatch: 112 \tLoss: 1.6265299320220947\n",
      "Epoch: 0 \tBatch: 113 \tLoss: 1.6360533237457275\n",
      "Epoch: 0 \tBatch: 114 \tLoss: 1.654760718345642\n",
      "Epoch: 0 \tBatch: 115 \tLoss: 1.647566556930542\n",
      "Epoch: 0 \tBatch: 116 \tLoss: 1.6410809755325317\n",
      "Epoch: 0 \tBatch: 117 \tLoss: 1.635153889656067\n",
      "Epoch: 0 \tBatch: 118 \tLoss: 1.6869393587112427\n",
      "Epoch: 0 \tBatch: 119 \tLoss: 1.6745636463165283\n",
      "Epoch: 0 \tBatch: 120 \tLoss: 1.609712839126587\n",
      "Epoch: 0 \tBatch: 121 \tLoss: 1.6674115657806396\n",
      "Epoch: 0 \tBatch: 122 \tLoss: 1.5796235799789429\n",
      "Epoch: 0 \tBatch: 123 \tLoss: 1.5870048999786377\n",
      "Epoch: 0 \tBatch: 124 \tLoss: 1.619039535522461\n",
      "Epoch: 0 \tBatch: 125 \tLoss: 1.6193922758102417\n",
      "Epoch: 0 \tBatch: 126 \tLoss: 1.6761411428451538\n",
      "Epoch: 0 \tBatch: 127 \tLoss: 1.6131861209869385\n",
      "Epoch: 0 \tBatch: 128 \tLoss: 1.6385117769241333\n",
      "Epoch: 0 \tBatch: 129 \tLoss: 1.6038649082183838\n",
      "Epoch: 0 \tBatch: 130 \tLoss: 1.5921473503112793\n",
      "Epoch: 0 \tBatch: 131 \tLoss: 1.6075564622879028\n",
      "Epoch: 0 \tBatch: 132 \tLoss: 1.5660817623138428\n",
      "Epoch: 0 \tBatch: 133 \tLoss: 1.6301519870758057\n",
      "Epoch: 0 \tBatch: 134 \tLoss: 1.5831282138824463\n",
      "Epoch: 0 \tBatch: 135 \tLoss: 1.6104791164398193\n",
      "Epoch: 0 \tBatch: 136 \tLoss: 1.6508512496948242\n",
      "Epoch: 0 \tBatch: 137 \tLoss: 1.6280701160430908\n",
      "Epoch: 0 \tBatch: 138 \tLoss: 1.6707242727279663\n",
      "Epoch: 0 \tBatch: 139 \tLoss: 1.659968376159668\n",
      "Epoch: 0 \tBatch: 140 \tLoss: 1.6433407068252563\n",
      "Epoch: 0 \tBatch: 141 \tLoss: 1.6404263973236084\n",
      "Epoch: 0 \tBatch: 142 \tLoss: 1.6040040254592896\n",
      "Epoch: 0 \tBatch: 143 \tLoss: 1.5772604942321777\n",
      "Epoch: 0 \tBatch: 144 \tLoss: 1.6179426908493042\n",
      "Epoch: 0 \tBatch: 145 \tLoss: 1.6599661111831665\n",
      "Epoch: 0 \tBatch: 146 \tLoss: 1.5491015911102295\n",
      "Epoch: 0 \tBatch: 147 \tLoss: 1.5911341905593872\n",
      "Epoch: 0 \tBatch: 148 \tLoss: 1.5879888534545898\n",
      "Epoch: 0 \tBatch: 149 \tLoss: 1.5998547077178955\n",
      "Epoch: 0 \tBatch: 150 \tLoss: 1.5983240604400635\n",
      "Epoch: 0 \tBatch: 151 \tLoss: 1.6047558784484863\n",
      "Epoch: 0 \tBatch: 152 \tLoss: 1.6242525577545166\n",
      "Epoch: 0 \tBatch: 153 \tLoss: 1.6643179655075073\n",
      "Epoch: 0 \tBatch: 154 \tLoss: 1.588161826133728\n",
      "Epoch: 0 \tBatch: 155 \tLoss: 1.614020824432373\n",
      "Epoch: 0 \tBatch: 156 \tLoss: 1.6494308710098267\n",
      "Epoch: 0 \tBatch: 157 \tLoss: 1.582434892654419\n",
      "Epoch: 0 \tBatch: 158 \tLoss: 1.6048996448516846\n",
      "Epoch: 0 \tBatch: 159 \tLoss: 1.6137486696243286\n",
      "Epoch: 0 \tBatch: 160 \tLoss: 1.6165177822113037\n",
      "Epoch: 0 \tBatch: 161 \tLoss: 1.5980088710784912\n",
      "Epoch: 0 \tBatch: 162 \tLoss: 1.5656903982162476\n",
      "Epoch: 0 \tBatch: 163 \tLoss: 1.5895954370498657\n",
      "Epoch: 0 \tBatch: 164 \tLoss: 1.5836305618286133\n",
      "Epoch: 0 \tBatch: 165 \tLoss: 1.6181985139846802\n",
      "Epoch: 0 \tBatch: 166 \tLoss: 1.6189857721328735\n",
      "Epoch: 0 \tBatch: 167 \tLoss: 1.605936884880066\n",
      "Epoch: 0 \tBatch: 168 \tLoss: 1.5894012451171875\n",
      "Epoch: 0 \tBatch: 169 \tLoss: 1.5981364250183105\n",
      "Epoch: 0 \tBatch: 170 \tLoss: 1.6211339235305786\n",
      "Epoch: 0 \tBatch: 171 \tLoss: 1.587512731552124\n",
      "Epoch: 0 \tBatch: 172 \tLoss: 1.6046226024627686\n",
      "Epoch: 0 \tBatch: 173 \tLoss: 1.5643941164016724\n",
      "Epoch: 0 \tBatch: 174 \tLoss: 1.5451316833496094\n",
      "Epoch: 0 \tBatch: 175 \tLoss: 1.6056902408599854\n",
      "Epoch: 0 \tBatch: 176 \tLoss: 1.511473298072815\n",
      "Epoch: 0 \tBatch: 177 \tLoss: 1.595795750617981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 178 \tLoss: 1.5762763023376465\n",
      "Epoch: 0 \tBatch: 179 \tLoss: 1.5800583362579346\n",
      "Epoch: 0 \tBatch: 180 \tLoss: 1.513466477394104\n",
      "Epoch: 0 \tBatch: 181 \tLoss: 1.6404004096984863\n",
      "Epoch: 0 \tBatch: 182 \tLoss: 1.5980699062347412\n",
      "Epoch: 0 \tBatch: 183 \tLoss: 1.5410723686218262\n",
      "Epoch: 0 \tBatch: 184 \tLoss: 1.5281157493591309\n",
      "Epoch: 0 \tBatch: 185 \tLoss: 1.5167372226715088\n",
      "Epoch: 0 \tBatch: 186 \tLoss: 1.5611010789871216\n",
      "Epoch: 0 \tBatch: 187 \tLoss: 1.5480053424835205\n",
      "Epoch: 0 \tBatch: 188 \tLoss: 1.5143311023712158\n",
      "Epoch: 0 \tBatch: 189 \tLoss: 1.5082833766937256\n",
      "Epoch: 0 \tBatch: 190 \tLoss: 1.5279673337936401\n",
      "Epoch: 0 \tBatch: 191 \tLoss: 1.5173019170761108\n",
      "Epoch: 0 \tBatch: 192 \tLoss: 1.522297978401184\n",
      "Epoch: 0 \tBatch: 193 \tLoss: 1.5212093591690063\n",
      "Epoch: 0 \tBatch: 194 \tLoss: 1.5414501428604126\n",
      "Epoch: 0 \tBatch: 195 \tLoss: 1.5443907976150513\n",
      "Epoch: 0 \tBatch: 196 \tLoss: 1.500675082206726\n",
      "Epoch: 0 \tBatch: 197 \tLoss: 1.528581142425537\n",
      "Epoch: 0 \tBatch: 198 \tLoss: 1.528075933456421\n",
      "Epoch: 0 \tBatch: 199 \tLoss: 1.5498945713043213\n",
      "Epoch: 0 \tBatch: 200 \tLoss: 1.537311315536499\n",
      "Epoch: 0 \tBatch: 201 \tLoss: 1.5206698179244995\n",
      "Epoch: 0 \tBatch: 202 \tLoss: 1.5299066305160522\n",
      "Epoch: 0 \tBatch: 203 \tLoss: 1.5099478960037231\n",
      "Epoch: 0 \tBatch: 204 \tLoss: 1.5109472274780273\n",
      "Epoch: 0 \tBatch: 205 \tLoss: 1.5333772897720337\n",
      "Epoch: 0 \tBatch: 206 \tLoss: 1.5608458518981934\n",
      "Epoch: 0 \tBatch: 207 \tLoss: 1.521843671798706\n",
      "Epoch: 0 \tBatch: 208 \tLoss: 1.5346664190292358\n",
      "Epoch: 0 \tBatch: 209 \tLoss: 1.5145751237869263\n",
      "Epoch: 0 \tBatch: 210 \tLoss: 1.514164924621582\n",
      "Epoch: 0 \tBatch: 211 \tLoss: 1.49713134765625\n",
      "Epoch: 0 \tBatch: 212 \tLoss: 1.5108903646469116\n",
      "Epoch: 0 \tBatch: 213 \tLoss: 1.5339760780334473\n",
      "Epoch: 0 \tBatch: 214 \tLoss: 1.5184645652770996\n",
      "Epoch: 0 \tBatch: 215 \tLoss: 1.4974784851074219\n",
      "Epoch: 0 \tBatch: 216 \tLoss: 1.4926782846450806\n",
      "Epoch: 0 \tBatch: 217 \tLoss: 1.5075229406356812\n",
      "Epoch: 0 \tBatch: 218 \tLoss: 1.5177618265151978\n",
      "Epoch: 0 \tBatch: 219 \tLoss: 1.494197130203247\n",
      "Epoch: 0 \tBatch: 220 \tLoss: 1.5008034706115723\n",
      "Epoch: 0 \tBatch: 221 \tLoss: 1.532741904258728\n",
      "Epoch: 0 \tBatch: 222 \tLoss: 1.5241602659225464\n",
      "Epoch: 0 \tBatch: 223 \tLoss: 1.5252578258514404\n",
      "Epoch: 0 \tBatch: 224 \tLoss: 1.5140987634658813\n",
      "Epoch: 0 \tBatch: 225 \tLoss: 1.5383081436157227\n",
      "Epoch: 0 \tBatch: 226 \tLoss: 1.4939903020858765\n",
      "Epoch: 0 \tBatch: 227 \tLoss: 1.4963302612304688\n",
      "Epoch: 0 \tBatch: 228 \tLoss: 1.5192296504974365\n",
      "Epoch: 0 \tBatch: 229 \tLoss: 1.5599935054779053\n",
      "Epoch: 0 \tBatch: 230 \tLoss: 1.526649832725525\n",
      "Epoch: 0 \tBatch: 231 \tLoss: 1.4959110021591187\n",
      "Epoch: 0 \tBatch: 232 \tLoss: 1.5478612184524536\n",
      "Epoch: 0 \tBatch: 233 \tLoss: 1.5100055932998657\n",
      "Epoch: 0 \tBatch: 234 \tLoss: 1.5017458200454712\n",
      "Epoch: 0 \tBatch: 235 \tLoss: 1.5216279029846191\n",
      "Epoch: 0 \tBatch: 236 \tLoss: 1.5160027742385864\n",
      "Epoch: 0 \tBatch: 237 \tLoss: 1.5359094142913818\n",
      "Epoch: 0 \tBatch: 238 \tLoss: 1.5088891983032227\n",
      "Epoch: 0 \tBatch: 239 \tLoss: 1.5052639245986938\n",
      "Epoch: 0 \tBatch: 240 \tLoss: 1.5006866455078125\n",
      "Epoch: 0 \tBatch: 241 \tLoss: 1.4955514669418335\n",
      "Epoch: 0 \tBatch: 242 \tLoss: 1.499843716621399\n",
      "Epoch: 0 \tBatch: 243 \tLoss: 1.5172407627105713\n",
      "Epoch: 0 \tBatch: 244 \tLoss: 1.5180460214614868\n",
      "Epoch: 0 \tBatch: 245 \tLoss: 1.5211161375045776\n",
      "Epoch: 0 \tBatch: 246 \tLoss: 1.5106478929519653\n",
      "Epoch: 0 \tBatch: 247 \tLoss: 1.500575304031372\n",
      "Epoch: 0 \tBatch: 248 \tLoss: 1.484907627105713\n",
      "Epoch: 0 \tBatch: 249 \tLoss: 1.527095913887024\n",
      "Epoch: 0 \tBatch: 250 \tLoss: 1.49362313747406\n",
      "Epoch: 0 \tBatch: 251 \tLoss: 1.5164587497711182\n",
      "Epoch: 0 \tBatch: 252 \tLoss: 1.4958666563034058\n",
      "Epoch: 0 \tBatch: 253 \tLoss: 1.5266730785369873\n",
      "Epoch: 0 \tBatch: 254 \tLoss: 1.5264207124710083\n",
      "Epoch: 0 \tBatch: 255 \tLoss: 1.502358317375183\n",
      "Epoch: 0 \tBatch: 256 \tLoss: 1.5305372476577759\n",
      "Epoch: 0 \tBatch: 257 \tLoss: 1.5178310871124268\n",
      "Epoch: 0 \tBatch: 258 \tLoss: 1.5291436910629272\n",
      "Epoch: 0 \tBatch: 259 \tLoss: 1.4999887943267822\n",
      "Epoch: 0 \tBatch: 260 \tLoss: 1.5016264915466309\n",
      "Epoch: 0 \tBatch: 261 \tLoss: 1.5360702276229858\n",
      "Epoch: 0 \tBatch: 262 \tLoss: 1.5213567018508911\n",
      "Epoch: 0 \tBatch: 263 \tLoss: 1.518926978111267\n",
      "Epoch: 0 \tBatch: 264 \tLoss: 1.475816011428833\n",
      "Epoch: 0 \tBatch: 265 \tLoss: 1.4878283739089966\n",
      "Epoch: 0 \tBatch: 266 \tLoss: 1.533894419670105\n",
      "Epoch: 0 \tBatch: 267 \tLoss: 1.5222225189208984\n",
      "Epoch: 0 \tBatch: 268 \tLoss: 1.4992142915725708\n",
      "Epoch: 0 \tBatch: 269 \tLoss: 1.5471138954162598\n",
      "Epoch: 0 \tBatch: 270 \tLoss: 1.4878007173538208\n",
      "Epoch: 0 \tBatch: 271 \tLoss: 1.5023725032806396\n",
      "Epoch: 0 \tBatch: 272 \tLoss: 1.5103825330734253\n",
      "Epoch: 0 \tBatch: 273 \tLoss: 1.5112193822860718\n",
      "Epoch: 0 \tBatch: 274 \tLoss: 1.5318962335586548\n",
      "Epoch: 0 \tBatch: 275 \tLoss: 1.5286098718643188\n",
      "Epoch: 0 \tBatch: 276 \tLoss: 1.538696050643921\n",
      "Epoch: 0 \tBatch: 277 \tLoss: 1.49888014793396\n",
      "Epoch: 0 \tBatch: 278 \tLoss: 1.4914509057998657\n",
      "Epoch: 0 \tBatch: 279 \tLoss: 1.5170323848724365\n",
      "Epoch: 0 \tBatch: 280 \tLoss: 1.4946538209915161\n",
      "Epoch: 0 \tBatch: 281 \tLoss: 1.4800984859466553\n",
      "Epoch: 0 \tBatch: 282 \tLoss: 1.4853196144104004\n",
      "Epoch: 0 \tBatch: 283 \tLoss: 1.48724365234375\n",
      "Epoch: 0 \tBatch: 284 \tLoss: 1.4945874214172363\n",
      "Epoch: 0 \tBatch: 285 \tLoss: 1.5202243328094482\n",
      "Epoch: 0 \tBatch: 286 \tLoss: 1.505922555923462\n",
      "Epoch: 0 \tBatch: 287 \tLoss: 1.5225268602371216\n",
      "Epoch: 0 \tBatch: 288 \tLoss: 1.51642906665802\n",
      "Epoch: 0 \tBatch: 289 \tLoss: 1.5065879821777344\n",
      "Epoch: 0 \tBatch: 290 \tLoss: 1.4748897552490234\n",
      "Epoch: 0 \tBatch: 291 \tLoss: 1.4846221208572388\n",
      "Epoch: 0 \tBatch: 292 \tLoss: 1.5162516832351685\n",
      "Epoch: 0 \tBatch: 293 \tLoss: 1.5085495710372925\n",
      "Epoch: 0 \tBatch: 294 \tLoss: 1.528609037399292\n",
      "Epoch: 0 \tBatch: 295 \tLoss: 1.4942865371704102\n",
      "Epoch: 0 \tBatch: 296 \tLoss: 1.4961817264556885\n",
      "Epoch: 0 \tBatch: 297 \tLoss: 1.513662576675415\n",
      "Epoch: 0 \tBatch: 298 \tLoss: 1.4868053197860718\n",
      "Epoch: 0 \tBatch: 299 \tLoss: 1.5004297494888306\n",
      "Epoch: 0 \tBatch: 300 \tLoss: 1.4914246797561646\n",
      "Epoch: 0 \tBatch: 301 \tLoss: 1.5060489177703857\n",
      "Epoch: 0 \tBatch: 302 \tLoss: 1.527274250984192\n",
      "Epoch: 0 \tBatch: 303 \tLoss: 1.5040735006332397\n",
      "Epoch: 0 \tBatch: 304 \tLoss: 1.5082052946090698\n",
      "Epoch: 0 \tBatch: 305 \tLoss: 1.5150986909866333\n",
      "Epoch: 0 \tBatch: 306 \tLoss: 1.4962238073349\n",
      "Epoch: 0 \tBatch: 307 \tLoss: 1.5075006484985352\n",
      "Epoch: 0 \tBatch: 308 \tLoss: 1.5242161750793457\n",
      "Epoch: 0 \tBatch: 309 \tLoss: 1.5195039510726929\n",
      "Epoch: 0 \tBatch: 310 \tLoss: 1.4854942560195923\n",
      "Epoch: 0 \tBatch: 311 \tLoss: 1.5272395610809326\n",
      "Epoch: 0 \tBatch: 312 \tLoss: 1.5091967582702637\n",
      "Epoch: 0 \tBatch: 313 \tLoss: 1.5262609720230103\n",
      "Epoch: 0 \tBatch: 314 \tLoss: 1.4913290739059448\n",
      "Epoch: 0 \tBatch: 315 \tLoss: 1.473191261291504\n",
      "Epoch: 0 \tBatch: 316 \tLoss: 1.4849416017532349\n",
      "Epoch: 0 \tBatch: 317 \tLoss: 1.5033671855926514\n",
      "Epoch: 0 \tBatch: 318 \tLoss: 1.4948300123214722\n",
      "Epoch: 0 \tBatch: 319 \tLoss: 1.5109143257141113\n",
      "Epoch: 0 \tBatch: 320 \tLoss: 1.4968341588974\n",
      "Epoch: 0 \tBatch: 321 \tLoss: 1.511260747909546\n",
      "Epoch: 0 \tBatch: 322 \tLoss: 1.4901628494262695\n",
      "Epoch: 0 \tBatch: 323 \tLoss: 1.4839838743209839\n",
      "Epoch: 0 \tBatch: 324 \tLoss: 1.509020209312439\n",
      "Epoch: 0 \tBatch: 325 \tLoss: 1.4978489875793457\n",
      "Epoch: 0 \tBatch: 326 \tLoss: 1.5007842779159546\n",
      "Epoch: 0 \tBatch: 327 \tLoss: 1.493965744972229\n",
      "Epoch: 0 \tBatch: 328 \tLoss: 1.505028486251831\n",
      "Epoch: 0 \tBatch: 329 \tLoss: 1.4690210819244385\n",
      "Epoch: 0 \tBatch: 330 \tLoss: 1.5182064771652222\n",
      "Epoch: 0 \tBatch: 331 \tLoss: 1.5330835580825806\n",
      "Epoch: 0 \tBatch: 332 \tLoss: 1.518495798110962\n",
      "Epoch: 0 \tBatch: 333 \tLoss: 1.5015370845794678\n",
      "Epoch: 0 \tBatch: 334 \tLoss: 1.5036770105361938\n",
      "Epoch: 0 \tBatch: 335 \tLoss: 1.4877697229385376\n",
      "Epoch: 0 \tBatch: 336 \tLoss: 1.4867855310440063\n",
      "Epoch: 0 \tBatch: 337 \tLoss: 1.4918053150177002\n",
      "Epoch: 0 \tBatch: 338 \tLoss: 1.518982172012329\n",
      "Epoch: 0 \tBatch: 339 \tLoss: 1.5138791799545288\n",
      "Epoch: 0 \tBatch: 340 \tLoss: 1.51438307762146\n",
      "Epoch: 0 \tBatch: 341 \tLoss: 1.4873902797698975\n",
      "Epoch: 0 \tBatch: 342 \tLoss: 1.4780563116073608\n",
      "Epoch: 0 \tBatch: 343 \tLoss: 1.5059276819229126\n",
      "Epoch: 0 \tBatch: 344 \tLoss: 1.5055594444274902\n",
      "Epoch: 0 \tBatch: 345 \tLoss: 1.5031031370162964\n",
      "Epoch: 0 \tBatch: 346 \tLoss: 1.5117337703704834\n",
      "Epoch: 0 \tBatch: 347 \tLoss: 1.5057101249694824\n",
      "Epoch: 0 \tBatch: 348 \tLoss: 1.4894042015075684\n",
      "Epoch: 0 \tBatch: 349 \tLoss: 1.4760971069335938\n",
      "Epoch: 0 \tBatch: 350 \tLoss: 1.5216474533081055\n",
      "Epoch: 0 \tBatch: 351 \tLoss: 1.482350468635559\n",
      "Epoch: 0 \tBatch: 352 \tLoss: 1.4906388521194458\n",
      "Epoch: 0 \tBatch: 353 \tLoss: 1.4916393756866455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 354 \tLoss: 1.514717698097229\n",
      "Epoch: 0 \tBatch: 355 \tLoss: 1.5119324922561646\n",
      "Epoch: 0 \tBatch: 356 \tLoss: 1.4943675994873047\n",
      "Epoch: 0 \tBatch: 357 \tLoss: 1.5194426774978638\n",
      "Epoch: 0 \tBatch: 358 \tLoss: 1.4939274787902832\n",
      "Epoch: 0 \tBatch: 359 \tLoss: 1.4959214925765991\n",
      "Epoch: 0 \tBatch: 360 \tLoss: 1.4813861846923828\n",
      "Epoch: 0 \tBatch: 361 \tLoss: 1.4961460828781128\n",
      "Epoch: 0 \tBatch: 362 \tLoss: 1.50568425655365\n",
      "Epoch: 0 \tBatch: 363 \tLoss: 1.485313057899475\n",
      "Epoch: 0 \tBatch: 364 \tLoss: 1.4955095052719116\n",
      "Epoch: 0 \tBatch: 365 \tLoss: 1.481604814529419\n",
      "Epoch: 0 \tBatch: 366 \tLoss: 1.514917254447937\n",
      "Epoch: 0 \tBatch: 367 \tLoss: 1.5027623176574707\n",
      "Epoch: 0 \tBatch: 368 \tLoss: 1.5014278888702393\n",
      "Epoch: 0 \tBatch: 369 \tLoss: 1.5141514539718628\n",
      "Epoch: 0 \tBatch: 370 \tLoss: 1.4771572351455688\n",
      "Epoch: 0 \tBatch: 371 \tLoss: 1.503778338432312\n",
      "Epoch: 0 \tBatch: 372 \tLoss: 1.500123381614685\n",
      "Epoch: 0 \tBatch: 373 \tLoss: 1.509087324142456\n",
      "Epoch: 0 \tBatch: 374 \tLoss: 1.4851549863815308\n",
      "Epoch: 0 \tBatch: 375 \tLoss: 1.4800626039505005\n",
      "Epoch: 0 \tBatch: 376 \tLoss: 1.481989860534668\n",
      "Epoch: 0 \tBatch: 377 \tLoss: 1.5050160884857178\n",
      "Epoch: 0 \tBatch: 378 \tLoss: 1.512783169746399\n",
      "Epoch: 0 \tBatch: 379 \tLoss: 1.5066176652908325\n",
      "Epoch: 0 \tBatch: 380 \tLoss: 1.5026596784591675\n",
      "Epoch: 0 \tBatch: 381 \tLoss: 1.4998759031295776\n",
      "Epoch: 0 \tBatch: 382 \tLoss: 1.48100745677948\n",
      "Epoch: 0 \tBatch: 383 \tLoss: 1.4866684675216675\n",
      "Epoch: 0 \tBatch: 384 \tLoss: 1.5278240442276\n",
      "Epoch: 0 \tBatch: 385 \tLoss: 1.493778109550476\n",
      "Epoch: 0 \tBatch: 386 \tLoss: 1.508935570716858\n",
      "Epoch: 0 \tBatch: 387 \tLoss: 1.4986340999603271\n",
      "Epoch: 0 \tBatch: 388 \tLoss: 1.4823356866836548\n",
      "Epoch: 0 \tBatch: 389 \tLoss: 1.4913402795791626\n",
      "Epoch: 0 \tBatch: 390 \tLoss: 1.498246431350708\n",
      "Epoch: 1 \tBatch: 0 \tLoss: 1.4939390420913696\n",
      "Epoch: 1 \tBatch: 1 \tLoss: 1.4801262617111206\n",
      "Epoch: 1 \tBatch: 2 \tLoss: 1.4837350845336914\n",
      "Epoch: 1 \tBatch: 3 \tLoss: 1.4923536777496338\n",
      "Epoch: 1 \tBatch: 4 \tLoss: 1.5184358358383179\n",
      "Epoch: 1 \tBatch: 5 \tLoss: 1.4972738027572632\n",
      "Epoch: 1 \tBatch: 6 \tLoss: 1.5032237768173218\n",
      "Epoch: 1 \tBatch: 7 \tLoss: 1.4938300848007202\n",
      "Epoch: 1 \tBatch: 8 \tLoss: 1.490911602973938\n",
      "Epoch: 1 \tBatch: 9 \tLoss: 1.5196170806884766\n",
      "Epoch: 1 \tBatch: 10 \tLoss: 1.5374034643173218\n",
      "Epoch: 1 \tBatch: 11 \tLoss: 1.48444402217865\n",
      "Epoch: 1 \tBatch: 12 \tLoss: 1.4883134365081787\n",
      "Epoch: 1 \tBatch: 13 \tLoss: 1.4845165014266968\n",
      "Epoch: 1 \tBatch: 14 \tLoss: 1.4801236391067505\n",
      "Epoch: 1 \tBatch: 15 \tLoss: 1.4866478443145752\n",
      "Epoch: 1 \tBatch: 16 \tLoss: 1.5042237043380737\n",
      "Epoch: 1 \tBatch: 17 \tLoss: 1.4949548244476318\n",
      "Epoch: 1 \tBatch: 18 \tLoss: 1.476868987083435\n",
      "Epoch: 1 \tBatch: 19 \tLoss: 1.4781999588012695\n",
      "Epoch: 1 \tBatch: 20 \tLoss: 1.5025516748428345\n",
      "Epoch: 1 \tBatch: 21 \tLoss: 1.4873384237289429\n",
      "Epoch: 1 \tBatch: 22 \tLoss: 1.4820265769958496\n",
      "Epoch: 1 \tBatch: 23 \tLoss: 1.4820280075073242\n",
      "Epoch: 1 \tBatch: 24 \tLoss: 1.4938379526138306\n",
      "Epoch: 1 \tBatch: 25 \tLoss: 1.4905661344528198\n",
      "Epoch: 1 \tBatch: 26 \tLoss: 1.52317476272583\n",
      "Epoch: 1 \tBatch: 27 \tLoss: 1.4857550859451294\n",
      "Epoch: 1 \tBatch: 28 \tLoss: 1.4777685403823853\n",
      "Epoch: 1 \tBatch: 29 \tLoss: 1.5041331052780151\n",
      "Epoch: 1 \tBatch: 30 \tLoss: 1.5000724792480469\n",
      "Epoch: 1 \tBatch: 31 \tLoss: 1.4888113737106323\n",
      "Epoch: 1 \tBatch: 32 \tLoss: 1.4891574382781982\n",
      "Epoch: 1 \tBatch: 33 \tLoss: 1.4868147373199463\n",
      "Epoch: 1 \tBatch: 34 \tLoss: 1.4771957397460938\n",
      "Epoch: 1 \tBatch: 35 \tLoss: 1.499354600906372\n",
      "Epoch: 1 \tBatch: 36 \tLoss: 1.4664337635040283\n",
      "Epoch: 1 \tBatch: 37 \tLoss: 1.500824213027954\n",
      "Epoch: 1 \tBatch: 38 \tLoss: 1.4984264373779297\n",
      "Epoch: 1 \tBatch: 39 \tLoss: 1.5182528495788574\n",
      "Epoch: 1 \tBatch: 40 \tLoss: 1.4858205318450928\n",
      "Epoch: 1 \tBatch: 41 \tLoss: 1.4837110042572021\n",
      "Epoch: 1 \tBatch: 42 \tLoss: 1.4947844743728638\n",
      "Epoch: 1 \tBatch: 43 \tLoss: 1.5134243965148926\n",
      "Epoch: 1 \tBatch: 44 \tLoss: 1.495176911354065\n",
      "Epoch: 1 \tBatch: 45 \tLoss: 1.4990969896316528\n",
      "Epoch: 1 \tBatch: 46 \tLoss: 1.5064443349838257\n",
      "Epoch: 1 \tBatch: 47 \tLoss: 1.4864486455917358\n",
      "Epoch: 1 \tBatch: 48 \tLoss: 1.5011881589889526\n",
      "Epoch: 1 \tBatch: 49 \tLoss: 1.4828070402145386\n",
      "Epoch: 1 \tBatch: 50 \tLoss: 1.4836677312850952\n",
      "Epoch: 1 \tBatch: 51 \tLoss: 1.4704786539077759\n",
      "Epoch: 1 \tBatch: 52 \tLoss: 1.488523006439209\n",
      "Epoch: 1 \tBatch: 53 \tLoss: 1.4987767934799194\n",
      "Epoch: 1 \tBatch: 54 \tLoss: 1.4742439985275269\n",
      "Epoch: 1 \tBatch: 55 \tLoss: 1.4978970289230347\n",
      "Epoch: 1 \tBatch: 56 \tLoss: 1.4887079000473022\n",
      "Epoch: 1 \tBatch: 57 \tLoss: 1.4894343614578247\n",
      "Epoch: 1 \tBatch: 58 \tLoss: 1.4724740982055664\n",
      "Epoch: 1 \tBatch: 59 \tLoss: 1.5010433197021484\n",
      "Epoch: 1 \tBatch: 60 \tLoss: 1.4843612909317017\n",
      "Epoch: 1 \tBatch: 61 \tLoss: 1.5039018392562866\n",
      "Epoch: 1 \tBatch: 62 \tLoss: 1.4819895029067993\n",
      "Epoch: 1 \tBatch: 63 \tLoss: 1.4988054037094116\n",
      "Epoch: 1 \tBatch: 64 \tLoss: 1.5163805484771729\n",
      "Epoch: 1 \tBatch: 65 \tLoss: 1.489215612411499\n",
      "Epoch: 1 \tBatch: 66 \tLoss: 1.485595941543579\n",
      "Epoch: 1 \tBatch: 67 \tLoss: 1.5109049081802368\n",
      "Epoch: 1 \tBatch: 68 \tLoss: 1.4832603931427002\n",
      "Epoch: 1 \tBatch: 69 \tLoss: 1.4892313480377197\n",
      "Epoch: 1 \tBatch: 70 \tLoss: 1.4846657514572144\n",
      "Epoch: 1 \tBatch: 71 \tLoss: 1.5119962692260742\n",
      "Epoch: 1 \tBatch: 72 \tLoss: 1.47931706905365\n",
      "Epoch: 1 \tBatch: 73 \tLoss: 1.5188935995101929\n",
      "Epoch: 1 \tBatch: 74 \tLoss: 1.5069204568862915\n",
      "Epoch: 1 \tBatch: 75 \tLoss: 1.4780848026275635\n",
      "Epoch: 1 \tBatch: 76 \tLoss: 1.4837912321090698\n",
      "Epoch: 1 \tBatch: 77 \tLoss: 1.4893423318862915\n",
      "Epoch: 1 \tBatch: 78 \tLoss: 1.4815770387649536\n",
      "Epoch: 1 \tBatch: 79 \tLoss: 1.478222131729126\n",
      "Epoch: 1 \tBatch: 80 \tLoss: 1.5106585025787354\n",
      "Epoch: 1 \tBatch: 81 \tLoss: 1.469948649406433\n",
      "Epoch: 1 \tBatch: 82 \tLoss: 1.508340835571289\n",
      "Epoch: 1 \tBatch: 83 \tLoss: 1.4910788536071777\n",
      "Epoch: 1 \tBatch: 84 \tLoss: 1.492583155632019\n",
      "Epoch: 1 \tBatch: 85 \tLoss: 1.4834309816360474\n",
      "Epoch: 1 \tBatch: 86 \tLoss: 1.4959732294082642\n",
      "Epoch: 1 \tBatch: 87 \tLoss: 1.480825424194336\n",
      "Epoch: 1 \tBatch: 88 \tLoss: 1.5195297002792358\n",
      "Epoch: 1 \tBatch: 89 \tLoss: 1.4979604482650757\n",
      "Epoch: 1 \tBatch: 90 \tLoss: 1.4991754293441772\n",
      "Epoch: 1 \tBatch: 91 \tLoss: 1.4645211696624756\n",
      "Epoch: 1 \tBatch: 92 \tLoss: 1.4871115684509277\n",
      "Epoch: 1 \tBatch: 93 \tLoss: 1.4955365657806396\n",
      "Epoch: 1 \tBatch: 94 \tLoss: 1.5049813985824585\n",
      "Epoch: 1 \tBatch: 95 \tLoss: 1.5236223936080933\n",
      "Epoch: 1 \tBatch: 96 \tLoss: 1.4909229278564453\n",
      "Epoch: 1 \tBatch: 97 \tLoss: 1.5088980197906494\n",
      "Epoch: 1 \tBatch: 98 \tLoss: 1.484025001525879\n",
      "Epoch: 1 \tBatch: 99 \tLoss: 1.4742625951766968\n",
      "Epoch: 1 \tBatch: 100 \tLoss: 1.476745843887329\n",
      "Epoch: 1 \tBatch: 101 \tLoss: 1.4846431016921997\n",
      "Epoch: 1 \tBatch: 102 \tLoss: 1.4911152124404907\n",
      "Epoch: 1 \tBatch: 103 \tLoss: 1.474556803703308\n",
      "Epoch: 1 \tBatch: 104 \tLoss: 1.4902210235595703\n",
      "Epoch: 1 \tBatch: 105 \tLoss: 1.5028983354568481\n",
      "Epoch: 1 \tBatch: 106 \tLoss: 1.4879757165908813\n",
      "Epoch: 1 \tBatch: 107 \tLoss: 1.4847629070281982\n",
      "Epoch: 1 \tBatch: 108 \tLoss: 1.5119074583053589\n",
      "Epoch: 1 \tBatch: 109 \tLoss: 1.484344482421875\n",
      "Epoch: 1 \tBatch: 110 \tLoss: 1.5228102207183838\n",
      "Epoch: 1 \tBatch: 111 \tLoss: 1.5044459104537964\n",
      "Epoch: 1 \tBatch: 112 \tLoss: 1.4877718687057495\n",
      "Epoch: 1 \tBatch: 113 \tLoss: 1.4858993291854858\n",
      "Epoch: 1 \tBatch: 114 \tLoss: 1.4895330667495728\n",
      "Epoch: 1 \tBatch: 115 \tLoss: 1.5006368160247803\n",
      "Epoch: 1 \tBatch: 116 \tLoss: 1.4775352478027344\n",
      "Epoch: 1 \tBatch: 117 \tLoss: 1.507178783416748\n",
      "Epoch: 1 \tBatch: 118 \tLoss: 1.4891843795776367\n",
      "Epoch: 1 \tBatch: 119 \tLoss: 1.497484803199768\n",
      "Epoch: 1 \tBatch: 120 \tLoss: 1.4735701084136963\n",
      "Epoch: 1 \tBatch: 121 \tLoss: 1.49668288230896\n",
      "Epoch: 1 \tBatch: 122 \tLoss: 1.504126787185669\n",
      "Epoch: 1 \tBatch: 123 \tLoss: 1.4625412225723267\n",
      "Epoch: 1 \tBatch: 124 \tLoss: 1.4795092344284058\n",
      "Epoch: 1 \tBatch: 125 \tLoss: 1.4777168035507202\n",
      "Epoch: 1 \tBatch: 126 \tLoss: 1.5179071426391602\n",
      "Epoch: 1 \tBatch: 127 \tLoss: 1.5072039365768433\n",
      "Epoch: 1 \tBatch: 128 \tLoss: 1.4900164604187012\n",
      "Epoch: 1 \tBatch: 129 \tLoss: 1.4726684093475342\n",
      "Epoch: 1 \tBatch: 130 \tLoss: 1.4865580797195435\n",
      "Epoch: 1 \tBatch: 131 \tLoss: 1.4945584535598755\n",
      "Epoch: 1 \tBatch: 132 \tLoss: 1.4828605651855469\n",
      "Epoch: 1 \tBatch: 133 \tLoss: 1.480136752128601\n",
      "Epoch: 1 \tBatch: 134 \tLoss: 1.491241455078125\n",
      "Epoch: 1 \tBatch: 135 \tLoss: 1.5006108283996582\n",
      "Epoch: 1 \tBatch: 136 \tLoss: 1.4643059968948364\n",
      "Epoch: 1 \tBatch: 137 \tLoss: 1.4722657203674316\n",
      "Epoch: 1 \tBatch: 138 \tLoss: 1.4870871305465698\n",
      "Epoch: 1 \tBatch: 139 \tLoss: 1.4798479080200195\n",
      "Epoch: 1 \tBatch: 140 \tLoss: 1.4985454082489014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 141 \tLoss: 1.492078423500061\n",
      "Epoch: 1 \tBatch: 142 \tLoss: 1.4773526191711426\n",
      "Epoch: 1 \tBatch: 143 \tLoss: 1.519448161125183\n",
      "Epoch: 1 \tBatch: 144 \tLoss: 1.4704116582870483\n",
      "Epoch: 1 \tBatch: 145 \tLoss: 1.486060619354248\n",
      "Epoch: 1 \tBatch: 146 \tLoss: 1.4779621362686157\n",
      "Epoch: 1 \tBatch: 147 \tLoss: 1.4981584548950195\n",
      "Epoch: 1 \tBatch: 148 \tLoss: 1.488427996635437\n",
      "Epoch: 1 \tBatch: 149 \tLoss: 1.471592903137207\n",
      "Epoch: 1 \tBatch: 150 \tLoss: 1.4668635129928589\n",
      "Epoch: 1 \tBatch: 151 \tLoss: 1.4785908460617065\n",
      "Epoch: 1 \tBatch: 152 \tLoss: 1.4954687356948853\n",
      "Epoch: 1 \tBatch: 153 \tLoss: 1.5071675777435303\n",
      "Epoch: 1 \tBatch: 154 \tLoss: 1.48356294631958\n",
      "Epoch: 1 \tBatch: 155 \tLoss: 1.5342893600463867\n",
      "Epoch: 1 \tBatch: 156 \tLoss: 1.4770921468734741\n",
      "Epoch: 1 \tBatch: 157 \tLoss: 1.5139811038970947\n",
      "Epoch: 1 \tBatch: 158 \tLoss: 1.4906487464904785\n",
      "Epoch: 1 \tBatch: 159 \tLoss: 1.4940723180770874\n",
      "Epoch: 1 \tBatch: 160 \tLoss: 1.4789819717407227\n",
      "Epoch: 1 \tBatch: 161 \tLoss: 1.5019302368164062\n",
      "Epoch: 1 \tBatch: 162 \tLoss: 1.5026578903198242\n",
      "Epoch: 1 \tBatch: 163 \tLoss: 1.4781285524368286\n",
      "Epoch: 1 \tBatch: 164 \tLoss: 1.4930109977722168\n",
      "Epoch: 1 \tBatch: 165 \tLoss: 1.5117796659469604\n",
      "Epoch: 1 \tBatch: 166 \tLoss: 1.4941613674163818\n",
      "Epoch: 1 \tBatch: 167 \tLoss: 1.4822274446487427\n",
      "Epoch: 1 \tBatch: 168 \tLoss: 1.4813178777694702\n",
      "Epoch: 1 \tBatch: 169 \tLoss: 1.4798465967178345\n",
      "Epoch: 1 \tBatch: 170 \tLoss: 1.4798400402069092\n",
      "Epoch: 1 \tBatch: 171 \tLoss: 1.5014476776123047\n",
      "Epoch: 1 \tBatch: 172 \tLoss: 1.488614559173584\n",
      "Epoch: 1 \tBatch: 173 \tLoss: 1.505048394203186\n",
      "Epoch: 1 \tBatch: 174 \tLoss: 1.4799338579177856\n",
      "Epoch: 1 \tBatch: 175 \tLoss: 1.4890902042388916\n",
      "Epoch: 1 \tBatch: 176 \tLoss: 1.479333758354187\n",
      "Epoch: 1 \tBatch: 177 \tLoss: 1.4843456745147705\n",
      "Epoch: 1 \tBatch: 178 \tLoss: 1.473431944847107\n",
      "Epoch: 1 \tBatch: 179 \tLoss: 1.465604543685913\n",
      "Epoch: 1 \tBatch: 180 \tLoss: 1.4913586378097534\n",
      "Epoch: 1 \tBatch: 181 \tLoss: 1.4920130968093872\n",
      "Epoch: 1 \tBatch: 182 \tLoss: 1.4787479639053345\n",
      "Epoch: 1 \tBatch: 183 \tLoss: 1.4834383726119995\n",
      "Epoch: 1 \tBatch: 184 \tLoss: 1.4699240922927856\n",
      "Epoch: 1 \tBatch: 185 \tLoss: 1.4891668558120728\n",
      "Epoch: 1 \tBatch: 186 \tLoss: 1.4650847911834717\n",
      "Epoch: 1 \tBatch: 187 \tLoss: 1.4847675561904907\n",
      "Epoch: 1 \tBatch: 188 \tLoss: 1.4806902408599854\n",
      "Epoch: 1 \tBatch: 189 \tLoss: 1.4738268852233887\n",
      "Epoch: 1 \tBatch: 190 \tLoss: 1.5040873289108276\n",
      "Epoch: 1 \tBatch: 191 \tLoss: 1.4849220514297485\n",
      "Epoch: 1 \tBatch: 192 \tLoss: 1.5081573724746704\n",
      "Epoch: 1 \tBatch: 193 \tLoss: 1.5037753582000732\n",
      "Epoch: 1 \tBatch: 194 \tLoss: 1.4973005056381226\n",
      "Epoch: 1 \tBatch: 195 \tLoss: 1.5008233785629272\n",
      "Epoch: 1 \tBatch: 196 \tLoss: 1.5126205682754517\n",
      "Epoch: 1 \tBatch: 197 \tLoss: 1.508087158203125\n",
      "Epoch: 1 \tBatch: 198 \tLoss: 1.4910601377487183\n",
      "Epoch: 1 \tBatch: 199 \tLoss: 1.4731080532073975\n",
      "Epoch: 1 \tBatch: 200 \tLoss: 1.486778974533081\n",
      "Epoch: 1 \tBatch: 201 \tLoss: 1.4941452741622925\n",
      "Epoch: 1 \tBatch: 202 \tLoss: 1.4969749450683594\n",
      "Epoch: 1 \tBatch: 203 \tLoss: 1.4657169580459595\n",
      "Epoch: 1 \tBatch: 204 \tLoss: 1.4789015054702759\n",
      "Epoch: 1 \tBatch: 205 \tLoss: 1.494415044784546\n",
      "Epoch: 1 \tBatch: 206 \tLoss: 1.511766791343689\n",
      "Epoch: 1 \tBatch: 207 \tLoss: 1.4772326946258545\n",
      "Epoch: 1 \tBatch: 208 \tLoss: 1.4847757816314697\n",
      "Epoch: 1 \tBatch: 209 \tLoss: 1.4866982698440552\n",
      "Epoch: 1 \tBatch: 210 \tLoss: 1.4976612329483032\n",
      "Epoch: 1 \tBatch: 211 \tLoss: 1.4953370094299316\n",
      "Epoch: 1 \tBatch: 212 \tLoss: 1.4895789623260498\n",
      "Epoch: 1 \tBatch: 213 \tLoss: 1.4783800840377808\n",
      "Epoch: 1 \tBatch: 214 \tLoss: 1.4962866306304932\n",
      "Epoch: 1 \tBatch: 215 \tLoss: 1.5189417600631714\n",
      "Epoch: 1 \tBatch: 216 \tLoss: 1.4790276288986206\n",
      "Epoch: 1 \tBatch: 217 \tLoss: 1.4757438898086548\n",
      "Epoch: 1 \tBatch: 218 \tLoss: 1.4960507154464722\n",
      "Epoch: 1 \tBatch: 219 \tLoss: 1.485418438911438\n",
      "Epoch: 1 \tBatch: 220 \tLoss: 1.4818617105484009\n",
      "Epoch: 1 \tBatch: 221 \tLoss: 1.504928708076477\n",
      "Epoch: 1 \tBatch: 222 \tLoss: 1.4852499961853027\n",
      "Epoch: 1 \tBatch: 223 \tLoss: 1.4894531965255737\n",
      "Epoch: 1 \tBatch: 224 \tLoss: 1.5131113529205322\n",
      "Epoch: 1 \tBatch: 225 \tLoss: 1.5009485483169556\n",
      "Epoch: 1 \tBatch: 226 \tLoss: 1.4637300968170166\n",
      "Epoch: 1 \tBatch: 227 \tLoss: 1.4726649522781372\n",
      "Epoch: 1 \tBatch: 228 \tLoss: 1.506285309791565\n",
      "Epoch: 1 \tBatch: 229 \tLoss: 1.4808000326156616\n",
      "Epoch: 1 \tBatch: 230 \tLoss: 1.4989843368530273\n",
      "Epoch: 1 \tBatch: 231 \tLoss: 1.5047687292099\n",
      "Epoch: 1 \tBatch: 232 \tLoss: 1.4910845756530762\n",
      "Epoch: 1 \tBatch: 233 \tLoss: 1.4919581413269043\n",
      "Epoch: 1 \tBatch: 234 \tLoss: 1.4725664854049683\n",
      "Epoch: 1 \tBatch: 235 \tLoss: 1.5154939889907837\n",
      "Epoch: 1 \tBatch: 236 \tLoss: 1.491068959236145\n",
      "Epoch: 1 \tBatch: 237 \tLoss: 1.4899756908416748\n",
      "Epoch: 1 \tBatch: 238 \tLoss: 1.4913091659545898\n",
      "Epoch: 1 \tBatch: 239 \tLoss: 1.498655915260315\n",
      "Epoch: 1 \tBatch: 240 \tLoss: 1.4877350330352783\n",
      "Epoch: 1 \tBatch: 241 \tLoss: 1.4845993518829346\n",
      "Epoch: 1 \tBatch: 242 \tLoss: 1.4873954057693481\n",
      "Epoch: 1 \tBatch: 243 \tLoss: 1.5067756175994873\n",
      "Epoch: 1 \tBatch: 244 \tLoss: 1.5101709365844727\n",
      "Epoch: 1 \tBatch: 245 \tLoss: 1.5007450580596924\n",
      "Epoch: 1 \tBatch: 246 \tLoss: 1.481768012046814\n",
      "Epoch: 1 \tBatch: 247 \tLoss: 1.4623140096664429\n",
      "Epoch: 1 \tBatch: 248 \tLoss: 1.4763014316558838\n",
      "Epoch: 1 \tBatch: 249 \tLoss: 1.471712350845337\n",
      "Epoch: 1 \tBatch: 250 \tLoss: 1.481218934059143\n",
      "Epoch: 1 \tBatch: 251 \tLoss: 1.5025899410247803\n",
      "Epoch: 1 \tBatch: 252 \tLoss: 1.4922034740447998\n",
      "Epoch: 1 \tBatch: 253 \tLoss: 1.4959633350372314\n",
      "Epoch: 1 \tBatch: 254 \tLoss: 1.4709421396255493\n",
      "Epoch: 1 \tBatch: 255 \tLoss: 1.4620447158813477\n",
      "Epoch: 1 \tBatch: 256 \tLoss: 1.494860291481018\n",
      "Epoch: 1 \tBatch: 257 \tLoss: 1.494321346282959\n",
      "Epoch: 1 \tBatch: 258 \tLoss: 1.4945820569992065\n",
      "Epoch: 1 \tBatch: 259 \tLoss: 1.4902145862579346\n",
      "Epoch: 1 \tBatch: 260 \tLoss: 1.5124225616455078\n",
      "Epoch: 1 \tBatch: 261 \tLoss: 1.4825433492660522\n",
      "Epoch: 1 \tBatch: 262 \tLoss: 1.491611123085022\n",
      "Epoch: 1 \tBatch: 263 \tLoss: 1.498868465423584\n",
      "Epoch: 1 \tBatch: 264 \tLoss: 1.4923123121261597\n",
      "Epoch: 1 \tBatch: 265 \tLoss: 1.4871073961257935\n",
      "Epoch: 1 \tBatch: 266 \tLoss: 1.516717791557312\n",
      "Epoch: 1 \tBatch: 267 \tLoss: 1.480798602104187\n",
      "Epoch: 1 \tBatch: 268 \tLoss: 1.494462251663208\n",
      "Epoch: 1 \tBatch: 269 \tLoss: 1.4889487028121948\n",
      "Epoch: 1 \tBatch: 270 \tLoss: 1.48232901096344\n",
      "Epoch: 1 \tBatch: 271 \tLoss: 1.4858310222625732\n",
      "Epoch: 1 \tBatch: 272 \tLoss: 1.485562801361084\n",
      "Epoch: 1 \tBatch: 273 \tLoss: 1.4750536680221558\n",
      "Epoch: 1 \tBatch: 274 \tLoss: 1.4759705066680908\n",
      "Epoch: 1 \tBatch: 275 \tLoss: 1.4691133499145508\n",
      "Epoch: 1 \tBatch: 276 \tLoss: 1.4793230295181274\n",
      "Epoch: 1 \tBatch: 277 \tLoss: 1.4910370111465454\n",
      "Epoch: 1 \tBatch: 278 \tLoss: 1.4822243452072144\n",
      "Epoch: 1 \tBatch: 279 \tLoss: 1.4870707988739014\n",
      "Epoch: 1 \tBatch: 280 \tLoss: 1.4936285018920898\n",
      "Epoch: 1 \tBatch: 281 \tLoss: 1.4852631092071533\n",
      "Epoch: 1 \tBatch: 282 \tLoss: 1.4757375717163086\n",
      "Epoch: 1 \tBatch: 283 \tLoss: 1.5027289390563965\n",
      "Epoch: 1 \tBatch: 284 \tLoss: 1.4859495162963867\n",
      "Epoch: 1 \tBatch: 285 \tLoss: 1.5100773572921753\n",
      "Epoch: 1 \tBatch: 286 \tLoss: 1.492895483970642\n",
      "Epoch: 1 \tBatch: 287 \tLoss: 1.4962188005447388\n",
      "Epoch: 1 \tBatch: 288 \tLoss: 1.489938735961914\n",
      "Epoch: 1 \tBatch: 289 \tLoss: 1.4945340156555176\n",
      "Epoch: 1 \tBatch: 290 \tLoss: 1.4935146570205688\n",
      "Epoch: 1 \tBatch: 291 \tLoss: 1.5034794807434082\n",
      "Epoch: 1 \tBatch: 292 \tLoss: 1.518315076828003\n",
      "Epoch: 1 \tBatch: 293 \tLoss: 1.468828558921814\n",
      "Epoch: 1 \tBatch: 294 \tLoss: 1.4868441820144653\n",
      "Epoch: 1 \tBatch: 295 \tLoss: 1.475637674331665\n",
      "Epoch: 1 \tBatch: 296 \tLoss: 1.4900236129760742\n",
      "Epoch: 1 \tBatch: 297 \tLoss: 1.4850884675979614\n",
      "Epoch: 1 \tBatch: 298 \tLoss: 1.50850248336792\n",
      "Epoch: 1 \tBatch: 299 \tLoss: 1.4923449754714966\n",
      "Epoch: 1 \tBatch: 300 \tLoss: 1.5035394430160522\n",
      "Epoch: 1 \tBatch: 301 \tLoss: 1.5081791877746582\n",
      "Epoch: 1 \tBatch: 302 \tLoss: 1.4815553426742554\n",
      "Epoch: 1 \tBatch: 303 \tLoss: 1.4727511405944824\n",
      "Epoch: 1 \tBatch: 304 \tLoss: 1.4731261730194092\n",
      "Epoch: 1 \tBatch: 305 \tLoss: 1.4652026891708374\n",
      "Epoch: 1 \tBatch: 306 \tLoss: 1.4923536777496338\n",
      "Epoch: 1 \tBatch: 307 \tLoss: 1.5209474563598633\n",
      "Epoch: 1 \tBatch: 308 \tLoss: 1.5058140754699707\n",
      "Epoch: 1 \tBatch: 309 \tLoss: 1.493849277496338\n",
      "Epoch: 1 \tBatch: 310 \tLoss: 1.4739707708358765\n",
      "Epoch: 1 \tBatch: 311 \tLoss: 1.5036040544509888\n",
      "Epoch: 1 \tBatch: 312 \tLoss: 1.4860199689865112\n",
      "Epoch: 1 \tBatch: 313 \tLoss: 1.4814542531967163\n",
      "Epoch: 1 \tBatch: 314 \tLoss: 1.484214186668396\n",
      "Epoch: 1 \tBatch: 315 \tLoss: 1.4883767366409302\n",
      "Epoch: 1 \tBatch: 316 \tLoss: 1.4970253705978394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 317 \tLoss: 1.4930119514465332\n",
      "Epoch: 1 \tBatch: 318 \tLoss: 1.5071690082550049\n",
      "Epoch: 1 \tBatch: 319 \tLoss: 1.4887018203735352\n",
      "Epoch: 1 \tBatch: 320 \tLoss: 1.4620970487594604\n",
      "Epoch: 1 \tBatch: 321 \tLoss: 1.5012500286102295\n",
      "Epoch: 1 \tBatch: 322 \tLoss: 1.5160245895385742\n",
      "Epoch: 1 \tBatch: 323 \tLoss: 1.4941270351409912\n",
      "Epoch: 1 \tBatch: 324 \tLoss: 1.4886984825134277\n",
      "Epoch: 1 \tBatch: 325 \tLoss: 1.478170394897461\n",
      "Epoch: 1 \tBatch: 326 \tLoss: 1.4698681831359863\n",
      "Epoch: 1 \tBatch: 327 \tLoss: 1.4855895042419434\n",
      "Epoch: 1 \tBatch: 328 \tLoss: 1.4944263696670532\n",
      "Epoch: 1 \tBatch: 329 \tLoss: 1.5045568943023682\n",
      "Epoch: 1 \tBatch: 330 \tLoss: 1.4916183948516846\n",
      "Epoch: 1 \tBatch: 331 \tLoss: 1.5043623447418213\n",
      "Epoch: 1 \tBatch: 332 \tLoss: 1.4804178476333618\n",
      "Epoch: 1 \tBatch: 333 \tLoss: 1.4848051071166992\n",
      "Epoch: 1 \tBatch: 334 \tLoss: 1.4971754550933838\n",
      "Epoch: 1 \tBatch: 335 \tLoss: 1.4868799448013306\n",
      "Epoch: 1 \tBatch: 336 \tLoss: 1.470826506614685\n",
      "Epoch: 1 \tBatch: 337 \tLoss: 1.5001806020736694\n",
      "Epoch: 1 \tBatch: 338 \tLoss: 1.5264579057693481\n",
      "Epoch: 1 \tBatch: 339 \tLoss: 1.498066782951355\n",
      "Epoch: 1 \tBatch: 340 \tLoss: 1.4951541423797607\n",
      "Epoch: 1 \tBatch: 341 \tLoss: 1.4891202449798584\n",
      "Epoch: 1 \tBatch: 342 \tLoss: 1.49237859249115\n",
      "Epoch: 1 \tBatch: 343 \tLoss: 1.4918131828308105\n",
      "Epoch: 1 \tBatch: 344 \tLoss: 1.483633041381836\n",
      "Epoch: 1 \tBatch: 345 \tLoss: 1.4966318607330322\n",
      "Epoch: 1 \tBatch: 346 \tLoss: 1.4835582971572876\n",
      "Epoch: 1 \tBatch: 347 \tLoss: 1.4904658794403076\n",
      "Epoch: 1 \tBatch: 348 \tLoss: 1.4734090566635132\n",
      "Epoch: 1 \tBatch: 349 \tLoss: 1.4724982976913452\n",
      "Epoch: 1 \tBatch: 350 \tLoss: 1.485945224761963\n",
      "Epoch: 1 \tBatch: 351 \tLoss: 1.4888267517089844\n",
      "Epoch: 1 \tBatch: 352 \tLoss: 1.4786896705627441\n",
      "Epoch: 1 \tBatch: 353 \tLoss: 1.4801838397979736\n",
      "Epoch: 1 \tBatch: 354 \tLoss: 1.501950740814209\n",
      "Epoch: 1 \tBatch: 355 \tLoss: 1.4914069175720215\n",
      "Epoch: 1 \tBatch: 356 \tLoss: 1.4778536558151245\n",
      "Epoch: 1 \tBatch: 357 \tLoss: 1.5087220668792725\n",
      "Epoch: 1 \tBatch: 358 \tLoss: 1.4827097654342651\n",
      "Epoch: 1 \tBatch: 359 \tLoss: 1.4966846704483032\n",
      "Epoch: 1 \tBatch: 360 \tLoss: 1.47121262550354\n",
      "Epoch: 1 \tBatch: 361 \tLoss: 1.5047876834869385\n",
      "Epoch: 1 \tBatch: 362 \tLoss: 1.4684102535247803\n",
      "Epoch: 1 \tBatch: 363 \tLoss: 1.480892300605774\n",
      "Epoch: 1 \tBatch: 364 \tLoss: 1.5138750076293945\n",
      "Epoch: 1 \tBatch: 365 \tLoss: 1.4675133228302002\n",
      "Epoch: 1 \tBatch: 366 \tLoss: 1.4754841327667236\n",
      "Epoch: 1 \tBatch: 367 \tLoss: 1.5017997026443481\n",
      "Epoch: 1 \tBatch: 368 \tLoss: 1.4912906885147095\n",
      "Epoch: 1 \tBatch: 369 \tLoss: 1.4977768659591675\n",
      "Epoch: 1 \tBatch: 370 \tLoss: 1.4917446374893188\n",
      "Epoch: 1 \tBatch: 371 \tLoss: 1.4757120609283447\n",
      "Epoch: 1 \tBatch: 372 \tLoss: 1.4722566604614258\n",
      "Epoch: 1 \tBatch: 373 \tLoss: 1.4644792079925537\n",
      "Epoch: 1 \tBatch: 374 \tLoss: 1.4723864793777466\n",
      "Epoch: 1 \tBatch: 375 \tLoss: 1.4914207458496094\n",
      "Epoch: 1 \tBatch: 376 \tLoss: 1.4934706687927246\n",
      "Epoch: 1 \tBatch: 377 \tLoss: 1.491346836090088\n",
      "Epoch: 1 \tBatch: 378 \tLoss: 1.4758557081222534\n",
      "Epoch: 1 \tBatch: 379 \tLoss: 1.4831939935684204\n",
      "Epoch: 1 \tBatch: 380 \tLoss: 1.5053369998931885\n",
      "Epoch: 1 \tBatch: 381 \tLoss: 1.4787708520889282\n",
      "Epoch: 1 \tBatch: 382 \tLoss: 1.4728035926818848\n",
      "Epoch: 1 \tBatch: 383 \tLoss: 1.4832655191421509\n",
      "Epoch: 1 \tBatch: 384 \tLoss: 1.4877666234970093\n",
      "Epoch: 1 \tBatch: 385 \tLoss: 1.4978493452072144\n",
      "Epoch: 1 \tBatch: 386 \tLoss: 1.4797779321670532\n",
      "Epoch: 1 \tBatch: 387 \tLoss: 1.4782140254974365\n",
      "Epoch: 1 \tBatch: 388 \tLoss: 1.4869580268859863\n",
      "Epoch: 1 \tBatch: 389 \tLoss: 1.4998449087142944\n",
      "Epoch: 1 \tBatch: 390 \tLoss: 1.4746142625808716\n",
      "Epoch: 2 \tBatch: 0 \tLoss: 1.4801809787750244\n",
      "Epoch: 2 \tBatch: 1 \tLoss: 1.4869967699050903\n",
      "Epoch: 2 \tBatch: 2 \tLoss: 1.4699351787567139\n",
      "Epoch: 2 \tBatch: 3 \tLoss: 1.4783977270126343\n",
      "Epoch: 2 \tBatch: 4 \tLoss: 1.4935922622680664\n",
      "Epoch: 2 \tBatch: 5 \tLoss: 1.4755423069000244\n",
      "Epoch: 2 \tBatch: 6 \tLoss: 1.4898004531860352\n",
      "Epoch: 2 \tBatch: 7 \tLoss: 1.4817731380462646\n",
      "Epoch: 2 \tBatch: 8 \tLoss: 1.496267318725586\n",
      "Epoch: 2 \tBatch: 9 \tLoss: 1.4844582080841064\n",
      "Epoch: 2 \tBatch: 10 \tLoss: 1.4884008169174194\n",
      "Epoch: 2 \tBatch: 11 \tLoss: 1.483066201210022\n",
      "Epoch: 2 \tBatch: 12 \tLoss: 1.4725000858306885\n",
      "Epoch: 2 \tBatch: 13 \tLoss: 1.4836369752883911\n",
      "Epoch: 2 \tBatch: 14 \tLoss: 1.4731335639953613\n",
      "Epoch: 2 \tBatch: 15 \tLoss: 1.4776822328567505\n",
      "Epoch: 2 \tBatch: 16 \tLoss: 1.466664433479309\n",
      "Epoch: 2 \tBatch: 17 \tLoss: 1.4893341064453125\n",
      "Epoch: 2 \tBatch: 18 \tLoss: 1.4759246110916138\n",
      "Epoch: 2 \tBatch: 19 \tLoss: 1.4841820001602173\n",
      "Epoch: 2 \tBatch: 20 \tLoss: 1.471980333328247\n",
      "Epoch: 2 \tBatch: 21 \tLoss: 1.4788079261779785\n",
      "Epoch: 2 \tBatch: 22 \tLoss: 1.4633954763412476\n",
      "Epoch: 2 \tBatch: 23 \tLoss: 1.4857611656188965\n",
      "Epoch: 2 \tBatch: 24 \tLoss: 1.489073395729065\n",
      "Epoch: 2 \tBatch: 25 \tLoss: 1.489950180053711\n",
      "Epoch: 2 \tBatch: 26 \tLoss: 1.4773534536361694\n",
      "Epoch: 2 \tBatch: 27 \tLoss: 1.4764494895935059\n",
      "Epoch: 2 \tBatch: 28 \tLoss: 1.4800188541412354\n",
      "Epoch: 2 \tBatch: 29 \tLoss: 1.479592204093933\n",
      "Epoch: 2 \tBatch: 30 \tLoss: 1.49636971950531\n",
      "Epoch: 2 \tBatch: 31 \tLoss: 1.4931031465530396\n",
      "Epoch: 2 \tBatch: 32 \tLoss: 1.5029462575912476\n",
      "Epoch: 2 \tBatch: 33 \tLoss: 1.498726725578308\n",
      "Epoch: 2 \tBatch: 34 \tLoss: 1.4697877168655396\n",
      "Epoch: 2 \tBatch: 35 \tLoss: 1.4741575717926025\n",
      "Epoch: 2 \tBatch: 36 \tLoss: 1.481071949005127\n",
      "Epoch: 2 \tBatch: 37 \tLoss: 1.4873552322387695\n",
      "Epoch: 2 \tBatch: 38 \tLoss: 1.4776990413665771\n",
      "Epoch: 2 \tBatch: 39 \tLoss: 1.4640603065490723\n",
      "Epoch: 2 \tBatch: 40 \tLoss: 1.5073047876358032\n",
      "Epoch: 2 \tBatch: 41 \tLoss: 1.4827908277511597\n",
      "Epoch: 2 \tBatch: 42 \tLoss: 1.4723559617996216\n",
      "Epoch: 2 \tBatch: 43 \tLoss: 1.4688035249710083\n",
      "Epoch: 2 \tBatch: 44 \tLoss: 1.4940645694732666\n",
      "Epoch: 2 \tBatch: 45 \tLoss: 1.48960542678833\n",
      "Epoch: 2 \tBatch: 46 \tLoss: 1.4848188161849976\n",
      "Epoch: 2 \tBatch: 47 \tLoss: 1.4779268503189087\n",
      "Epoch: 2 \tBatch: 48 \tLoss: 1.4840364456176758\n",
      "Epoch: 2 \tBatch: 49 \tLoss: 1.4836539030075073\n",
      "Epoch: 2 \tBatch: 50 \tLoss: 1.4829480648040771\n",
      "Epoch: 2 \tBatch: 51 \tLoss: 1.478039026260376\n",
      "Epoch: 2 \tBatch: 52 \tLoss: 1.475536823272705\n",
      "Epoch: 2 \tBatch: 53 \tLoss: 1.4966405630111694\n",
      "Epoch: 2 \tBatch: 54 \tLoss: 1.4812041521072388\n",
      "Epoch: 2 \tBatch: 55 \tLoss: 1.4974017143249512\n",
      "Epoch: 2 \tBatch: 56 \tLoss: 1.484187126159668\n",
      "Epoch: 2 \tBatch: 57 \tLoss: 1.4921069145202637\n",
      "Epoch: 2 \tBatch: 58 \tLoss: 1.4844847917556763\n",
      "Epoch: 2 \tBatch: 59 \tLoss: 1.474334955215454\n",
      "Epoch: 2 \tBatch: 60 \tLoss: 1.4838252067565918\n",
      "Epoch: 2 \tBatch: 61 \tLoss: 1.4794807434082031\n",
      "Epoch: 2 \tBatch: 62 \tLoss: 1.4950857162475586\n",
      "Epoch: 2 \tBatch: 63 \tLoss: 1.4814419746398926\n",
      "Epoch: 2 \tBatch: 64 \tLoss: 1.479588508605957\n",
      "Epoch: 2 \tBatch: 65 \tLoss: 1.4843946695327759\n",
      "Epoch: 2 \tBatch: 66 \tLoss: 1.4770346879959106\n",
      "Epoch: 2 \tBatch: 67 \tLoss: 1.4873979091644287\n",
      "Epoch: 2 \tBatch: 68 \tLoss: 1.4820384979248047\n",
      "Epoch: 2 \tBatch: 69 \tLoss: 1.5169858932495117\n",
      "Epoch: 2 \tBatch: 70 \tLoss: 1.4866087436676025\n",
      "Epoch: 2 \tBatch: 71 \tLoss: 1.4857525825500488\n",
      "Epoch: 2 \tBatch: 72 \tLoss: 1.4959073066711426\n",
      "Epoch: 2 \tBatch: 73 \tLoss: 1.47830069065094\n",
      "Epoch: 2 \tBatch: 74 \tLoss: 1.4768916368484497\n",
      "Epoch: 2 \tBatch: 75 \tLoss: 1.4780426025390625\n",
      "Epoch: 2 \tBatch: 76 \tLoss: 1.479175090789795\n",
      "Epoch: 2 \tBatch: 77 \tLoss: 1.4708722829818726\n",
      "Epoch: 2 \tBatch: 78 \tLoss: 1.4892821311950684\n",
      "Epoch: 2 \tBatch: 79 \tLoss: 1.4962232112884521\n",
      "Epoch: 2 \tBatch: 80 \tLoss: 1.4804751873016357\n",
      "Epoch: 2 \tBatch: 81 \tLoss: 1.4861422777175903\n",
      "Epoch: 2 \tBatch: 82 \tLoss: 1.4862247705459595\n",
      "Epoch: 2 \tBatch: 83 \tLoss: 1.4958689212799072\n",
      "Epoch: 2 \tBatch: 84 \tLoss: 1.4929125308990479\n",
      "Epoch: 2 \tBatch: 85 \tLoss: 1.4844077825546265\n",
      "Epoch: 2 \tBatch: 86 \tLoss: 1.521148920059204\n",
      "Epoch: 2 \tBatch: 87 \tLoss: 1.479477047920227\n",
      "Epoch: 2 \tBatch: 88 \tLoss: 1.4785022735595703\n",
      "Epoch: 2 \tBatch: 89 \tLoss: 1.4770393371582031\n",
      "Epoch: 2 \tBatch: 90 \tLoss: 1.49345064163208\n",
      "Epoch: 2 \tBatch: 91 \tLoss: 1.5091831684112549\n",
      "Epoch: 2 \tBatch: 92 \tLoss: 1.5219002962112427\n",
      "Epoch: 2 \tBatch: 93 \tLoss: 1.4830052852630615\n",
      "Epoch: 2 \tBatch: 94 \tLoss: 1.490885853767395\n",
      "Epoch: 2 \tBatch: 95 \tLoss: 1.4767488241195679\n",
      "Epoch: 2 \tBatch: 96 \tLoss: 1.471578598022461\n",
      "Epoch: 2 \tBatch: 97 \tLoss: 1.4800224304199219\n",
      "Epoch: 2 \tBatch: 98 \tLoss: 1.4832630157470703\n",
      "Epoch: 2 \tBatch: 99 \tLoss: 1.4868370294570923\n",
      "Epoch: 2 \tBatch: 100 \tLoss: 1.4957541227340698\n",
      "Epoch: 2 \tBatch: 101 \tLoss: 1.4617700576782227\n",
      "Epoch: 2 \tBatch: 102 \tLoss: 1.4904372692108154\n",
      "Epoch: 2 \tBatch: 103 \tLoss: 1.492501974105835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 104 \tLoss: 1.4794830083847046\n",
      "Epoch: 2 \tBatch: 105 \tLoss: 1.4968507289886475\n",
      "Epoch: 2 \tBatch: 106 \tLoss: 1.4734063148498535\n",
      "Epoch: 2 \tBatch: 107 \tLoss: 1.4752341508865356\n",
      "Epoch: 2 \tBatch: 108 \tLoss: 1.4823784828186035\n",
      "Epoch: 2 \tBatch: 109 \tLoss: 1.4898021221160889\n",
      "Epoch: 2 \tBatch: 110 \tLoss: 1.4615025520324707\n",
      "Epoch: 2 \tBatch: 111 \tLoss: 1.4909194707870483\n",
      "Epoch: 2 \tBatch: 112 \tLoss: 1.4893649816513062\n",
      "Epoch: 2 \tBatch: 113 \tLoss: 1.476210117340088\n",
      "Epoch: 2 \tBatch: 114 \tLoss: 1.4908045530319214\n",
      "Epoch: 2 \tBatch: 115 \tLoss: 1.4810725450515747\n",
      "Epoch: 2 \tBatch: 116 \tLoss: 1.4932399988174438\n",
      "Epoch: 2 \tBatch: 117 \tLoss: 1.4695250988006592\n",
      "Epoch: 2 \tBatch: 118 \tLoss: 1.4624707698822021\n",
      "Epoch: 2 \tBatch: 119 \tLoss: 1.4618043899536133\n",
      "Epoch: 2 \tBatch: 120 \tLoss: 1.481270670890808\n",
      "Epoch: 2 \tBatch: 121 \tLoss: 1.4703561067581177\n",
      "Epoch: 2 \tBatch: 122 \tLoss: 1.4688477516174316\n",
      "Epoch: 2 \tBatch: 123 \tLoss: 1.4919017553329468\n",
      "Epoch: 2 \tBatch: 124 \tLoss: 1.492050051689148\n",
      "Epoch: 2 \tBatch: 125 \tLoss: 1.4670623540878296\n",
      "Epoch: 2 \tBatch: 126 \tLoss: 1.5178791284561157\n",
      "Epoch: 2 \tBatch: 127 \tLoss: 1.4789295196533203\n",
      "Epoch: 2 \tBatch: 128 \tLoss: 1.4682238101959229\n",
      "Epoch: 2 \tBatch: 129 \tLoss: 1.4910050630569458\n",
      "Epoch: 2 \tBatch: 130 \tLoss: 1.4843693971633911\n",
      "Epoch: 2 \tBatch: 131 \tLoss: 1.4622222185134888\n",
      "Epoch: 2 \tBatch: 132 \tLoss: 1.46885347366333\n",
      "Epoch: 2 \tBatch: 133 \tLoss: 1.477763295173645\n",
      "Epoch: 2 \tBatch: 134 \tLoss: 1.4757155179977417\n",
      "Epoch: 2 \tBatch: 135 \tLoss: 1.473253607749939\n",
      "Epoch: 2 \tBatch: 136 \tLoss: 1.4836938381195068\n",
      "Epoch: 2 \tBatch: 137 \tLoss: 1.4696966409683228\n",
      "Epoch: 2 \tBatch: 138 \tLoss: 1.4779670238494873\n",
      "Epoch: 2 \tBatch: 139 \tLoss: 1.475400686264038\n",
      "Epoch: 2 \tBatch: 140 \tLoss: 1.483303427696228\n",
      "Epoch: 2 \tBatch: 141 \tLoss: 1.486020803451538\n",
      "Epoch: 2 \tBatch: 142 \tLoss: 1.496123194694519\n",
      "Epoch: 2 \tBatch: 143 \tLoss: 1.4705010652542114\n",
      "Epoch: 2 \tBatch: 144 \tLoss: 1.4889099597930908\n",
      "Epoch: 2 \tBatch: 145 \tLoss: 1.5062472820281982\n",
      "Epoch: 2 \tBatch: 146 \tLoss: 1.4636152982711792\n",
      "Epoch: 2 \tBatch: 147 \tLoss: 1.494584560394287\n",
      "Epoch: 2 \tBatch: 148 \tLoss: 1.4734644889831543\n",
      "Epoch: 2 \tBatch: 149 \tLoss: 1.4940941333770752\n",
      "Epoch: 2 \tBatch: 150 \tLoss: 1.4957880973815918\n",
      "Epoch: 2 \tBatch: 151 \tLoss: 1.4741407632827759\n",
      "Epoch: 2 \tBatch: 152 \tLoss: 1.5095832347869873\n",
      "Epoch: 2 \tBatch: 153 \tLoss: 1.4652904272079468\n",
      "Epoch: 2 \tBatch: 154 \tLoss: 1.4908697605133057\n",
      "Epoch: 2 \tBatch: 155 \tLoss: 1.4955672025680542\n",
      "Epoch: 2 \tBatch: 156 \tLoss: 1.4794259071350098\n",
      "Epoch: 2 \tBatch: 157 \tLoss: 1.4907764196395874\n",
      "Epoch: 2 \tBatch: 158 \tLoss: 1.4650375843048096\n",
      "Epoch: 2 \tBatch: 159 \tLoss: 1.473089337348938\n",
      "Epoch: 2 \tBatch: 160 \tLoss: 1.4742375612258911\n",
      "Epoch: 2 \tBatch: 161 \tLoss: 1.461940884590149\n",
      "Epoch: 2 \tBatch: 162 \tLoss: 1.4718499183654785\n",
      "Epoch: 2 \tBatch: 163 \tLoss: 1.4778324365615845\n",
      "Epoch: 2 \tBatch: 164 \tLoss: 1.4870706796646118\n",
      "Epoch: 2 \tBatch: 165 \tLoss: 1.4680758714675903\n",
      "Epoch: 2 \tBatch: 166 \tLoss: 1.471570372581482\n",
      "Epoch: 2 \tBatch: 167 \tLoss: 1.4832476377487183\n",
      "Epoch: 2 \tBatch: 168 \tLoss: 1.507763385772705\n",
      "Epoch: 2 \tBatch: 169 \tLoss: 1.4855098724365234\n",
      "Epoch: 2 \tBatch: 170 \tLoss: 1.4780592918395996\n",
      "Epoch: 2 \tBatch: 171 \tLoss: 1.4794772863388062\n",
      "Epoch: 2 \tBatch: 172 \tLoss: 1.4692332744598389\n",
      "Epoch: 2 \tBatch: 173 \tLoss: 1.4703071117401123\n",
      "Epoch: 2 \tBatch: 174 \tLoss: 1.4689316749572754\n",
      "Epoch: 2 \tBatch: 175 \tLoss: 1.483682632446289\n",
      "Epoch: 2 \tBatch: 176 \tLoss: 1.4686899185180664\n",
      "Epoch: 2 \tBatch: 177 \tLoss: 1.4798059463500977\n",
      "Epoch: 2 \tBatch: 178 \tLoss: 1.4828088283538818\n",
      "Epoch: 2 \tBatch: 179 \tLoss: 1.4801067113876343\n",
      "Epoch: 2 \tBatch: 180 \tLoss: 1.4679346084594727\n",
      "Epoch: 2 \tBatch: 181 \tLoss: 1.46573805809021\n",
      "Epoch: 2 \tBatch: 182 \tLoss: 1.4891390800476074\n",
      "Epoch: 2 \tBatch: 183 \tLoss: 1.473893642425537\n",
      "Epoch: 2 \tBatch: 184 \tLoss: 1.5009890794754028\n",
      "Epoch: 2 \tBatch: 185 \tLoss: 1.4747655391693115\n",
      "Epoch: 2 \tBatch: 186 \tLoss: 1.481360673904419\n",
      "Epoch: 2 \tBatch: 187 \tLoss: 1.489807367324829\n",
      "Epoch: 2 \tBatch: 188 \tLoss: 1.468144178390503\n",
      "Epoch: 2 \tBatch: 189 \tLoss: 1.4723398685455322\n",
      "Epoch: 2 \tBatch: 190 \tLoss: 1.4645907878875732\n",
      "Epoch: 2 \tBatch: 191 \tLoss: 1.4881536960601807\n",
      "Epoch: 2 \tBatch: 192 \tLoss: 1.4885730743408203\n",
      "Epoch: 2 \tBatch: 193 \tLoss: 1.4631792306900024\n",
      "Epoch: 2 \tBatch: 194 \tLoss: 1.4730198383331299\n",
      "Epoch: 2 \tBatch: 195 \tLoss: 1.497504472732544\n",
      "Epoch: 2 \tBatch: 196 \tLoss: 1.4675066471099854\n",
      "Epoch: 2 \tBatch: 197 \tLoss: 1.4911589622497559\n",
      "Epoch: 2 \tBatch: 198 \tLoss: 1.475394606590271\n",
      "Epoch: 2 \tBatch: 199 \tLoss: 1.4730231761932373\n",
      "Epoch: 2 \tBatch: 200 \tLoss: 1.479016900062561\n",
      "Epoch: 2 \tBatch: 201 \tLoss: 1.4837313890457153\n",
      "Epoch: 2 \tBatch: 202 \tLoss: 1.4797420501708984\n",
      "Epoch: 2 \tBatch: 203 \tLoss: 1.4723899364471436\n",
      "Epoch: 2 \tBatch: 204 \tLoss: 1.4823546409606934\n",
      "Epoch: 2 \tBatch: 205 \tLoss: 1.4698110818862915\n",
      "Epoch: 2 \tBatch: 206 \tLoss: 1.4822852611541748\n",
      "Epoch: 2 \tBatch: 207 \tLoss: 1.4881680011749268\n",
      "Epoch: 2 \tBatch: 208 \tLoss: 1.4691389799118042\n",
      "Epoch: 2 \tBatch: 209 \tLoss: 1.462674617767334\n",
      "Epoch: 2 \tBatch: 210 \tLoss: 1.47056245803833\n",
      "Epoch: 2 \tBatch: 211 \tLoss: 1.4851198196411133\n",
      "Epoch: 2 \tBatch: 212 \tLoss: 1.4877995252609253\n",
      "Epoch: 2 \tBatch: 213 \tLoss: 1.4789175987243652\n",
      "Epoch: 2 \tBatch: 214 \tLoss: 1.4855320453643799\n",
      "Epoch: 2 \tBatch: 215 \tLoss: 1.463690996170044\n",
      "Epoch: 2 \tBatch: 216 \tLoss: 1.4772361516952515\n",
      "Epoch: 2 \tBatch: 217 \tLoss: 1.486034870147705\n",
      "Epoch: 2 \tBatch: 218 \tLoss: 1.4780728816986084\n",
      "Epoch: 2 \tBatch: 219 \tLoss: 1.4863958358764648\n",
      "Epoch: 2 \tBatch: 220 \tLoss: 1.4655604362487793\n",
      "Epoch: 2 \tBatch: 221 \tLoss: 1.4976178407669067\n",
      "Epoch: 2 \tBatch: 222 \tLoss: 1.4927181005477905\n",
      "Epoch: 2 \tBatch: 223 \tLoss: 1.4917947053909302\n",
      "Epoch: 2 \tBatch: 224 \tLoss: 1.4972972869873047\n",
      "Epoch: 2 \tBatch: 225 \tLoss: 1.484184741973877\n",
      "Epoch: 2 \tBatch: 226 \tLoss: 1.483275055885315\n",
      "Epoch: 2 \tBatch: 227 \tLoss: 1.5000157356262207\n",
      "Epoch: 2 \tBatch: 228 \tLoss: 1.4828754663467407\n",
      "Epoch: 2 \tBatch: 229 \tLoss: 1.4688512086868286\n",
      "Epoch: 2 \tBatch: 230 \tLoss: 1.4838509559631348\n",
      "Epoch: 2 \tBatch: 231 \tLoss: 1.4859397411346436\n",
      "Epoch: 2 \tBatch: 232 \tLoss: 1.5060524940490723\n",
      "Epoch: 2 \tBatch: 233 \tLoss: 1.4894311428070068\n",
      "Epoch: 2 \tBatch: 234 \tLoss: 1.485077977180481\n",
      "Epoch: 2 \tBatch: 235 \tLoss: 1.461724877357483\n",
      "Epoch: 2 \tBatch: 236 \tLoss: 1.4911067485809326\n",
      "Epoch: 2 \tBatch: 237 \tLoss: 1.4774671792984009\n",
      "Epoch: 2 \tBatch: 238 \tLoss: 1.480650544166565\n",
      "Epoch: 2 \tBatch: 239 \tLoss: 1.4787431955337524\n",
      "Epoch: 2 \tBatch: 240 \tLoss: 1.4666838645935059\n",
      "Epoch: 2 \tBatch: 241 \tLoss: 1.4897061586380005\n",
      "Epoch: 2 \tBatch: 242 \tLoss: 1.485208511352539\n",
      "Epoch: 2 \tBatch: 243 \tLoss: 1.4797637462615967\n",
      "Epoch: 2 \tBatch: 244 \tLoss: 1.476683497428894\n",
      "Epoch: 2 \tBatch: 245 \tLoss: 1.4761238098144531\n",
      "Epoch: 2 \tBatch: 246 \tLoss: 1.483423113822937\n",
      "Epoch: 2 \tBatch: 247 \tLoss: 1.4928210973739624\n",
      "Epoch: 2 \tBatch: 248 \tLoss: 1.4636404514312744\n",
      "Epoch: 2 \tBatch: 249 \tLoss: 1.473263144493103\n",
      "Epoch: 2 \tBatch: 250 \tLoss: 1.4632112979888916\n",
      "Epoch: 2 \tBatch: 251 \tLoss: 1.4746376276016235\n",
      "Epoch: 2 \tBatch: 252 \tLoss: 1.4838736057281494\n",
      "Epoch: 2 \tBatch: 253 \tLoss: 1.4719067811965942\n",
      "Epoch: 2 \tBatch: 254 \tLoss: 1.4803158044815063\n",
      "Epoch: 2 \tBatch: 255 \tLoss: 1.4711837768554688\n",
      "Epoch: 2 \tBatch: 256 \tLoss: 1.4642339944839478\n",
      "Epoch: 2 \tBatch: 257 \tLoss: 1.4713337421417236\n",
      "Epoch: 2 \tBatch: 258 \tLoss: 1.4740486145019531\n",
      "Epoch: 2 \tBatch: 259 \tLoss: 1.4891444444656372\n",
      "Epoch: 2 \tBatch: 260 \tLoss: 1.4956879615783691\n",
      "Epoch: 2 \tBatch: 261 \tLoss: 1.4772377014160156\n",
      "Epoch: 2 \tBatch: 262 \tLoss: 1.513431191444397\n",
      "Epoch: 2 \tBatch: 263 \tLoss: 1.4778295755386353\n",
      "Epoch: 2 \tBatch: 264 \tLoss: 1.472067952156067\n",
      "Epoch: 2 \tBatch: 265 \tLoss: 1.486878752708435\n",
      "Epoch: 2 \tBatch: 266 \tLoss: 1.5122697353363037\n",
      "Epoch: 2 \tBatch: 267 \tLoss: 1.489692211151123\n",
      "Epoch: 2 \tBatch: 268 \tLoss: 1.476873517036438\n",
      "Epoch: 2 \tBatch: 269 \tLoss: 1.4742815494537354\n",
      "Epoch: 2 \tBatch: 270 \tLoss: 1.4800649881362915\n",
      "Epoch: 2 \tBatch: 271 \tLoss: 1.48699951171875\n",
      "Epoch: 2 \tBatch: 272 \tLoss: 1.490709900856018\n",
      "Epoch: 2 \tBatch: 273 \tLoss: 1.479284405708313\n",
      "Epoch: 2 \tBatch: 274 \tLoss: 1.494026780128479\n",
      "Epoch: 2 \tBatch: 275 \tLoss: 1.4906591176986694\n",
      "Epoch: 2 \tBatch: 276 \tLoss: 1.4788262844085693\n",
      "Epoch: 2 \tBatch: 277 \tLoss: 1.4855663776397705\n",
      "Epoch: 2 \tBatch: 278 \tLoss: 1.4905543327331543\n",
      "Epoch: 2 \tBatch: 279 \tLoss: 1.5064318180084229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 280 \tLoss: 1.4721254110336304\n",
      "Epoch: 2 \tBatch: 281 \tLoss: 1.5044232606887817\n",
      "Epoch: 2 \tBatch: 282 \tLoss: 1.4749666452407837\n",
      "Epoch: 2 \tBatch: 283 \tLoss: 1.480296015739441\n",
      "Epoch: 2 \tBatch: 284 \tLoss: 1.4627007246017456\n",
      "Epoch: 2 \tBatch: 285 \tLoss: 1.4796613454818726\n",
      "Epoch: 2 \tBatch: 286 \tLoss: 1.4879428148269653\n",
      "Epoch: 2 \tBatch: 287 \tLoss: 1.4709537029266357\n",
      "Epoch: 2 \tBatch: 288 \tLoss: 1.4835783243179321\n",
      "Epoch: 2 \tBatch: 289 \tLoss: 1.4952327013015747\n",
      "Epoch: 2 \tBatch: 290 \tLoss: 1.4774657487869263\n",
      "Epoch: 2 \tBatch: 291 \tLoss: 1.4731324911117554\n",
      "Epoch: 2 \tBatch: 292 \tLoss: 1.4617807865142822\n",
      "Epoch: 2 \tBatch: 293 \tLoss: 1.4763169288635254\n",
      "Epoch: 2 \tBatch: 294 \tLoss: 1.4882967472076416\n",
      "Epoch: 2 \tBatch: 295 \tLoss: 1.4782402515411377\n",
      "Epoch: 2 \tBatch: 296 \tLoss: 1.4911203384399414\n",
      "Epoch: 2 \tBatch: 297 \tLoss: 1.5028622150421143\n",
      "Epoch: 2 \tBatch: 298 \tLoss: 1.4735734462738037\n",
      "Epoch: 2 \tBatch: 299 \tLoss: 1.4735013246536255\n",
      "Epoch: 2 \tBatch: 300 \tLoss: 1.4645079374313354\n",
      "Epoch: 2 \tBatch: 301 \tLoss: 1.47899329662323\n",
      "Epoch: 2 \tBatch: 302 \tLoss: 1.4956289529800415\n",
      "Epoch: 2 \tBatch: 303 \tLoss: 1.4957990646362305\n",
      "Epoch: 2 \tBatch: 304 \tLoss: 1.4846419095993042\n",
      "Epoch: 2 \tBatch: 305 \tLoss: 1.4864667654037476\n",
      "Epoch: 2 \tBatch: 306 \tLoss: 1.477981448173523\n",
      "Epoch: 2 \tBatch: 307 \tLoss: 1.4725420475006104\n",
      "Epoch: 2 \tBatch: 308 \tLoss: 1.4839898347854614\n",
      "Epoch: 2 \tBatch: 309 \tLoss: 1.4693801403045654\n",
      "Epoch: 2 \tBatch: 310 \tLoss: 1.4898204803466797\n",
      "Epoch: 2 \tBatch: 311 \tLoss: 1.4739718437194824\n",
      "Epoch: 2 \tBatch: 312 \tLoss: 1.4867950677871704\n",
      "Epoch: 2 \tBatch: 313 \tLoss: 1.4816581010818481\n",
      "Epoch: 2 \tBatch: 314 \tLoss: 1.4928004741668701\n",
      "Epoch: 2 \tBatch: 315 \tLoss: 1.4806678295135498\n",
      "Epoch: 2 \tBatch: 316 \tLoss: 1.5006123781204224\n",
      "Epoch: 2 \tBatch: 317 \tLoss: 1.485719919204712\n",
      "Epoch: 2 \tBatch: 318 \tLoss: 1.4640390872955322\n",
      "Epoch: 2 \tBatch: 319 \tLoss: 1.4789940118789673\n",
      "Epoch: 2 \tBatch: 320 \tLoss: 1.4805554151535034\n",
      "Epoch: 2 \tBatch: 321 \tLoss: 1.4873027801513672\n",
      "Epoch: 2 \tBatch: 322 \tLoss: 1.463207721710205\n",
      "Epoch: 2 \tBatch: 323 \tLoss: 1.476206660270691\n",
      "Epoch: 2 \tBatch: 324 \tLoss: 1.4952853918075562\n",
      "Epoch: 2 \tBatch: 325 \tLoss: 1.4703410863876343\n",
      "Epoch: 2 \tBatch: 326 \tLoss: 1.4993791580200195\n",
      "Epoch: 2 \tBatch: 327 \tLoss: 1.477480411529541\n",
      "Epoch: 2 \tBatch: 328 \tLoss: 1.480454683303833\n",
      "Epoch: 2 \tBatch: 329 \tLoss: 1.4920003414154053\n",
      "Epoch: 2 \tBatch: 330 \tLoss: 1.4698731899261475\n",
      "Epoch: 2 \tBatch: 331 \tLoss: 1.4730644226074219\n",
      "Epoch: 2 \tBatch: 332 \tLoss: 1.479993224143982\n",
      "Epoch: 2 \tBatch: 333 \tLoss: 1.4693354368209839\n",
      "Epoch: 2 \tBatch: 334 \tLoss: 1.479897141456604\n",
      "Epoch: 2 \tBatch: 335 \tLoss: 1.482422113418579\n",
      "Epoch: 2 \tBatch: 336 \tLoss: 1.4763412475585938\n",
      "Epoch: 2 \tBatch: 337 \tLoss: 1.4849398136138916\n",
      "Epoch: 2 \tBatch: 338 \tLoss: 1.4700970649719238\n",
      "Epoch: 2 \tBatch: 339 \tLoss: 1.4885139465332031\n",
      "Epoch: 2 \tBatch: 340 \tLoss: 1.484429121017456\n",
      "Epoch: 2 \tBatch: 341 \tLoss: 1.4882521629333496\n",
      "Epoch: 2 \tBatch: 342 \tLoss: 1.5098108053207397\n",
      "Epoch: 2 \tBatch: 343 \tLoss: 1.472726583480835\n",
      "Epoch: 2 \tBatch: 344 \tLoss: 1.474410057067871\n",
      "Epoch: 2 \tBatch: 345 \tLoss: 1.4859271049499512\n",
      "Epoch: 2 \tBatch: 346 \tLoss: 1.4686397314071655\n",
      "Epoch: 2 \tBatch: 347 \tLoss: 1.4864176511764526\n",
      "Epoch: 2 \tBatch: 348 \tLoss: 1.467239260673523\n",
      "Epoch: 2 \tBatch: 349 \tLoss: 1.5015957355499268\n",
      "Epoch: 2 \tBatch: 350 \tLoss: 1.4992395639419556\n",
      "Epoch: 2 \tBatch: 351 \tLoss: 1.4907994270324707\n",
      "Epoch: 2 \tBatch: 352 \tLoss: 1.4929625988006592\n",
      "Epoch: 2 \tBatch: 353 \tLoss: 1.4666301012039185\n",
      "Epoch: 2 \tBatch: 354 \tLoss: 1.5017588138580322\n",
      "Epoch: 2 \tBatch: 355 \tLoss: 1.4701588153839111\n",
      "Epoch: 2 \tBatch: 356 \tLoss: 1.476843237876892\n",
      "Epoch: 2 \tBatch: 357 \tLoss: 1.4783880710601807\n",
      "Epoch: 2 \tBatch: 358 \tLoss: 1.4749424457550049\n",
      "Epoch: 2 \tBatch: 359 \tLoss: 1.492246389389038\n",
      "Epoch: 2 \tBatch: 360 \tLoss: 1.4778168201446533\n",
      "Epoch: 2 \tBatch: 361 \tLoss: 1.4771466255187988\n",
      "Epoch: 2 \tBatch: 362 \tLoss: 1.4876670837402344\n",
      "Epoch: 2 \tBatch: 363 \tLoss: 1.4661245346069336\n",
      "Epoch: 2 \tBatch: 364 \tLoss: 1.5010795593261719\n",
      "Epoch: 2 \tBatch: 365 \tLoss: 1.4719436168670654\n",
      "Epoch: 2 \tBatch: 366 \tLoss: 1.4728772640228271\n",
      "Epoch: 2 \tBatch: 367 \tLoss: 1.4681140184402466\n",
      "Epoch: 2 \tBatch: 368 \tLoss: 1.5130093097686768\n",
      "Epoch: 2 \tBatch: 369 \tLoss: 1.47704017162323\n",
      "Epoch: 2 \tBatch: 370 \tLoss: 1.480517029762268\n",
      "Epoch: 2 \tBatch: 371 \tLoss: 1.4942164421081543\n",
      "Epoch: 2 \tBatch: 372 \tLoss: 1.4685105085372925\n",
      "Epoch: 2 \tBatch: 373 \tLoss: 1.467154860496521\n",
      "Epoch: 2 \tBatch: 374 \tLoss: 1.4814646244049072\n",
      "Epoch: 2 \tBatch: 375 \tLoss: 1.4880695343017578\n",
      "Epoch: 2 \tBatch: 376 \tLoss: 1.476280689239502\n",
      "Epoch: 2 \tBatch: 377 \tLoss: 1.4976516962051392\n",
      "Epoch: 2 \tBatch: 378 \tLoss: 1.5007374286651611\n",
      "Epoch: 2 \tBatch: 379 \tLoss: 1.4834855794906616\n",
      "Epoch: 2 \tBatch: 380 \tLoss: 1.4841117858886719\n",
      "Epoch: 2 \tBatch: 381 \tLoss: 1.4643791913986206\n",
      "Epoch: 2 \tBatch: 382 \tLoss: 1.4777441024780273\n",
      "Epoch: 2 \tBatch: 383 \tLoss: 1.470263123512268\n",
      "Epoch: 2 \tBatch: 384 \tLoss: 1.4768199920654297\n",
      "Epoch: 2 \tBatch: 385 \tLoss: 1.4885355234146118\n",
      "Epoch: 2 \tBatch: 386 \tLoss: 1.4812133312225342\n",
      "Epoch: 2 \tBatch: 387 \tLoss: 1.5209360122680664\n",
      "Epoch: 2 \tBatch: 388 \tLoss: 1.4901132583618164\n",
      "Epoch: 2 \tBatch: 389 \tLoss: 1.4883906841278076\n",
      "Epoch: 2 \tBatch: 390 \tLoss: 1.4685583114624023\n",
      "Epoch: 3 \tBatch: 0 \tLoss: 1.4790360927581787\n",
      "Epoch: 3 \tBatch: 1 \tLoss: 1.4743680953979492\n",
      "Epoch: 3 \tBatch: 2 \tLoss: 1.4653245210647583\n",
      "Epoch: 3 \tBatch: 3 \tLoss: 1.478074073791504\n",
      "Epoch: 3 \tBatch: 4 \tLoss: 1.4784141778945923\n",
      "Epoch: 3 \tBatch: 5 \tLoss: 1.4883290529251099\n",
      "Epoch: 3 \tBatch: 6 \tLoss: 1.4661445617675781\n",
      "Epoch: 3 \tBatch: 7 \tLoss: 1.4801831245422363\n",
      "Epoch: 3 \tBatch: 8 \tLoss: 1.4694072008132935\n",
      "Epoch: 3 \tBatch: 9 \tLoss: 1.469411015510559\n",
      "Epoch: 3 \tBatch: 10 \tLoss: 1.4745700359344482\n",
      "Epoch: 3 \tBatch: 11 \tLoss: 1.4986286163330078\n",
      "Epoch: 3 \tBatch: 12 \tLoss: 1.4757705926895142\n",
      "Epoch: 3 \tBatch: 13 \tLoss: 1.5006747245788574\n",
      "Epoch: 3 \tBatch: 14 \tLoss: 1.4699451923370361\n",
      "Epoch: 3 \tBatch: 15 \tLoss: 1.4689825773239136\n",
      "Epoch: 3 \tBatch: 16 \tLoss: 1.5034335851669312\n",
      "Epoch: 3 \tBatch: 17 \tLoss: 1.4973589181900024\n",
      "Epoch: 3 \tBatch: 18 \tLoss: 1.4730861186981201\n",
      "Epoch: 3 \tBatch: 19 \tLoss: 1.4872550964355469\n",
      "Epoch: 3 \tBatch: 20 \tLoss: 1.4727191925048828\n",
      "Epoch: 3 \tBatch: 21 \tLoss: 1.470698595046997\n",
      "Epoch: 3 \tBatch: 22 \tLoss: 1.4677084684371948\n",
      "Epoch: 3 \tBatch: 23 \tLoss: 1.4842052459716797\n",
      "Epoch: 3 \tBatch: 24 \tLoss: 1.481748104095459\n",
      "Epoch: 3 \tBatch: 25 \tLoss: 1.476596713066101\n",
      "Epoch: 3 \tBatch: 26 \tLoss: 1.493699550628662\n",
      "Epoch: 3 \tBatch: 27 \tLoss: 1.4810196161270142\n",
      "Epoch: 3 \tBatch: 28 \tLoss: 1.4852995872497559\n",
      "Epoch: 3 \tBatch: 29 \tLoss: 1.469415307044983\n",
      "Epoch: 3 \tBatch: 30 \tLoss: 1.4814132452011108\n",
      "Epoch: 3 \tBatch: 31 \tLoss: 1.496896505355835\n",
      "Epoch: 3 \tBatch: 32 \tLoss: 1.4719855785369873\n",
      "Epoch: 3 \tBatch: 33 \tLoss: 1.471109390258789\n",
      "Epoch: 3 \tBatch: 34 \tLoss: 1.508836269378662\n",
      "Epoch: 3 \tBatch: 35 \tLoss: 1.4843661785125732\n",
      "Epoch: 3 \tBatch: 36 \tLoss: 1.4723464250564575\n",
      "Epoch: 3 \tBatch: 37 \tLoss: 1.4649873971939087\n",
      "Epoch: 3 \tBatch: 38 \tLoss: 1.485514760017395\n",
      "Epoch: 3 \tBatch: 39 \tLoss: 1.4704034328460693\n",
      "Epoch: 3 \tBatch: 40 \tLoss: 1.4635926485061646\n",
      "Epoch: 3 \tBatch: 41 \tLoss: 1.4898273944854736\n",
      "Epoch: 3 \tBatch: 42 \tLoss: 1.461544394493103\n",
      "Epoch: 3 \tBatch: 43 \tLoss: 1.4995510578155518\n",
      "Epoch: 3 \tBatch: 44 \tLoss: 1.4691495895385742\n",
      "Epoch: 3 \tBatch: 45 \tLoss: 1.4977415800094604\n",
      "Epoch: 3 \tBatch: 46 \tLoss: 1.474264144897461\n",
      "Epoch: 3 \tBatch: 47 \tLoss: 1.4823914766311646\n",
      "Epoch: 3 \tBatch: 48 \tLoss: 1.4706059694290161\n",
      "Epoch: 3 \tBatch: 49 \tLoss: 1.4658799171447754\n",
      "Epoch: 3 \tBatch: 50 \tLoss: 1.481895089149475\n",
      "Epoch: 3 \tBatch: 51 \tLoss: 1.4619016647338867\n",
      "Epoch: 3 \tBatch: 52 \tLoss: 1.464953899383545\n",
      "Epoch: 3 \tBatch: 53 \tLoss: 1.4788305759429932\n",
      "Epoch: 3 \tBatch: 54 \tLoss: 1.4845699071884155\n",
      "Epoch: 3 \tBatch: 55 \tLoss: 1.4713960886001587\n",
      "Epoch: 3 \tBatch: 56 \tLoss: 1.4721311330795288\n",
      "Epoch: 3 \tBatch: 57 \tLoss: 1.467609167098999\n",
      "Epoch: 3 \tBatch: 58 \tLoss: 1.4767554998397827\n",
      "Epoch: 3 \tBatch: 59 \tLoss: 1.469970464706421\n",
      "Epoch: 3 \tBatch: 60 \tLoss: 1.493562936782837\n",
      "Epoch: 3 \tBatch: 61 \tLoss: 1.4777907133102417\n",
      "Epoch: 3 \tBatch: 62 \tLoss: 1.484907627105713\n",
      "Epoch: 3 \tBatch: 63 \tLoss: 1.4670541286468506\n",
      "Epoch: 3 \tBatch: 64 \tLoss: 1.465361475944519\n",
      "Epoch: 3 \tBatch: 65 \tLoss: 1.4733058214187622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 66 \tLoss: 1.472416639328003\n",
      "Epoch: 3 \tBatch: 67 \tLoss: 1.483299970626831\n",
      "Epoch: 3 \tBatch: 68 \tLoss: 1.4664599895477295\n",
      "Epoch: 3 \tBatch: 69 \tLoss: 1.4676605463027954\n",
      "Epoch: 3 \tBatch: 70 \tLoss: 1.4874482154846191\n",
      "Epoch: 3 \tBatch: 71 \tLoss: 1.4773831367492676\n",
      "Epoch: 3 \tBatch: 72 \tLoss: 1.485595703125\n",
      "Epoch: 3 \tBatch: 73 \tLoss: 1.4706720113754272\n",
      "Epoch: 3 \tBatch: 74 \tLoss: 1.4755325317382812\n",
      "Epoch: 3 \tBatch: 75 \tLoss: 1.4685062170028687\n",
      "Epoch: 3 \tBatch: 76 \tLoss: 1.4805947542190552\n",
      "Epoch: 3 \tBatch: 77 \tLoss: 1.4858568906784058\n",
      "Epoch: 3 \tBatch: 78 \tLoss: 1.4774038791656494\n",
      "Epoch: 3 \tBatch: 79 \tLoss: 1.5007885694503784\n",
      "Epoch: 3 \tBatch: 80 \tLoss: 1.4830162525177002\n",
      "Epoch: 3 \tBatch: 81 \tLoss: 1.4768098592758179\n",
      "Epoch: 3 \tBatch: 82 \tLoss: 1.4987499713897705\n",
      "Epoch: 3 \tBatch: 83 \tLoss: 1.4624226093292236\n",
      "Epoch: 3 \tBatch: 84 \tLoss: 1.4708383083343506\n",
      "Epoch: 3 \tBatch: 85 \tLoss: 1.468704104423523\n",
      "Epoch: 3 \tBatch: 86 \tLoss: 1.5032600164413452\n",
      "Epoch: 3 \tBatch: 87 \tLoss: 1.461472988128662\n",
      "Epoch: 3 \tBatch: 88 \tLoss: 1.488133430480957\n",
      "Epoch: 3 \tBatch: 89 \tLoss: 1.4772626161575317\n",
      "Epoch: 3 \tBatch: 90 \tLoss: 1.4918190240859985\n",
      "Epoch: 3 \tBatch: 91 \tLoss: 1.4885388612747192\n",
      "Epoch: 3 \tBatch: 92 \tLoss: 1.471746563911438\n",
      "Epoch: 3 \tBatch: 93 \tLoss: 1.4693714380264282\n",
      "Epoch: 3 \tBatch: 94 \tLoss: 1.480363130569458\n",
      "Epoch: 3 \tBatch: 95 \tLoss: 1.4835915565490723\n",
      "Epoch: 3 \tBatch: 96 \tLoss: 1.461187720298767\n",
      "Epoch: 3 \tBatch: 97 \tLoss: 1.48576819896698\n",
      "Epoch: 3 \tBatch: 98 \tLoss: 1.478033423423767\n",
      "Epoch: 3 \tBatch: 99 \tLoss: 1.4696435928344727\n",
      "Epoch: 3 \tBatch: 100 \tLoss: 1.484592080116272\n",
      "Epoch: 3 \tBatch: 101 \tLoss: 1.4736158847808838\n",
      "Epoch: 3 \tBatch: 102 \tLoss: 1.4686864614486694\n",
      "Epoch: 3 \tBatch: 103 \tLoss: 1.4839576482772827\n",
      "Epoch: 3 \tBatch: 104 \tLoss: 1.4930599927902222\n",
      "Epoch: 3 \tBatch: 105 \tLoss: 1.4740668535232544\n",
      "Epoch: 3 \tBatch: 106 \tLoss: 1.4641181230545044\n",
      "Epoch: 3 \tBatch: 107 \tLoss: 1.4801428318023682\n",
      "Epoch: 3 \tBatch: 108 \tLoss: 1.4707038402557373\n",
      "Epoch: 3 \tBatch: 109 \tLoss: 1.4752541780471802\n",
      "Epoch: 3 \tBatch: 110 \tLoss: 1.4879416227340698\n",
      "Epoch: 3 \tBatch: 111 \tLoss: 1.4617347717285156\n",
      "Epoch: 3 \tBatch: 112 \tLoss: 1.4703288078308105\n",
      "Epoch: 3 \tBatch: 113 \tLoss: 1.4740110635757446\n",
      "Epoch: 3 \tBatch: 114 \tLoss: 1.4629777669906616\n",
      "Epoch: 3 \tBatch: 115 \tLoss: 1.4734883308410645\n",
      "Epoch: 3 \tBatch: 116 \tLoss: 1.4817651510238647\n",
      "Epoch: 3 \tBatch: 117 \tLoss: 1.4718888998031616\n",
      "Epoch: 3 \tBatch: 118 \tLoss: 1.4804089069366455\n",
      "Epoch: 3 \tBatch: 119 \tLoss: 1.4774175882339478\n",
      "Epoch: 3 \tBatch: 120 \tLoss: 1.4916621446609497\n",
      "Epoch: 3 \tBatch: 121 \tLoss: 1.4877264499664307\n",
      "Epoch: 3 \tBatch: 122 \tLoss: 1.4729065895080566\n",
      "Epoch: 3 \tBatch: 123 \tLoss: 1.4768412113189697\n",
      "Epoch: 3 \tBatch: 124 \tLoss: 1.4692575931549072\n",
      "Epoch: 3 \tBatch: 125 \tLoss: 1.4692001342773438\n",
      "Epoch: 3 \tBatch: 126 \tLoss: 1.477817177772522\n",
      "Epoch: 3 \tBatch: 127 \tLoss: 1.490699291229248\n",
      "Epoch: 3 \tBatch: 128 \tLoss: 1.4776159524917603\n",
      "Epoch: 3 \tBatch: 129 \tLoss: 1.4685875177383423\n",
      "Epoch: 3 \tBatch: 130 \tLoss: 1.4695619344711304\n",
      "Epoch: 3 \tBatch: 131 \tLoss: 1.4812790155410767\n",
      "Epoch: 3 \tBatch: 132 \tLoss: 1.4615684747695923\n",
      "Epoch: 3 \tBatch: 133 \tLoss: 1.477709174156189\n",
      "Epoch: 3 \tBatch: 134 \tLoss: 1.4677194356918335\n",
      "Epoch: 3 \tBatch: 135 \tLoss: 1.4748255014419556\n",
      "Epoch: 3 \tBatch: 136 \tLoss: 1.4799437522888184\n",
      "Epoch: 3 \tBatch: 137 \tLoss: 1.4894733428955078\n",
      "Epoch: 3 \tBatch: 138 \tLoss: 1.4876859188079834\n",
      "Epoch: 3 \tBatch: 139 \tLoss: 1.4927746057510376\n",
      "Epoch: 3 \tBatch: 140 \tLoss: 1.4845694303512573\n",
      "Epoch: 3 \tBatch: 141 \tLoss: 1.463517189025879\n",
      "Epoch: 3 \tBatch: 142 \tLoss: 1.4718531370162964\n",
      "Epoch: 3 \tBatch: 143 \tLoss: 1.4798716306686401\n",
      "Epoch: 3 \tBatch: 144 \tLoss: 1.4706624746322632\n",
      "Epoch: 3 \tBatch: 145 \tLoss: 1.4839982986450195\n",
      "Epoch: 3 \tBatch: 146 \tLoss: 1.4851387739181519\n",
      "Epoch: 3 \tBatch: 147 \tLoss: 1.4744750261306763\n",
      "Epoch: 3 \tBatch: 148 \tLoss: 1.4764384031295776\n",
      "Epoch: 3 \tBatch: 149 \tLoss: 1.4693653583526611\n",
      "Epoch: 3 \tBatch: 150 \tLoss: 1.4683427810668945\n",
      "Epoch: 3 \tBatch: 151 \tLoss: 1.4880139827728271\n",
      "Epoch: 3 \tBatch: 152 \tLoss: 1.4885677099227905\n",
      "Epoch: 3 \tBatch: 153 \tLoss: 1.4824644327163696\n",
      "Epoch: 3 \tBatch: 154 \tLoss: 1.4779229164123535\n",
      "Epoch: 3 \tBatch: 155 \tLoss: 1.4902799129486084\n",
      "Epoch: 3 \tBatch: 156 \tLoss: 1.4640311002731323\n",
      "Epoch: 3 \tBatch: 157 \tLoss: 1.4757872819900513\n",
      "Epoch: 3 \tBatch: 158 \tLoss: 1.469150185585022\n",
      "Epoch: 3 \tBatch: 159 \tLoss: 1.4917150735855103\n",
      "Epoch: 3 \tBatch: 160 \tLoss: 1.5007084608078003\n",
      "Epoch: 3 \tBatch: 161 \tLoss: 1.4692871570587158\n",
      "Epoch: 3 \tBatch: 162 \tLoss: 1.4936728477478027\n",
      "Epoch: 3 \tBatch: 163 \tLoss: 1.4718962907791138\n",
      "Epoch: 3 \tBatch: 164 \tLoss: 1.4747763872146606\n",
      "Epoch: 3 \tBatch: 165 \tLoss: 1.4803045988082886\n",
      "Epoch: 3 \tBatch: 166 \tLoss: 1.4805814027786255\n",
      "Epoch: 3 \tBatch: 167 \tLoss: 1.468959927558899\n",
      "Epoch: 3 \tBatch: 168 \tLoss: 1.4691206216812134\n",
      "Epoch: 3 \tBatch: 169 \tLoss: 1.4634466171264648\n",
      "Epoch: 3 \tBatch: 170 \tLoss: 1.469578742980957\n",
      "Epoch: 3 \tBatch: 171 \tLoss: 1.492921233177185\n",
      "Epoch: 3 \tBatch: 172 \tLoss: 1.4698433876037598\n",
      "Epoch: 3 \tBatch: 173 \tLoss: 1.4831149578094482\n",
      "Epoch: 3 \tBatch: 174 \tLoss: 1.4810537099838257\n",
      "Epoch: 3 \tBatch: 175 \tLoss: 1.4746644496917725\n",
      "Epoch: 3 \tBatch: 176 \tLoss: 1.4813200235366821\n",
      "Epoch: 3 \tBatch: 177 \tLoss: 1.4771716594696045\n",
      "Epoch: 3 \tBatch: 178 \tLoss: 1.4969984292984009\n",
      "Epoch: 3 \tBatch: 179 \tLoss: 1.487734317779541\n",
      "Epoch: 3 \tBatch: 180 \tLoss: 1.4643677473068237\n",
      "Epoch: 3 \tBatch: 181 \tLoss: 1.4763357639312744\n",
      "Epoch: 3 \tBatch: 182 \tLoss: 1.4701398611068726\n",
      "Epoch: 3 \tBatch: 183 \tLoss: 1.480075478553772\n",
      "Epoch: 3 \tBatch: 184 \tLoss: 1.4968360662460327\n",
      "Epoch: 3 \tBatch: 185 \tLoss: 1.4862655401229858\n",
      "Epoch: 3 \tBatch: 186 \tLoss: 1.4985402822494507\n",
      "Epoch: 3 \tBatch: 187 \tLoss: 1.4797639846801758\n",
      "Epoch: 3 \tBatch: 188 \tLoss: 1.4716216325759888\n",
      "Epoch: 3 \tBatch: 189 \tLoss: 1.463138461112976\n",
      "Epoch: 3 \tBatch: 190 \tLoss: 1.4845483303070068\n",
      "Epoch: 3 \tBatch: 191 \tLoss: 1.4816477298736572\n",
      "Epoch: 3 \tBatch: 192 \tLoss: 1.4809373617172241\n",
      "Epoch: 3 \tBatch: 193 \tLoss: 1.4743435382843018\n",
      "Epoch: 3 \tBatch: 194 \tLoss: 1.4849923849105835\n",
      "Epoch: 3 \tBatch: 195 \tLoss: 1.4612553119659424\n",
      "Epoch: 3 \tBatch: 196 \tLoss: 1.4903016090393066\n",
      "Epoch: 3 \tBatch: 197 \tLoss: 1.4690040349960327\n",
      "Epoch: 3 \tBatch: 198 \tLoss: 1.4927512407302856\n",
      "Epoch: 3 \tBatch: 199 \tLoss: 1.475278377532959\n",
      "Epoch: 3 \tBatch: 200 \tLoss: 1.4741384983062744\n",
      "Epoch: 3 \tBatch: 201 \tLoss: 1.4835045337677002\n",
      "Epoch: 3 \tBatch: 202 \tLoss: 1.4643113613128662\n",
      "Epoch: 3 \tBatch: 203 \tLoss: 1.4786767959594727\n",
      "Epoch: 3 \tBatch: 204 \tLoss: 1.4942965507507324\n",
      "Epoch: 3 \tBatch: 205 \tLoss: 1.4902880191802979\n",
      "Epoch: 3 \tBatch: 206 \tLoss: 1.474918007850647\n",
      "Epoch: 3 \tBatch: 207 \tLoss: 1.4958044290542603\n",
      "Epoch: 3 \tBatch: 208 \tLoss: 1.4922873973846436\n",
      "Epoch: 3 \tBatch: 209 \tLoss: 1.5067075490951538\n",
      "Epoch: 3 \tBatch: 210 \tLoss: 1.478904366493225\n",
      "Epoch: 3 \tBatch: 211 \tLoss: 1.4776909351348877\n",
      "Epoch: 3 \tBatch: 212 \tLoss: 1.4669861793518066\n",
      "Epoch: 3 \tBatch: 213 \tLoss: 1.4844329357147217\n",
      "Epoch: 3 \tBatch: 214 \tLoss: 1.4858124256134033\n",
      "Epoch: 3 \tBatch: 215 \tLoss: 1.4693102836608887\n",
      "Epoch: 3 \tBatch: 216 \tLoss: 1.4769996404647827\n",
      "Epoch: 3 \tBatch: 217 \tLoss: 1.4798108339309692\n",
      "Epoch: 3 \tBatch: 218 \tLoss: 1.4783259630203247\n",
      "Epoch: 3 \tBatch: 219 \tLoss: 1.4882235527038574\n",
      "Epoch: 3 \tBatch: 220 \tLoss: 1.4763847589492798\n",
      "Epoch: 3 \tBatch: 221 \tLoss: 1.476248860359192\n",
      "Epoch: 3 \tBatch: 222 \tLoss: 1.483224868774414\n",
      "Epoch: 3 \tBatch: 223 \tLoss: 1.4622026681900024\n",
      "Epoch: 3 \tBatch: 224 \tLoss: 1.4623374938964844\n",
      "Epoch: 3 \tBatch: 225 \tLoss: 1.4954688549041748\n",
      "Epoch: 3 \tBatch: 226 \tLoss: 1.473332405090332\n",
      "Epoch: 3 \tBatch: 227 \tLoss: 1.4929848909378052\n",
      "Epoch: 3 \tBatch: 228 \tLoss: 1.4781277179718018\n",
      "Epoch: 3 \tBatch: 229 \tLoss: 1.474271535873413\n",
      "Epoch: 3 \tBatch: 230 \tLoss: 1.4777965545654297\n",
      "Epoch: 3 \tBatch: 231 \tLoss: 1.4854865074157715\n",
      "Epoch: 3 \tBatch: 232 \tLoss: 1.4672162532806396\n",
      "Epoch: 3 \tBatch: 233 \tLoss: 1.4869494438171387\n",
      "Epoch: 3 \tBatch: 234 \tLoss: 1.493890404701233\n",
      "Epoch: 3 \tBatch: 235 \tLoss: 1.4777474403381348\n",
      "Epoch: 3 \tBatch: 236 \tLoss: 1.4622963666915894\n",
      "Epoch: 3 \tBatch: 237 \tLoss: 1.4850715398788452\n",
      "Epoch: 3 \tBatch: 238 \tLoss: 1.4814751148223877\n",
      "Epoch: 3 \tBatch: 239 \tLoss: 1.482189416885376\n",
      "Epoch: 3 \tBatch: 240 \tLoss: 1.4911274909973145\n",
      "Epoch: 3 \tBatch: 241 \tLoss: 1.491650104522705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 242 \tLoss: 1.4631470441818237\n",
      "Epoch: 3 \tBatch: 243 \tLoss: 1.4767448902130127\n",
      "Epoch: 3 \tBatch: 244 \tLoss: 1.4686821699142456\n",
      "Epoch: 3 \tBatch: 245 \tLoss: 1.4776479005813599\n",
      "Epoch: 3 \tBatch: 246 \tLoss: 1.4781948328018188\n",
      "Epoch: 3 \tBatch: 247 \tLoss: 1.4702091217041016\n",
      "Epoch: 3 \tBatch: 248 \tLoss: 1.4721261262893677\n",
      "Epoch: 3 \tBatch: 249 \tLoss: 1.4751652479171753\n",
      "Epoch: 3 \tBatch: 250 \tLoss: 1.504431962966919\n",
      "Epoch: 3 \tBatch: 251 \tLoss: 1.4737296104431152\n",
      "Epoch: 3 \tBatch: 252 \tLoss: 1.482075810432434\n",
      "Epoch: 3 \tBatch: 253 \tLoss: 1.4803149700164795\n",
      "Epoch: 3 \tBatch: 254 \tLoss: 1.4727816581726074\n",
      "Epoch: 3 \tBatch: 255 \tLoss: 1.4846874475479126\n",
      "Epoch: 3 \tBatch: 256 \tLoss: 1.4824999570846558\n",
      "Epoch: 3 \tBatch: 257 \tLoss: 1.4818408489227295\n",
      "Epoch: 3 \tBatch: 258 \tLoss: 1.4769618511199951\n",
      "Epoch: 3 \tBatch: 259 \tLoss: 1.476941466331482\n",
      "Epoch: 3 \tBatch: 260 \tLoss: 1.4688018560409546\n",
      "Epoch: 3 \tBatch: 261 \tLoss: 1.4844698905944824\n",
      "Epoch: 3 \tBatch: 262 \tLoss: 1.4644871950149536\n",
      "Epoch: 3 \tBatch: 263 \tLoss: 1.4695041179656982\n",
      "Epoch: 3 \tBatch: 264 \tLoss: 1.4835035800933838\n",
      "Epoch: 3 \tBatch: 265 \tLoss: 1.4713417291641235\n",
      "Epoch: 3 \tBatch: 266 \tLoss: 1.4689749479293823\n",
      "Epoch: 3 \tBatch: 267 \tLoss: 1.4704378843307495\n",
      "Epoch: 3 \tBatch: 268 \tLoss: 1.4792652130126953\n",
      "Epoch: 3 \tBatch: 269 \tLoss: 1.4812978506088257\n",
      "Epoch: 3 \tBatch: 270 \tLoss: 1.4756543636322021\n",
      "Epoch: 3 \tBatch: 271 \tLoss: 1.4813177585601807\n",
      "Epoch: 3 \tBatch: 272 \tLoss: 1.4764301776885986\n",
      "Epoch: 3 \tBatch: 273 \tLoss: 1.4759198427200317\n",
      "Epoch: 3 \tBatch: 274 \tLoss: 1.4903110265731812\n",
      "Epoch: 3 \tBatch: 275 \tLoss: 1.4888361692428589\n",
      "Epoch: 3 \tBatch: 276 \tLoss: 1.480711817741394\n",
      "Epoch: 3 \tBatch: 277 \tLoss: 1.4781816005706787\n",
      "Epoch: 3 \tBatch: 278 \tLoss: 1.4737563133239746\n",
      "Epoch: 3 \tBatch: 279 \tLoss: 1.492521047592163\n",
      "Epoch: 3 \tBatch: 280 \tLoss: 1.4697628021240234\n",
      "Epoch: 3 \tBatch: 281 \tLoss: 1.4805588722229004\n",
      "Epoch: 3 \tBatch: 282 \tLoss: 1.4779480695724487\n",
      "Epoch: 3 \tBatch: 283 \tLoss: 1.489151120185852\n",
      "Epoch: 3 \tBatch: 284 \tLoss: 1.4709681272506714\n",
      "Epoch: 3 \tBatch: 285 \tLoss: 1.4623509645462036\n",
      "Epoch: 3 \tBatch: 286 \tLoss: 1.4715772867202759\n",
      "Epoch: 3 \tBatch: 287 \tLoss: 1.4835262298583984\n",
      "Epoch: 3 \tBatch: 288 \tLoss: 1.472754716873169\n",
      "Epoch: 3 \tBatch: 289 \tLoss: 1.4852131605148315\n",
      "Epoch: 3 \tBatch: 290 \tLoss: 1.4901257753372192\n",
      "Epoch: 3 \tBatch: 291 \tLoss: 1.4831030368804932\n",
      "Epoch: 3 \tBatch: 292 \tLoss: 1.4712271690368652\n",
      "Epoch: 3 \tBatch: 293 \tLoss: 1.476502537727356\n",
      "Epoch: 3 \tBatch: 294 \tLoss: 1.4829444885253906\n",
      "Epoch: 3 \tBatch: 295 \tLoss: 1.4632731676101685\n",
      "Epoch: 3 \tBatch: 296 \tLoss: 1.4858962297439575\n",
      "Epoch: 3 \tBatch: 297 \tLoss: 1.468814492225647\n",
      "Epoch: 3 \tBatch: 298 \tLoss: 1.4770663976669312\n",
      "Epoch: 3 \tBatch: 299 \tLoss: 1.4631779193878174\n",
      "Epoch: 3 \tBatch: 300 \tLoss: 1.4706591367721558\n",
      "Epoch: 3 \tBatch: 301 \tLoss: 1.4917795658111572\n",
      "Epoch: 3 \tBatch: 302 \tLoss: 1.4890539646148682\n",
      "Epoch: 3 \tBatch: 303 \tLoss: 1.4764118194580078\n",
      "Epoch: 3 \tBatch: 304 \tLoss: 1.496590256690979\n",
      "Epoch: 3 \tBatch: 305 \tLoss: 1.4914354085922241\n",
      "Epoch: 3 \tBatch: 306 \tLoss: 1.4771288633346558\n",
      "Epoch: 3 \tBatch: 307 \tLoss: 1.4748728275299072\n",
      "Epoch: 3 \tBatch: 308 \tLoss: 1.4829936027526855\n",
      "Epoch: 3 \tBatch: 309 \tLoss: 1.4821138381958008\n",
      "Epoch: 3 \tBatch: 310 \tLoss: 1.4775046110153198\n",
      "Epoch: 3 \tBatch: 311 \tLoss: 1.469199538230896\n",
      "Epoch: 3 \tBatch: 312 \tLoss: 1.4838443994522095\n",
      "Epoch: 3 \tBatch: 313 \tLoss: 1.4692002534866333\n",
      "Epoch: 3 \tBatch: 314 \tLoss: 1.4933732748031616\n",
      "Epoch: 3 \tBatch: 315 \tLoss: 1.4782196283340454\n",
      "Epoch: 3 \tBatch: 316 \tLoss: 1.4717998504638672\n",
      "Epoch: 3 \tBatch: 317 \tLoss: 1.4826902151107788\n",
      "Epoch: 3 \tBatch: 318 \tLoss: 1.4763786792755127\n",
      "Epoch: 3 \tBatch: 319 \tLoss: 1.4695192575454712\n",
      "Epoch: 3 \tBatch: 320 \tLoss: 1.5200634002685547\n",
      "Epoch: 3 \tBatch: 321 \tLoss: 1.483078122138977\n",
      "Epoch: 3 \tBatch: 322 \tLoss: 1.4712896347045898\n",
      "Epoch: 3 \tBatch: 323 \tLoss: 1.472024917602539\n",
      "Epoch: 3 \tBatch: 324 \tLoss: 1.4856925010681152\n",
      "Epoch: 3 \tBatch: 325 \tLoss: 1.469272494316101\n",
      "Epoch: 3 \tBatch: 326 \tLoss: 1.479691505432129\n",
      "Epoch: 3 \tBatch: 327 \tLoss: 1.4698230028152466\n",
      "Epoch: 3 \tBatch: 328 \tLoss: 1.4692438840866089\n",
      "Epoch: 3 \tBatch: 329 \tLoss: 1.4717425107955933\n",
      "Epoch: 3 \tBatch: 330 \tLoss: 1.490652322769165\n",
      "Epoch: 3 \tBatch: 331 \tLoss: 1.486755132675171\n",
      "Epoch: 3 \tBatch: 332 \tLoss: 1.4762357473373413\n",
      "Epoch: 3 \tBatch: 333 \tLoss: 1.4697628021240234\n",
      "Epoch: 3 \tBatch: 334 \tLoss: 1.5062092542648315\n",
      "Epoch: 3 \tBatch: 335 \tLoss: 1.469260334968567\n",
      "Epoch: 3 \tBatch: 336 \tLoss: 1.4703421592712402\n",
      "Epoch: 3 \tBatch: 337 \tLoss: 1.4690117835998535\n",
      "Epoch: 3 \tBatch: 338 \tLoss: 1.4654874801635742\n",
      "Epoch: 3 \tBatch: 339 \tLoss: 1.4905096292495728\n",
      "Epoch: 3 \tBatch: 340 \tLoss: 1.469069480895996\n",
      "Epoch: 3 \tBatch: 341 \tLoss: 1.4861326217651367\n",
      "Epoch: 3 \tBatch: 342 \tLoss: 1.4668339490890503\n",
      "Epoch: 3 \tBatch: 343 \tLoss: 1.4704636335372925\n",
      "Epoch: 3 \tBatch: 344 \tLoss: 1.4950793981552124\n",
      "Epoch: 3 \tBatch: 345 \tLoss: 1.4917458295822144\n",
      "Epoch: 3 \tBatch: 346 \tLoss: 1.4619252681732178\n",
      "Epoch: 3 \tBatch: 347 \tLoss: 1.4729721546173096\n",
      "Epoch: 3 \tBatch: 348 \tLoss: 1.5030943155288696\n",
      "Epoch: 3 \tBatch: 349 \tLoss: 1.472670555114746\n",
      "Epoch: 3 \tBatch: 350 \tLoss: 1.463688850402832\n",
      "Epoch: 3 \tBatch: 351 \tLoss: 1.4673606157302856\n",
      "Epoch: 3 \tBatch: 352 \tLoss: 1.4702867269515991\n",
      "Epoch: 3 \tBatch: 353 \tLoss: 1.468353271484375\n",
      "Epoch: 3 \tBatch: 354 \tLoss: 1.4719756841659546\n",
      "Epoch: 3 \tBatch: 355 \tLoss: 1.483514666557312\n",
      "Epoch: 3 \tBatch: 356 \tLoss: 1.4676512479782104\n",
      "Epoch: 3 \tBatch: 357 \tLoss: 1.477142572402954\n",
      "Epoch: 3 \tBatch: 358 \tLoss: 1.4681165218353271\n",
      "Epoch: 3 \tBatch: 359 \tLoss: 1.4744824171066284\n",
      "Epoch: 3 \tBatch: 360 \tLoss: 1.4815175533294678\n",
      "Epoch: 3 \tBatch: 361 \tLoss: 1.4735169410705566\n",
      "Epoch: 3 \tBatch: 362 \tLoss: 1.4842097759246826\n",
      "Epoch: 3 \tBatch: 363 \tLoss: 1.477870225906372\n",
      "Epoch: 3 \tBatch: 364 \tLoss: 1.4706865549087524\n",
      "Epoch: 3 \tBatch: 365 \tLoss: 1.4613240957260132\n",
      "Epoch: 3 \tBatch: 366 \tLoss: 1.4700870513916016\n",
      "Epoch: 3 \tBatch: 367 \tLoss: 1.4757182598114014\n",
      "Epoch: 3 \tBatch: 368 \tLoss: 1.4918895959854126\n",
      "Epoch: 3 \tBatch: 369 \tLoss: 1.46267569065094\n",
      "Epoch: 3 \tBatch: 370 \tLoss: 1.4821425676345825\n",
      "Epoch: 3 \tBatch: 371 \tLoss: 1.4617863893508911\n",
      "Epoch: 3 \tBatch: 372 \tLoss: 1.492490530014038\n",
      "Epoch: 3 \tBatch: 373 \tLoss: 1.4820597171783447\n",
      "Epoch: 3 \tBatch: 374 \tLoss: 1.4882612228393555\n",
      "Epoch: 3 \tBatch: 375 \tLoss: 1.477344036102295\n",
      "Epoch: 3 \tBatch: 376 \tLoss: 1.462377667427063\n",
      "Epoch: 3 \tBatch: 377 \tLoss: 1.4691334962844849\n",
      "Epoch: 3 \tBatch: 378 \tLoss: 1.4836772680282593\n",
      "Epoch: 3 \tBatch: 379 \tLoss: 1.4717462062835693\n",
      "Epoch: 3 \tBatch: 380 \tLoss: 1.476696491241455\n",
      "Epoch: 3 \tBatch: 381 \tLoss: 1.4816393852233887\n",
      "Epoch: 3 \tBatch: 382 \tLoss: 1.4867914915084839\n",
      "Epoch: 3 \tBatch: 383 \tLoss: 1.4834990501403809\n",
      "Epoch: 3 \tBatch: 384 \tLoss: 1.4846007823944092\n",
      "Epoch: 3 \tBatch: 385 \tLoss: 1.4898701906204224\n",
      "Epoch: 3 \tBatch: 386 \tLoss: 1.503249168395996\n",
      "Epoch: 3 \tBatch: 387 \tLoss: 1.4643455743789673\n",
      "Epoch: 3 \tBatch: 388 \tLoss: 1.4844602346420288\n",
      "Epoch: 3 \tBatch: 389 \tLoss: 1.4763109683990479\n",
      "Epoch: 3 \tBatch: 390 \tLoss: 1.4737112522125244\n",
      "Epoch: 4 \tBatch: 0 \tLoss: 1.4799095392227173\n",
      "Epoch: 4 \tBatch: 1 \tLoss: 1.470367431640625\n",
      "Epoch: 4 \tBatch: 2 \tLoss: 1.473533272743225\n",
      "Epoch: 4 \tBatch: 3 \tLoss: 1.470261573791504\n",
      "Epoch: 4 \tBatch: 4 \tLoss: 1.4637202024459839\n",
      "Epoch: 4 \tBatch: 5 \tLoss: 1.4937632083892822\n",
      "Epoch: 4 \tBatch: 6 \tLoss: 1.485439419746399\n",
      "Epoch: 4 \tBatch: 7 \tLoss: 1.4698814153671265\n",
      "Epoch: 4 \tBatch: 8 \tLoss: 1.4712873697280884\n",
      "Epoch: 4 \tBatch: 9 \tLoss: 1.4920812845230103\n",
      "Epoch: 4 \tBatch: 10 \tLoss: 1.464049220085144\n",
      "Epoch: 4 \tBatch: 11 \tLoss: 1.4778928756713867\n",
      "Epoch: 4 \tBatch: 12 \tLoss: 1.4639382362365723\n",
      "Epoch: 4 \tBatch: 13 \tLoss: 1.4768831729888916\n",
      "Epoch: 4 \tBatch: 14 \tLoss: 1.4691243171691895\n",
      "Epoch: 4 \tBatch: 15 \tLoss: 1.4770392179489136\n",
      "Epoch: 4 \tBatch: 16 \tLoss: 1.47425377368927\n",
      "Epoch: 4 \tBatch: 17 \tLoss: 1.4614496231079102\n",
      "Epoch: 4 \tBatch: 18 \tLoss: 1.4833340644836426\n",
      "Epoch: 4 \tBatch: 19 \tLoss: 1.4728649854660034\n",
      "Epoch: 4 \tBatch: 20 \tLoss: 1.4773049354553223\n",
      "Epoch: 4 \tBatch: 21 \tLoss: 1.482235312461853\n",
      "Epoch: 4 \tBatch: 22 \tLoss: 1.469295620918274\n",
      "Epoch: 4 \tBatch: 23 \tLoss: 1.4792665243148804\n",
      "Epoch: 4 \tBatch: 24 \tLoss: 1.470453143119812\n",
      "Epoch: 4 \tBatch: 25 \tLoss: 1.4618034362792969\n",
      "Epoch: 4 \tBatch: 26 \tLoss: 1.4931063652038574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 27 \tLoss: 1.4616276025772095\n",
      "Epoch: 4 \tBatch: 28 \tLoss: 1.474488377571106\n",
      "Epoch: 4 \tBatch: 29 \tLoss: 1.4628113508224487\n",
      "Epoch: 4 \tBatch: 30 \tLoss: 1.473405122756958\n",
      "Epoch: 4 \tBatch: 31 \tLoss: 1.4695358276367188\n",
      "Epoch: 4 \tBatch: 32 \tLoss: 1.4625904560089111\n",
      "Epoch: 4 \tBatch: 33 \tLoss: 1.470141887664795\n",
      "Epoch: 4 \tBatch: 34 \tLoss: 1.469040870666504\n",
      "Epoch: 4 \tBatch: 35 \tLoss: 1.4616608619689941\n",
      "Epoch: 4 \tBatch: 36 \tLoss: 1.4845010042190552\n",
      "Epoch: 4 \tBatch: 37 \tLoss: 1.476773738861084\n",
      "Epoch: 4 \tBatch: 38 \tLoss: 1.4709007740020752\n",
      "Epoch: 4 \tBatch: 39 \tLoss: 1.4784371852874756\n",
      "Epoch: 4 \tBatch: 40 \tLoss: 1.4785902500152588\n",
      "Epoch: 4 \tBatch: 41 \tLoss: 1.4843676090240479\n",
      "Epoch: 4 \tBatch: 42 \tLoss: 1.4915410280227661\n",
      "Epoch: 4 \tBatch: 43 \tLoss: 1.4803842306137085\n",
      "Epoch: 4 \tBatch: 44 \tLoss: 1.4649620056152344\n",
      "Epoch: 4 \tBatch: 45 \tLoss: 1.4733548164367676\n",
      "Epoch: 4 \tBatch: 46 \tLoss: 1.4695310592651367\n",
      "Epoch: 4 \tBatch: 47 \tLoss: 1.4767056703567505\n",
      "Epoch: 4 \tBatch: 48 \tLoss: 1.4693187475204468\n",
      "Epoch: 4 \tBatch: 49 \tLoss: 1.4678473472595215\n",
      "Epoch: 4 \tBatch: 50 \tLoss: 1.4620600938796997\n",
      "Epoch: 4 \tBatch: 51 \tLoss: 1.470221996307373\n",
      "Epoch: 4 \tBatch: 52 \tLoss: 1.4920098781585693\n",
      "Epoch: 4 \tBatch: 53 \tLoss: 1.4693641662597656\n",
      "Epoch: 4 \tBatch: 54 \tLoss: 1.4767874479293823\n",
      "Epoch: 4 \tBatch: 55 \tLoss: 1.4929863214492798\n",
      "Epoch: 4 \tBatch: 56 \tLoss: 1.4875783920288086\n",
      "Epoch: 4 \tBatch: 57 \tLoss: 1.4630804061889648\n",
      "Epoch: 4 \tBatch: 58 \tLoss: 1.4616506099700928\n",
      "Epoch: 4 \tBatch: 59 \tLoss: 1.4643014669418335\n",
      "Epoch: 4 \tBatch: 60 \tLoss: 1.4654515981674194\n",
      "Epoch: 4 \tBatch: 61 \tLoss: 1.4656282663345337\n",
      "Epoch: 4 \tBatch: 62 \tLoss: 1.4614392518997192\n",
      "Epoch: 4 \tBatch: 63 \tLoss: 1.4649344682693481\n",
      "Epoch: 4 \tBatch: 64 \tLoss: 1.4786021709442139\n",
      "Epoch: 4 \tBatch: 65 \tLoss: 1.4830914735794067\n",
      "Epoch: 4 \tBatch: 66 \tLoss: 1.4743410348892212\n",
      "Epoch: 4 \tBatch: 67 \tLoss: 1.4894002676010132\n",
      "Epoch: 4 \tBatch: 68 \tLoss: 1.4770835638046265\n",
      "Epoch: 4 \tBatch: 69 \tLoss: 1.4939957857131958\n",
      "Epoch: 4 \tBatch: 70 \tLoss: 1.4781757593154907\n",
      "Epoch: 4 \tBatch: 71 \tLoss: 1.4777271747589111\n",
      "Epoch: 4 \tBatch: 72 \tLoss: 1.475888729095459\n",
      "Epoch: 4 \tBatch: 73 \tLoss: 1.4870511293411255\n",
      "Epoch: 4 \tBatch: 74 \tLoss: 1.4690920114517212\n",
      "Epoch: 4 \tBatch: 75 \tLoss: 1.4905582666397095\n",
      "Epoch: 4 \tBatch: 76 \tLoss: 1.4716088771820068\n",
      "Epoch: 4 \tBatch: 77 \tLoss: 1.4653211832046509\n",
      "Epoch: 4 \tBatch: 78 \tLoss: 1.4636681079864502\n",
      "Epoch: 4 \tBatch: 79 \tLoss: 1.4695898294448853\n",
      "Epoch: 4 \tBatch: 80 \tLoss: 1.468984603881836\n",
      "Epoch: 4 \tBatch: 81 \tLoss: 1.4727766513824463\n",
      "Epoch: 4 \tBatch: 82 \tLoss: 1.4677561521530151\n",
      "Epoch: 4 \tBatch: 83 \tLoss: 1.469508409500122\n",
      "Epoch: 4 \tBatch: 84 \tLoss: 1.4613083600997925\n",
      "Epoch: 4 \tBatch: 85 \tLoss: 1.4843275547027588\n",
      "Epoch: 4 \tBatch: 86 \tLoss: 1.463550090789795\n",
      "Epoch: 4 \tBatch: 87 \tLoss: 1.4726636409759521\n",
      "Epoch: 4 \tBatch: 88 \tLoss: 1.4746097326278687\n",
      "Epoch: 4 \tBatch: 89 \tLoss: 1.4633122682571411\n",
      "Epoch: 4 \tBatch: 90 \tLoss: 1.4710265398025513\n",
      "Epoch: 4 \tBatch: 91 \tLoss: 1.468563437461853\n",
      "Epoch: 4 \tBatch: 92 \tLoss: 1.4859548807144165\n",
      "Epoch: 4 \tBatch: 93 \tLoss: 1.4835292100906372\n",
      "Epoch: 4 \tBatch: 94 \tLoss: 1.4614624977111816\n",
      "Epoch: 4 \tBatch: 95 \tLoss: 1.473718523979187\n",
      "Epoch: 4 \tBatch: 96 \tLoss: 1.4879862070083618\n",
      "Epoch: 4 \tBatch: 97 \tLoss: 1.4772310256958008\n",
      "Epoch: 4 \tBatch: 98 \tLoss: 1.499337911605835\n",
      "Epoch: 4 \tBatch: 99 \tLoss: 1.4870153665542603\n",
      "Epoch: 4 \tBatch: 100 \tLoss: 1.4747543334960938\n",
      "Epoch: 4 \tBatch: 101 \tLoss: 1.4684710502624512\n",
      "Epoch: 4 \tBatch: 102 \tLoss: 1.4813188314437866\n",
      "Epoch: 4 \tBatch: 103 \tLoss: 1.4685035943984985\n",
      "Epoch: 4 \tBatch: 104 \tLoss: 1.4886407852172852\n",
      "Epoch: 4 \tBatch: 105 \tLoss: 1.4939920902252197\n",
      "Epoch: 4 \tBatch: 106 \tLoss: 1.4957001209259033\n",
      "Epoch: 4 \tBatch: 107 \tLoss: 1.4797677993774414\n",
      "Epoch: 4 \tBatch: 108 \tLoss: 1.4749435186386108\n",
      "Epoch: 4 \tBatch: 109 \tLoss: 1.4846643209457397\n",
      "Epoch: 4 \tBatch: 110 \tLoss: 1.4696193933486938\n",
      "Epoch: 4 \tBatch: 111 \tLoss: 1.5006580352783203\n",
      "Epoch: 4 \tBatch: 112 \tLoss: 1.4667776823043823\n",
      "Epoch: 4 \tBatch: 113 \tLoss: 1.482668399810791\n",
      "Epoch: 4 \tBatch: 114 \tLoss: 1.4855180978775024\n",
      "Epoch: 4 \tBatch: 115 \tLoss: 1.4763058423995972\n",
      "Epoch: 4 \tBatch: 116 \tLoss: 1.4721723794937134\n",
      "Epoch: 4 \tBatch: 117 \tLoss: 1.4784889221191406\n",
      "Epoch: 4 \tBatch: 118 \tLoss: 1.471629023551941\n",
      "Epoch: 4 \tBatch: 119 \tLoss: 1.4742894172668457\n",
      "Epoch: 4 \tBatch: 120 \tLoss: 1.4901223182678223\n",
      "Epoch: 4 \tBatch: 121 \tLoss: 1.4724010229110718\n",
      "Epoch: 4 \tBatch: 122 \tLoss: 1.4869182109832764\n",
      "Epoch: 4 \tBatch: 123 \tLoss: 1.496840238571167\n",
      "Epoch: 4 \tBatch: 124 \tLoss: 1.4903887510299683\n",
      "Epoch: 4 \tBatch: 125 \tLoss: 1.4933979511260986\n",
      "Epoch: 4 \tBatch: 126 \tLoss: 1.4713411331176758\n",
      "Epoch: 4 \tBatch: 127 \tLoss: 1.4760727882385254\n",
      "Epoch: 4 \tBatch: 128 \tLoss: 1.4924801588058472\n",
      "Epoch: 4 \tBatch: 129 \tLoss: 1.4817153215408325\n",
      "Epoch: 4 \tBatch: 130 \tLoss: 1.4851323366165161\n",
      "Epoch: 4 \tBatch: 131 \tLoss: 1.4938582181930542\n",
      "Epoch: 4 \tBatch: 132 \tLoss: 1.487383484840393\n",
      "Epoch: 4 \tBatch: 133 \tLoss: 1.4841400384902954\n",
      "Epoch: 4 \tBatch: 134 \tLoss: 1.4714771509170532\n",
      "Epoch: 4 \tBatch: 135 \tLoss: 1.4901469945907593\n",
      "Epoch: 4 \tBatch: 136 \tLoss: 1.4808377027511597\n",
      "Epoch: 4 \tBatch: 137 \tLoss: 1.4691506624221802\n",
      "Epoch: 4 \tBatch: 138 \tLoss: 1.4816051721572876\n",
      "Epoch: 4 \tBatch: 139 \tLoss: 1.4781070947647095\n",
      "Epoch: 4 \tBatch: 140 \tLoss: 1.4718106985092163\n",
      "Epoch: 4 \tBatch: 141 \tLoss: 1.4807425737380981\n",
      "Epoch: 4 \tBatch: 142 \tLoss: 1.4738836288452148\n",
      "Epoch: 4 \tBatch: 143 \tLoss: 1.4691845178604126\n",
      "Epoch: 4 \tBatch: 144 \tLoss: 1.469160556793213\n",
      "Epoch: 4 \tBatch: 145 \tLoss: 1.4692301750183105\n",
      "Epoch: 4 \tBatch: 146 \tLoss: 1.4622176885604858\n",
      "Epoch: 4 \tBatch: 147 \tLoss: 1.4812589883804321\n",
      "Epoch: 4 \tBatch: 148 \tLoss: 1.4638954401016235\n",
      "Epoch: 4 \tBatch: 149 \tLoss: 1.4781652688980103\n",
      "Epoch: 4 \tBatch: 150 \tLoss: 1.4947479963302612\n",
      "Epoch: 4 \tBatch: 151 \tLoss: 1.4772764444351196\n",
      "Epoch: 4 \tBatch: 152 \tLoss: 1.4764955043792725\n",
      "Epoch: 4 \tBatch: 153 \tLoss: 1.4765875339508057\n",
      "Epoch: 4 \tBatch: 154 \tLoss: 1.471747636795044\n",
      "Epoch: 4 \tBatch: 155 \tLoss: 1.4690994024276733\n",
      "Epoch: 4 \tBatch: 156 \tLoss: 1.4612081050872803\n",
      "Epoch: 4 \tBatch: 157 \tLoss: 1.5013344287872314\n",
      "Epoch: 4 \tBatch: 158 \tLoss: 1.485952615737915\n",
      "Epoch: 4 \tBatch: 159 \tLoss: 1.4745265245437622\n",
      "Epoch: 4 \tBatch: 160 \tLoss: 1.4720520973205566\n",
      "Epoch: 4 \tBatch: 161 \tLoss: 1.477640151977539\n",
      "Epoch: 4 \tBatch: 162 \tLoss: 1.469738245010376\n",
      "Epoch: 4 \tBatch: 163 \tLoss: 1.4731247425079346\n",
      "Epoch: 4 \tBatch: 164 \tLoss: 1.4843937158584595\n",
      "Epoch: 4 \tBatch: 165 \tLoss: 1.4817535877227783\n",
      "Epoch: 4 \tBatch: 166 \tLoss: 1.4813997745513916\n",
      "Epoch: 4 \tBatch: 167 \tLoss: 1.4824784994125366\n",
      "Epoch: 4 \tBatch: 168 \tLoss: 1.4877755641937256\n",
      "Epoch: 4 \tBatch: 169 \tLoss: 1.475514531135559\n",
      "Epoch: 4 \tBatch: 170 \tLoss: 1.4864622354507446\n",
      "Epoch: 4 \tBatch: 171 \tLoss: 1.47662353515625\n",
      "Epoch: 4 \tBatch: 172 \tLoss: 1.4742242097854614\n",
      "Epoch: 4 \tBatch: 173 \tLoss: 1.4730417728424072\n",
      "Epoch: 4 \tBatch: 174 \tLoss: 1.4633784294128418\n",
      "Epoch: 4 \tBatch: 175 \tLoss: 1.4773411750793457\n",
      "Epoch: 4 \tBatch: 176 \tLoss: 1.4639866352081299\n",
      "Epoch: 4 \tBatch: 177 \tLoss: 1.4758754968643188\n",
      "Epoch: 4 \tBatch: 178 \tLoss: 1.491072416305542\n",
      "Epoch: 4 \tBatch: 179 \tLoss: 1.4639604091644287\n",
      "Epoch: 4 \tBatch: 180 \tLoss: 1.5000553131103516\n",
      "Epoch: 4 \tBatch: 181 \tLoss: 1.4616950750350952\n",
      "Epoch: 4 \tBatch: 182 \tLoss: 1.487776517868042\n",
      "Epoch: 4 \tBatch: 183 \tLoss: 1.4743839502334595\n",
      "Epoch: 4 \tBatch: 184 \tLoss: 1.461171269416809\n",
      "Epoch: 4 \tBatch: 185 \tLoss: 1.4778308868408203\n",
      "Epoch: 4 \tBatch: 186 \tLoss: 1.48812735080719\n",
      "Epoch: 4 \tBatch: 187 \tLoss: 1.4679224491119385\n",
      "Epoch: 4 \tBatch: 188 \tLoss: 1.469000220298767\n",
      "Epoch: 4 \tBatch: 189 \tLoss: 1.492287516593933\n",
      "Epoch: 4 \tBatch: 190 \tLoss: 1.4702441692352295\n",
      "Epoch: 4 \tBatch: 191 \tLoss: 1.4738444089889526\n",
      "Epoch: 4 \tBatch: 192 \tLoss: 1.4796233177185059\n",
      "Epoch: 4 \tBatch: 193 \tLoss: 1.4914114475250244\n",
      "Epoch: 4 \tBatch: 194 \tLoss: 1.4631708860397339\n",
      "Epoch: 4 \tBatch: 195 \tLoss: 1.461620569229126\n",
      "Epoch: 4 \tBatch: 196 \tLoss: 1.4746816158294678\n",
      "Epoch: 4 \tBatch: 197 \tLoss: 1.4712865352630615\n",
      "Epoch: 4 \tBatch: 198 \tLoss: 1.4699829816818237\n",
      "Epoch: 4 \tBatch: 199 \tLoss: 1.47614324092865\n",
      "Epoch: 4 \tBatch: 200 \tLoss: 1.472569465637207\n",
      "Epoch: 4 \tBatch: 201 \tLoss: 1.4786708354949951\n",
      "Epoch: 4 \tBatch: 202 \tLoss: 1.4700779914855957\n",
      "Epoch: 4 \tBatch: 203 \tLoss: 1.4701786041259766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 204 \tLoss: 1.4804421663284302\n",
      "Epoch: 4 \tBatch: 205 \tLoss: 1.4664713144302368\n",
      "Epoch: 4 \tBatch: 206 \tLoss: 1.4688502550125122\n",
      "Epoch: 4 \tBatch: 207 \tLoss: 1.4773348569869995\n",
      "Epoch: 4 \tBatch: 208 \tLoss: 1.4670206308364868\n",
      "Epoch: 4 \tBatch: 209 \tLoss: 1.4887651205062866\n",
      "Epoch: 4 \tBatch: 210 \tLoss: 1.4808531999588013\n",
      "Epoch: 4 \tBatch: 211 \tLoss: 1.469669222831726\n",
      "Epoch: 4 \tBatch: 212 \tLoss: 1.4725536108016968\n",
      "Epoch: 4 \tBatch: 213 \tLoss: 1.465897798538208\n",
      "Epoch: 4 \tBatch: 214 \tLoss: 1.4793319702148438\n",
      "Epoch: 4 \tBatch: 215 \tLoss: 1.4847230911254883\n",
      "Epoch: 4 \tBatch: 216 \tLoss: 1.4708094596862793\n",
      "Epoch: 4 \tBatch: 217 \tLoss: 1.471500039100647\n",
      "Epoch: 4 \tBatch: 218 \tLoss: 1.5087584257125854\n",
      "Epoch: 4 \tBatch: 219 \tLoss: 1.4915016889572144\n",
      "Epoch: 4 \tBatch: 220 \tLoss: 1.478767991065979\n",
      "Epoch: 4 \tBatch: 221 \tLoss: 1.464300513267517\n",
      "Epoch: 4 \tBatch: 222 \tLoss: 1.4808247089385986\n",
      "Epoch: 4 \tBatch: 223 \tLoss: 1.464365839958191\n",
      "Epoch: 4 \tBatch: 224 \tLoss: 1.4650676250457764\n",
      "Epoch: 4 \tBatch: 225 \tLoss: 1.4788334369659424\n",
      "Epoch: 4 \tBatch: 226 \tLoss: 1.4655177593231201\n",
      "Epoch: 4 \tBatch: 227 \tLoss: 1.469448447227478\n",
      "Epoch: 4 \tBatch: 228 \tLoss: 1.4906456470489502\n",
      "Epoch: 4 \tBatch: 229 \tLoss: 1.502550721168518\n",
      "Epoch: 4 \tBatch: 230 \tLoss: 1.4777202606201172\n",
      "Epoch: 4 \tBatch: 231 \tLoss: 1.4693719148635864\n",
      "Epoch: 4 \tBatch: 232 \tLoss: 1.473669409751892\n",
      "Epoch: 4 \tBatch: 233 \tLoss: 1.4663923978805542\n",
      "Epoch: 4 \tBatch: 234 \tLoss: 1.471009612083435\n",
      "Epoch: 4 \tBatch: 235 \tLoss: 1.4714583158493042\n",
      "Epoch: 4 \tBatch: 236 \tLoss: 1.4713199138641357\n",
      "Epoch: 4 \tBatch: 237 \tLoss: 1.5081877708435059\n",
      "Epoch: 4 \tBatch: 238 \tLoss: 1.4985069036483765\n",
      "Epoch: 4 \tBatch: 239 \tLoss: 1.4761936664581299\n",
      "Epoch: 4 \tBatch: 240 \tLoss: 1.5043967962265015\n",
      "Epoch: 4 \tBatch: 241 \tLoss: 1.4713287353515625\n",
      "Epoch: 4 \tBatch: 242 \tLoss: 1.470357060432434\n",
      "Epoch: 4 \tBatch: 243 \tLoss: 1.4841444492340088\n",
      "Epoch: 4 \tBatch: 244 \tLoss: 1.4768331050872803\n",
      "Epoch: 4 \tBatch: 245 \tLoss: 1.4772638082504272\n",
      "Epoch: 4 \tBatch: 246 \tLoss: 1.4673103094100952\n",
      "Epoch: 4 \tBatch: 247 \tLoss: 1.4763689041137695\n",
      "Epoch: 4 \tBatch: 248 \tLoss: 1.4767534732818604\n",
      "Epoch: 4 \tBatch: 249 \tLoss: 1.478655457496643\n",
      "Epoch: 4 \tBatch: 250 \tLoss: 1.4844119548797607\n",
      "Epoch: 4 \tBatch: 251 \tLoss: 1.4791648387908936\n",
      "Epoch: 4 \tBatch: 252 \tLoss: 1.4824460744857788\n",
      "Epoch: 4 \tBatch: 253 \tLoss: 1.4691945314407349\n",
      "Epoch: 4 \tBatch: 254 \tLoss: 1.4835739135742188\n",
      "Epoch: 4 \tBatch: 255 \tLoss: 1.4703326225280762\n",
      "Epoch: 4 \tBatch: 256 \tLoss: 1.4764246940612793\n",
      "Epoch: 4 \tBatch: 257 \tLoss: 1.4868682622909546\n",
      "Epoch: 4 \tBatch: 258 \tLoss: 1.4693156480789185\n",
      "Epoch: 4 \tBatch: 259 \tLoss: 1.4651435613632202\n",
      "Epoch: 4 \tBatch: 260 \tLoss: 1.4668885469436646\n",
      "Epoch: 4 \tBatch: 261 \tLoss: 1.4848672151565552\n",
      "Epoch: 4 \tBatch: 262 \tLoss: 1.4697071313858032\n",
      "Epoch: 4 \tBatch: 263 \tLoss: 1.4688467979431152\n",
      "Epoch: 4 \tBatch: 264 \tLoss: 1.5045610666275024\n",
      "Epoch: 4 \tBatch: 265 \tLoss: 1.4912062883377075\n",
      "Epoch: 4 \tBatch: 266 \tLoss: 1.4845232963562012\n",
      "Epoch: 4 \tBatch: 267 \tLoss: 1.4708836078643799\n",
      "Epoch: 4 \tBatch: 268 \tLoss: 1.4946955442428589\n",
      "Epoch: 4 \tBatch: 269 \tLoss: 1.4767580032348633\n",
      "Epoch: 4 \tBatch: 270 \tLoss: 1.465606927871704\n",
      "Epoch: 4 \tBatch: 271 \tLoss: 1.4687241315841675\n",
      "Epoch: 4 \tBatch: 272 \tLoss: 1.4766374826431274\n",
      "Epoch: 4 \tBatch: 273 \tLoss: 1.485099196434021\n",
      "Epoch: 4 \tBatch: 274 \tLoss: 1.5002015829086304\n",
      "Epoch: 4 \tBatch: 275 \tLoss: 1.4714351892471313\n",
      "Epoch: 4 \tBatch: 276 \tLoss: 1.4908312559127808\n",
      "Epoch: 4 \tBatch: 277 \tLoss: 1.4612149000167847\n",
      "Epoch: 4 \tBatch: 278 \tLoss: 1.4839681386947632\n",
      "Epoch: 4 \tBatch: 279 \tLoss: 1.4866443872451782\n",
      "Epoch: 4 \tBatch: 280 \tLoss: 1.4649505615234375\n",
      "Epoch: 4 \tBatch: 281 \tLoss: 1.4718466997146606\n",
      "Epoch: 4 \tBatch: 282 \tLoss: 1.4692766666412354\n",
      "Epoch: 4 \tBatch: 283 \tLoss: 1.4651895761489868\n",
      "Epoch: 4 \tBatch: 284 \tLoss: 1.4612338542938232\n",
      "Epoch: 4 \tBatch: 285 \tLoss: 1.4691399335861206\n",
      "Epoch: 4 \tBatch: 286 \tLoss: 1.4770385026931763\n",
      "Epoch: 4 \tBatch: 287 \tLoss: 1.5089856386184692\n",
      "Epoch: 4 \tBatch: 288 \tLoss: 1.47490394115448\n",
      "Epoch: 4 \tBatch: 289 \tLoss: 1.475955605506897\n",
      "Epoch: 4 \tBatch: 290 \tLoss: 1.4959608316421509\n",
      "Epoch: 4 \tBatch: 291 \tLoss: 1.4731639623641968\n",
      "Epoch: 4 \tBatch: 292 \tLoss: 1.4850773811340332\n",
      "Epoch: 4 \tBatch: 293 \tLoss: 1.4674538373947144\n",
      "Epoch: 4 \tBatch: 294 \tLoss: 1.4888771772384644\n",
      "Epoch: 4 \tBatch: 295 \tLoss: 1.4698126316070557\n",
      "Epoch: 4 \tBatch: 296 \tLoss: 1.4780808687210083\n",
      "Epoch: 4 \tBatch: 297 \tLoss: 1.465346097946167\n",
      "Epoch: 4 \tBatch: 298 \tLoss: 1.483239769935608\n",
      "Epoch: 4 \tBatch: 299 \tLoss: 1.470859169960022\n",
      "Epoch: 4 \tBatch: 300 \tLoss: 1.4672678709030151\n",
      "Epoch: 4 \tBatch: 301 \tLoss: 1.4689924716949463\n",
      "Epoch: 4 \tBatch: 302 \tLoss: 1.4874924421310425\n",
      "Epoch: 4 \tBatch: 303 \tLoss: 1.4715182781219482\n",
      "Epoch: 4 \tBatch: 304 \tLoss: 1.4704296588897705\n",
      "Epoch: 4 \tBatch: 305 \tLoss: 1.4624016284942627\n",
      "Epoch: 4 \tBatch: 306 \tLoss: 1.4848499298095703\n",
      "Epoch: 4 \tBatch: 307 \tLoss: 1.4623361825942993\n",
      "Epoch: 4 \tBatch: 308 \tLoss: 1.462369441986084\n",
      "Epoch: 4 \tBatch: 309 \tLoss: 1.4632841348648071\n",
      "Epoch: 4 \tBatch: 310 \tLoss: 1.4691855907440186\n",
      "Epoch: 4 \tBatch: 311 \tLoss: 1.468998908996582\n",
      "Epoch: 4 \tBatch: 312 \tLoss: 1.4766954183578491\n",
      "Epoch: 4 \tBatch: 313 \tLoss: 1.4625626802444458\n",
      "Epoch: 4 \tBatch: 314 \tLoss: 1.4769840240478516\n",
      "Epoch: 4 \tBatch: 315 \tLoss: 1.468148112297058\n",
      "Epoch: 4 \tBatch: 316 \tLoss: 1.465767741203308\n",
      "Epoch: 4 \tBatch: 317 \tLoss: 1.4846324920654297\n",
      "Epoch: 4 \tBatch: 318 \tLoss: 1.4794057607650757\n",
      "Epoch: 4 \tBatch: 319 \tLoss: 1.46649968624115\n",
      "Epoch: 4 \tBatch: 320 \tLoss: 1.486480474472046\n",
      "Epoch: 4 \tBatch: 321 \tLoss: 1.4652540683746338\n",
      "Epoch: 4 \tBatch: 322 \tLoss: 1.4911730289459229\n",
      "Epoch: 4 \tBatch: 323 \tLoss: 1.4901562929153442\n",
      "Epoch: 4 \tBatch: 324 \tLoss: 1.4780223369598389\n",
      "Epoch: 4 \tBatch: 325 \tLoss: 1.478904128074646\n",
      "Epoch: 4 \tBatch: 326 \tLoss: 1.461398959159851\n",
      "Epoch: 4 \tBatch: 327 \tLoss: 1.4690582752227783\n",
      "Epoch: 4 \tBatch: 328 \tLoss: 1.4613498449325562\n",
      "Epoch: 4 \tBatch: 329 \tLoss: 1.4819422960281372\n",
      "Epoch: 4 \tBatch: 330 \tLoss: 1.4952081441879272\n",
      "Epoch: 4 \tBatch: 331 \tLoss: 1.47101891040802\n",
      "Epoch: 4 \tBatch: 332 \tLoss: 1.4691134691238403\n",
      "Epoch: 4 \tBatch: 333 \tLoss: 1.4643200635910034\n",
      "Epoch: 4 \tBatch: 334 \tLoss: 1.471061110496521\n",
      "Epoch: 4 \tBatch: 335 \tLoss: 1.4727776050567627\n",
      "Epoch: 4 \tBatch: 336 \tLoss: 1.4661049842834473\n",
      "Epoch: 4 \tBatch: 337 \tLoss: 1.4626590013504028\n",
      "Epoch: 4 \tBatch: 338 \tLoss: 1.4783872365951538\n",
      "Epoch: 4 \tBatch: 339 \tLoss: 1.4907797574996948\n",
      "Epoch: 4 \tBatch: 340 \tLoss: 1.4809812307357788\n",
      "Epoch: 4 \tBatch: 341 \tLoss: 1.4621766805648804\n",
      "Epoch: 4 \tBatch: 342 \tLoss: 1.465899109840393\n",
      "Epoch: 4 \tBatch: 343 \tLoss: 1.4769182205200195\n",
      "Epoch: 4 \tBatch: 344 \tLoss: 1.4690481424331665\n",
      "Epoch: 4 \tBatch: 345 \tLoss: 1.461268663406372\n",
      "Epoch: 4 \tBatch: 346 \tLoss: 1.476202368736267\n",
      "Epoch: 4 \tBatch: 347 \tLoss: 1.479792833328247\n",
      "Epoch: 4 \tBatch: 348 \tLoss: 1.4731969833374023\n",
      "Epoch: 4 \tBatch: 349 \tLoss: 1.4908627271652222\n",
      "Epoch: 4 \tBatch: 350 \tLoss: 1.4627108573913574\n",
      "Epoch: 4 \tBatch: 351 \tLoss: 1.4778008460998535\n",
      "Epoch: 4 \tBatch: 352 \tLoss: 1.4674066305160522\n",
      "Epoch: 4 \tBatch: 353 \tLoss: 1.461552381515503\n",
      "Epoch: 4 \tBatch: 354 \tLoss: 1.4711259603500366\n",
      "Epoch: 4 \tBatch: 355 \tLoss: 1.4793070554733276\n",
      "Epoch: 4 \tBatch: 356 \tLoss: 1.4637035131454468\n",
      "Epoch: 4 \tBatch: 357 \tLoss: 1.4620906114578247\n",
      "Epoch: 4 \tBatch: 358 \tLoss: 1.4933418035507202\n",
      "Epoch: 4 \tBatch: 359 \tLoss: 1.4661599397659302\n",
      "Epoch: 4 \tBatch: 360 \tLoss: 1.47542142868042\n",
      "Epoch: 4 \tBatch: 361 \tLoss: 1.4766961336135864\n",
      "Epoch: 4 \tBatch: 362 \tLoss: 1.5074118375778198\n",
      "Epoch: 4 \tBatch: 363 \tLoss: 1.4621423482894897\n",
      "Epoch: 4 \tBatch: 364 \tLoss: 1.4890366792678833\n",
      "Epoch: 4 \tBatch: 365 \tLoss: 1.4617109298706055\n",
      "Epoch: 4 \tBatch: 366 \tLoss: 1.4772838354110718\n",
      "Epoch: 4 \tBatch: 367 \tLoss: 1.471800446510315\n",
      "Epoch: 4 \tBatch: 368 \tLoss: 1.4813132286071777\n",
      "Epoch: 4 \tBatch: 369 \tLoss: 1.4847933053970337\n",
      "Epoch: 4 \tBatch: 370 \tLoss: 1.4906281232833862\n",
      "Epoch: 4 \tBatch: 371 \tLoss: 1.4715543985366821\n",
      "Epoch: 4 \tBatch: 372 \tLoss: 1.4745182991027832\n",
      "Epoch: 4 \tBatch: 373 \tLoss: 1.4701107740402222\n",
      "Epoch: 4 \tBatch: 374 \tLoss: 1.487334966659546\n",
      "Epoch: 4 \tBatch: 375 \tLoss: 1.4678211212158203\n",
      "Epoch: 4 \tBatch: 376 \tLoss: 1.480475902557373\n",
      "Epoch: 4 \tBatch: 377 \tLoss: 1.477880835533142\n",
      "Epoch: 4 \tBatch: 378 \tLoss: 1.4884231090545654\n",
      "Epoch: 4 \tBatch: 379 \tLoss: 1.4824734926223755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 380 \tLoss: 1.4674445390701294\n",
      "Epoch: 4 \tBatch: 381 \tLoss: 1.4842629432678223\n",
      "Epoch: 4 \tBatch: 382 \tLoss: 1.4700963497161865\n",
      "Epoch: 4 \tBatch: 383 \tLoss: 1.475220799446106\n",
      "Epoch: 4 \tBatch: 384 \tLoss: 1.4763848781585693\n",
      "Epoch: 4 \tBatch: 385 \tLoss: 1.4841045141220093\n",
      "Epoch: 4 \tBatch: 386 \tLoss: 1.4767814874649048\n",
      "Epoch: 4 \tBatch: 387 \tLoss: 1.481117844581604\n",
      "Epoch: 4 \tBatch: 388 \tLoss: 1.4701899290084839\n",
      "Epoch: 4 \tBatch: 389 \tLoss: 1.4689701795578003\n",
      "Epoch: 4 \tBatch: 390 \tLoss: 1.477339506149292\n",
      "Training Complete. Final loss = 1.477339506149292\n"
     ]
    }
   ],
   "source": [
    "def train(model, epochs, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())\n",
    "    \n",
    "train(cnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.904\n",
      "Validation Accuracy: 98.16\n",
      "Test Accuracy: 98.46000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER: len(dataloader) would just be number of batches\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies  # do a max along the rows and preserve number of columns\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices), [1] is the index of the maximum value\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct\n",
    "\n",
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's done\n",
    "You should now understand\n",
    "- the advantages of using CNNs vs vanilla neural networks\n",
    "- how an image is represented as data, including its channels\n",
    "- what convolution is in the context of machine learning\n",
    "- the new convolutional and pooling layers that we have used in this notebook\n",
    "\n",
    "## Next steps\n",
    "- [Custom Datasets](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Custom%20Datasets.ipynb)\n",
    "\n",
    "## Appendix\n",
    "- [Empirical Benchmarking of Fully Connected vs Convolutional Architecture on MNIST](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Empirical%20Benchmarking%20of%20Fully%20Connected%20vs%20Convolutional%20Architecture%20on%20MNIST.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
