{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "__Prerequisites__\n",
    "\n",
    "- [Neural Networks](https://github.com/AI-Core/Neural-Networks/blob/master/Neural%20Networks.ipynb)\n",
    "\n",
    "## What's wrong with how neural networks process images?\n",
    "\n",
    "The fully connected neural network we looked at in the previous lesson takes in a vector as input. So we flattened our images by stacking the rows so that it could be passed in as input and used for classification problems successfully. \n",
    "\n",
    "#### Spacially structured data\n",
    "\n",
    "For some problems, the order of the features in each example does not matter (e.g. age, height, hair length). But this isn't the case for images. If we randomly reorder the pixels in an image, then it will likely be unrecognisable. Most of the useful information in images comes not from the values of the features (pixels), but from their relative positions. The same is true for processing any other **spacially structured** data such as videos, soundwaves, medical scans, 3D-models etc. \n",
    "\n",
    "The spatial relationships between the different pixels is information that is crucial to our understanding of an image. When we flatten the image, we lose this information.\n",
    "\n",
    "#### Weight sharing across space\n",
    "\n",
    "Regardless of where I see something interesting in my field of view, it can often be processed in the same way. \n",
    "\n",
    "Neural networks have individual weights for each input feature because they expect each feature to represent a totally different thing (e.g. age, height, weight). In other domains like computer vision however, different features (pixels) can represent the same thing just in different locations (e.g. money on my left, money on my right).\n",
    "\n",
    "Instead of learning to look for the same features of an image with different weights for each position that that feature might be in, we should try to share the same learnt weights over all positions of the input. This will save us both time and memory in computing and storing these duplicate weights. \n",
    "\n",
    "#### So what\n",
    "\n",
    "Using our prior understanding of how image data should be processed spacially, we'd like to find some kind of model that can retain the spacial structure of an input, and look for the same features over the whole of this space. This is what we will use convolutional neural networks for.\n",
    "\n",
    "## Images as data\n",
    "\n",
    "Images are not naturally vectors. They obviously have a height and a width rather than just a length - so they need to at least be a matrix. \n",
    "\n",
    "#### Channels\n",
    "\n",
    "Any non-black color can be made by combining 3 primary colors.\n",
    "As such, as well as height and width, color images have another axis called the **channels**, which specifies the intensity (contribution) of each of these primary colors.\n",
    "Red, green and blue are the (arbitrary) standard primary colors. \n",
    "So most images that we will work with have a red channel, a green channel and a blue channel.\n",
    "This is illustrated below.\n",
    "\n",
    "![image](images/CNN_RGB.JPG)\n",
    "\n",
    "Some images can also have transparent backgrounds, in which case they might have a fourth channel to represent the opacity at each pixel location.\n",
    "\n",
    "## How was computer vision done before deep learning?\n",
    "\n",
    "In the past, people would try to draw patterns that they thought would appear in images and be useful for the problem that they were trying to solve. This was a painstakingly long process, and was obviously susceptible to a lot of bias by these feature designers.\n",
    "\n",
    "## Filters/Kernels\n",
    "These supposedly useful patterns mentioned above are known as **filters** or **kernels**. \n",
    "Each filter looks for a particular pattern.\n",
    "E.g. a filter that looks for circles would have high values in a circle and low values in other locations.\n",
    "\n",
    "![title](images/kernels.jpg)\n",
    "\n",
    "Filters *look* for the patterns they represent by seeing how similar the pixels at any particular location match the values that they contain. A mathematically convenient way to do this is by taking a **dot product** between the filter's values and the input values which it covers - an element wise multiplication and sum of the results. **This produces a single value** which should be larger when the input better matches the feature that the filter looks for.\n",
    "\n",
    "It is standard for filters to always convolve through the full depth of the input. So if we have an input with 3 channels (e.g. a color image), our kernel will also have a depth of 3 - where each channel of the filter is what it looks for from that corresponding color channel. If our input has 54 channels, then so will our filter. \n",
    "\n",
    "The width and height of our kernels is up to us (they are hyperparameters). It's standard to have kernels with equal width and height.\n",
    "\n",
    "## The convolution operation\n",
    "\n",
    "In machine learning, convolution is the proccess of moving a filter across every possible position of the input and computing a value for how well it is matched at each location. \n",
    "\n",
    "This pattern matching over the spacially structured input produces a similar spacially structured output. We call this output an **activation map** or a **feature map** because it represents the activations in the next layer that should represent some higher level (more complex) features than the feature maps in the input.\n",
    "\n",
    "The animation below shows how a 1x3x3 filter is applied to a 1x5x5 image (for simplicity, input channels = 1). \n",
    "On the left is the filter that we will convolve over the input. In the centre is the input being convolved over. On the right is the output activation map produced by convolving this filter over this input.\n",
    "\n",
    "Notice how the output has high  values when the filter is passed over locations where there is an X shape in the input image. This is because the values of the filter are such that it is performing pattern matching for the X shape.\n",
    "\n",
    "![image](images/convolution_animation.gif)\n",
    "\n",
    "The convolution operation has a few possible parameters:\n",
    "\n",
    "### Stride\n",
    "The stride is the number of pixels we shift our kernel along by to compute the next value in the output activation map. Increased stride means less values are computed for the output activation map, which means that we have to do less computation and store less output information, decreasing computing time and cost and reducing memory requirements but reducing output resolution.\n",
    "\n",
    "### Padding\n",
    "We can *pad* the input with a border of extra pixels around the edge. Why might we want to do this?\n",
    "\n",
    "##### Model depth limitations\n",
    "\n",
    "When we use a kernel size larger than one, each single output value is a function of many input values (all the pixels which the filter covers). This means that the size of the convolution output is smaller than the input. As such, there is a limit to the number of successive convolutions that we can apply because eventually the input gets so small that there is only one location of the input that the filter can be placed on the input and the output will then have a height and width of 1 and cannot be convolved over (convolution with a 1x1 filter is equivalent to multiplication).\n",
    "\n",
    "##### Equal input from each pixel\n",
    "\n",
    "When we use a kernel size larger than one, the corner pixels will only contribute to a single output value because they only enter the kernel at it's very extreme positions. As such they contribute less to the final predictions than the other pixels. The same is true for pixels near the edge, but to a lesser extent.\n",
    "\n",
    "#### Different padding modes\n",
    "\n",
    "We can use different \"padding modes\" to specify what we pad the image with. Options include padding it with zeros, continuing the last color outwards, reflecting the inwards colors. See options provided by PyTorch [here](\n",
    "\n",
    "![image](images/CNN_diagram.JPG)\n",
    "\n",
    "For convolution, each computed value in the output feature map is a linear function of the pixels in a local region of the input as opposed to fully connected nets where each computed feature is a linear function of all the values in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The convolutional layer\n",
    "\n",
    "In practice, we want to look for more than just one feature in any input. When we used a neural network, each layer had multiple outputs corresponding to different learnt features. Similarly, instead of convolving just a single filter over the input to produce a single activation map, we convolve many filters over the input to produce many activation maps. This produces a stack of activation maps as the output. The output then has an extra dimension, in addition to the spacial ones, which corresponds to which output activation map you're looking at. This dimension is the convolutional analogy to the number of outputs from a linear layer.\n",
    "\n",
    "Also just like linear layers, convolutional layers apply a simple linear transformation to their input and can be applied successively with activation functions to represent very complex non-linear transformations. Models with such layers are **convolutional neural networks**. These are appropriate for tackling problems like object detection and image segmentation. These convolutional layers have values for each weight within each filter and also include biases to shift each output feature. \n",
    "Just like before, each successive layer in the network learns successively higher level abstract features from the inputs.\n",
    "\n",
    "These convolutional layers are also provided by PyTorch. In this notebook we will use `torch.nn.Conv2D` to convolve over our input in 2 directions (width and height).\n",
    "\n",
    "![image](images/CNN_FNN_comparison.JPG)\n",
    "\n",
    "## What does each filter look for?\n",
    "Engineers used to have to tune filter values manually. Now, just like the weights and biases in linear layers of neural networks, they can be learnt automatically by backpropagation and gradient descent.\n",
    "\n",
    "## Pooling layers\n",
    "Immediately after a convolutional layer, it is common to apply some form of **pooling**. Pooling is a technique that summarises/downsamples the values in a local region of its input. This reduces the number of values in its output, therefore reducing the number of parameters that need to be learned for a succeeding parameterised operation such as a further convolutional or linear layer.\n",
    "\n",
    "Because pooling summarises values in local spacial regions it can help models to be robust under translation of the input, making them more **translation invariant**.\n",
    "\n",
    "Pooling layers also slide kernels over their input, and reduce the values within that grid location to a single value. But they perform different operations than a linear combination like in convolution (see below).\n",
    "\n",
    "**Max pooling** replaces the values at each grid location with their maximum.\n",
    "\n",
    "**Average Pooling** replaces the values at each grid location with their average.\n",
    "\n",
    "See the PyTorch [docs](https://pytorch.org/docs/stable/nn.html#pooling-layers) for more pooling layers\n",
    "\n",
    "## The output of convolutional neural networks\n",
    "\n",
    "Unless we keep applying convolutional layers to our data until it is reduced to a height and width of 1, the output will still retain some spacial dimensions. This means that as well as our input, our output can also be an image for example. This can be useful for problems such as image segmentation, where the output is a pixelwise classification mask of everything in the scene. In this case the output is the same shape as the input image, but with each pixel location taking the value of a class label (e.g. all pixels of cars in the image have value=1, all roads have value=2 etc).\n",
    "\n",
    "In our case though, we want to perform image classification for 10 classes. It is common practice to flatten the output of the convolutional layers of a network into a vector, and then transform them into a vector of the desired output shape by applying a final linear layer. This is what we do below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement a convolutional neural network\n",
    "\n",
    "The first cell is just the same boilerplate we've used before. Make sure you understand it and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )\n",
    "\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example\n",
    "#print(x)\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "            # conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10) # put your linear architecture here using torch.nn.Sequential \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "\n",
    "cnn = ConvNet().to(device) #.to(device)#instantiate model\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: 2.3031537532806396\n",
      "Epoch: 0 \tBatch: 1 \tLoss: 2.3003499507904053\n",
      "Epoch: 0 \tBatch: 2 \tLoss: 2.293708324432373\n",
      "Epoch: 0 \tBatch: 3 \tLoss: 2.2891857624053955\n",
      "Epoch: 0 \tBatch: 4 \tLoss: 2.281893491744995\n",
      "Epoch: 0 \tBatch: 5 \tLoss: 2.269807815551758\n",
      "Epoch: 0 \tBatch: 6 \tLoss: 2.24129581451416\n",
      "Epoch: 0 \tBatch: 7 \tLoss: 2.234517812728882\n",
      "Epoch: 0 \tBatch: 8 \tLoss: 2.2141432762145996\n",
      "Epoch: 0 \tBatch: 9 \tLoss: 2.1698319911956787\n",
      "Epoch: 0 \tBatch: 10 \tLoss: 2.144582986831665\n",
      "Epoch: 0 \tBatch: 11 \tLoss: 2.099961996078491\n",
      "Epoch: 0 \tBatch: 12 \tLoss: 2.079479217529297\n",
      "Epoch: 0 \tBatch: 13 \tLoss: 2.043231964111328\n",
      "Epoch: 0 \tBatch: 14 \tLoss: 1.9976125955581665\n",
      "Epoch: 0 \tBatch: 15 \tLoss: 1.9679197072982788\n",
      "Epoch: 0 \tBatch: 16 \tLoss: 1.9102213382720947\n",
      "Epoch: 0 \tBatch: 17 \tLoss: 2.002774715423584\n",
      "Epoch: 0 \tBatch: 18 \tLoss: 1.8900495767593384\n",
      "Epoch: 0 \tBatch: 19 \tLoss: 1.9211920499801636\n",
      "Epoch: 0 \tBatch: 20 \tLoss: 1.8307785987854004\n",
      "Epoch: 0 \tBatch: 21 \tLoss: 1.9268606901168823\n",
      "Epoch: 0 \tBatch: 22 \tLoss: 1.847485899925232\n",
      "Epoch: 0 \tBatch: 23 \tLoss: 1.7836010456085205\n",
      "Epoch: 0 \tBatch: 24 \tLoss: 1.8352214097976685\n",
      "Epoch: 0 \tBatch: 25 \tLoss: 1.7201507091522217\n",
      "Epoch: 0 \tBatch: 26 \tLoss: 1.7703112363815308\n",
      "Epoch: 0 \tBatch: 27 \tLoss: 1.7399605512619019\n",
      "Epoch: 0 \tBatch: 28 \tLoss: 1.7837785482406616\n",
      "Epoch: 0 \tBatch: 29 \tLoss: 1.8141224384307861\n",
      "Epoch: 0 \tBatch: 30 \tLoss: 1.7670785188674927\n",
      "Epoch: 0 \tBatch: 31 \tLoss: 1.747585415840149\n",
      "Epoch: 0 \tBatch: 32 \tLoss: 1.8104991912841797\n",
      "Epoch: 0 \tBatch: 33 \tLoss: 1.7479442358016968\n",
      "Epoch: 0 \tBatch: 34 \tLoss: 1.7512131929397583\n",
      "Epoch: 0 \tBatch: 35 \tLoss: 1.8360873460769653\n",
      "Epoch: 0 \tBatch: 36 \tLoss: 1.6784011125564575\n",
      "Epoch: 0 \tBatch: 37 \tLoss: 1.7868038415908813\n",
      "Epoch: 0 \tBatch: 38 \tLoss: 1.7611628770828247\n",
      "Epoch: 0 \tBatch: 39 \tLoss: 1.6915780305862427\n",
      "Epoch: 0 \tBatch: 40 \tLoss: 1.7759934663772583\n",
      "Epoch: 0 \tBatch: 41 \tLoss: 1.6992731094360352\n",
      "Epoch: 0 \tBatch: 42 \tLoss: 1.7643754482269287\n",
      "Epoch: 0 \tBatch: 43 \tLoss: 1.7185851335525513\n",
      "Epoch: 0 \tBatch: 44 \tLoss: 1.7956143617630005\n",
      "Epoch: 0 \tBatch: 45 \tLoss: 1.7521390914916992\n",
      "Epoch: 0 \tBatch: 46 \tLoss: 1.7465847730636597\n",
      "Epoch: 0 \tBatch: 47 \tLoss: 1.629878282546997\n",
      "Epoch: 0 \tBatch: 48 \tLoss: 1.8015373945236206\n",
      "Epoch: 0 \tBatch: 49 \tLoss: 1.7596173286437988\n",
      "Epoch: 0 \tBatch: 50 \tLoss: 1.7069597244262695\n",
      "Epoch: 0 \tBatch: 51 \tLoss: 1.786947250366211\n",
      "Epoch: 0 \tBatch: 52 \tLoss: 1.7555861473083496\n",
      "Epoch: 0 \tBatch: 53 \tLoss: 1.7824667692184448\n",
      "Epoch: 0 \tBatch: 54 \tLoss: 1.7467033863067627\n",
      "Epoch: 0 \tBatch: 55 \tLoss: 1.7981469631195068\n",
      "Epoch: 0 \tBatch: 56 \tLoss: 1.7209945917129517\n",
      "Epoch: 0 \tBatch: 57 \tLoss: 1.7577083110809326\n",
      "Epoch: 0 \tBatch: 58 \tLoss: 1.6846567392349243\n",
      "Epoch: 0 \tBatch: 59 \tLoss: 1.735242486000061\n",
      "Epoch: 0 \tBatch: 60 \tLoss: 1.7224997282028198\n",
      "Epoch: 0 \tBatch: 61 \tLoss: 1.7632546424865723\n",
      "Epoch: 0 \tBatch: 62 \tLoss: 1.677677869796753\n",
      "Epoch: 0 \tBatch: 63 \tLoss: 1.6935551166534424\n",
      "Epoch: 0 \tBatch: 64 \tLoss: 1.6839464902877808\n",
      "Epoch: 0 \tBatch: 65 \tLoss: 1.7130223512649536\n",
      "Epoch: 0 \tBatch: 66 \tLoss: 1.724924921989441\n",
      "Epoch: 0 \tBatch: 67 \tLoss: 1.7759400606155396\n",
      "Epoch: 0 \tBatch: 68 \tLoss: 1.7114673852920532\n",
      "Epoch: 0 \tBatch: 69 \tLoss: 1.84055495262146\n",
      "Epoch: 0 \tBatch: 70 \tLoss: 1.7108088731765747\n",
      "Epoch: 0 \tBatch: 71 \tLoss: 1.6951024532318115\n",
      "Epoch: 0 \tBatch: 72 \tLoss: 1.7496528625488281\n",
      "Epoch: 0 \tBatch: 73 \tLoss: 1.713453769683838\n",
      "Epoch: 0 \tBatch: 74 \tLoss: 1.7267166376113892\n",
      "Epoch: 0 \tBatch: 75 \tLoss: 1.6633408069610596\n",
      "Epoch: 0 \tBatch: 76 \tLoss: 1.729877233505249\n",
      "Epoch: 0 \tBatch: 77 \tLoss: 1.7031784057617188\n",
      "Epoch: 0 \tBatch: 78 \tLoss: 1.6899428367614746\n",
      "Epoch: 0 \tBatch: 79 \tLoss: 1.7064543962478638\n",
      "Epoch: 0 \tBatch: 80 \tLoss: 1.6829229593276978\n",
      "Epoch: 0 \tBatch: 81 \tLoss: 1.7614835500717163\n",
      "Epoch: 0 \tBatch: 82 \tLoss: 1.7151052951812744\n",
      "Epoch: 0 \tBatch: 83 \tLoss: 1.8482507467269897\n",
      "Epoch: 0 \tBatch: 84 \tLoss: 1.7129898071289062\n",
      "Epoch: 0 \tBatch: 85 \tLoss: 1.6863305568695068\n",
      "Epoch: 0 \tBatch: 86 \tLoss: 1.7461572885513306\n",
      "Epoch: 0 \tBatch: 87 \tLoss: 1.7392412424087524\n",
      "Epoch: 0 \tBatch: 88 \tLoss: 1.714795470237732\n",
      "Epoch: 0 \tBatch: 89 \tLoss: 1.7521840333938599\n",
      "Epoch: 0 \tBatch: 90 \tLoss: 1.750644326210022\n",
      "Epoch: 0 \tBatch: 91 \tLoss: 1.70645010471344\n",
      "Epoch: 0 \tBatch: 92 \tLoss: 1.7720891237258911\n",
      "Epoch: 0 \tBatch: 93 \tLoss: 1.7739291191101074\n",
      "Epoch: 0 \tBatch: 94 \tLoss: 1.7514076232910156\n",
      "Epoch: 0 \tBatch: 95 \tLoss: 1.7306219339370728\n",
      "Epoch: 0 \tBatch: 96 \tLoss: 1.7116577625274658\n",
      "Epoch: 0 \tBatch: 97 \tLoss: 1.7573034763336182\n",
      "Epoch: 0 \tBatch: 98 \tLoss: 1.7263580560684204\n",
      "Epoch: 0 \tBatch: 99 \tLoss: 1.7155966758728027\n",
      "Epoch: 0 \tBatch: 100 \tLoss: 1.7091141939163208\n",
      "Epoch: 0 \tBatch: 101 \tLoss: 1.7175731658935547\n",
      "Epoch: 0 \tBatch: 102 \tLoss: 1.709922194480896\n",
      "Epoch: 0 \tBatch: 103 \tLoss: 1.7363600730895996\n",
      "Epoch: 0 \tBatch: 104 \tLoss: 1.7321120500564575\n",
      "Epoch: 0 \tBatch: 105 \tLoss: 1.732354760169983\n",
      "Epoch: 0 \tBatch: 106 \tLoss: 1.7044594287872314\n",
      "Epoch: 0 \tBatch: 107 \tLoss: 1.6641799211502075\n",
      "Epoch: 0 \tBatch: 108 \tLoss: 1.7320557832717896\n",
      "Epoch: 0 \tBatch: 109 \tLoss: 1.7337268590927124\n",
      "Epoch: 0 \tBatch: 110 \tLoss: 1.6751989126205444\n",
      "Epoch: 0 \tBatch: 111 \tLoss: 1.7624629735946655\n",
      "Epoch: 0 \tBatch: 112 \tLoss: 1.663634181022644\n",
      "Epoch: 0 \tBatch: 113 \tLoss: 1.705051064491272\n",
      "Epoch: 0 \tBatch: 114 \tLoss: 1.6649558544158936\n",
      "Epoch: 0 \tBatch: 115 \tLoss: 1.6365381479263306\n",
      "Epoch: 0 \tBatch: 116 \tLoss: 1.7058048248291016\n",
      "Epoch: 0 \tBatch: 117 \tLoss: 1.806803584098816\n",
      "Epoch: 0 \tBatch: 118 \tLoss: 1.7102960348129272\n",
      "Epoch: 0 \tBatch: 119 \tLoss: 1.7762646675109863\n",
      "Epoch: 0 \tBatch: 120 \tLoss: 1.7155174016952515\n",
      "Epoch: 0 \tBatch: 121 \tLoss: 1.7754617929458618\n",
      "Epoch: 0 \tBatch: 122 \tLoss: 1.7796756029129028\n",
      "Epoch: 0 \tBatch: 123 \tLoss: 1.7677860260009766\n",
      "Epoch: 0 \tBatch: 124 \tLoss: 1.7626835107803345\n",
      "Epoch: 0 \tBatch: 125 \tLoss: 1.7186754941940308\n",
      "Epoch: 0 \tBatch: 126 \tLoss: 1.7245405912399292\n",
      "Epoch: 0 \tBatch: 127 \tLoss: 1.7405099868774414\n",
      "Epoch: 0 \tBatch: 128 \tLoss: 1.77738356590271\n",
      "Epoch: 0 \tBatch: 129 \tLoss: 1.7189666032791138\n",
      "Epoch: 0 \tBatch: 130 \tLoss: 1.6823378801345825\n",
      "Epoch: 0 \tBatch: 131 \tLoss: 1.8432782888412476\n",
      "Epoch: 0 \tBatch: 132 \tLoss: 1.7743710279464722\n",
      "Epoch: 0 \tBatch: 133 \tLoss: 1.6494247913360596\n",
      "Epoch: 0 \tBatch: 134 \tLoss: 1.6655285358428955\n",
      "Epoch: 0 \tBatch: 135 \tLoss: 1.7194950580596924\n",
      "Epoch: 0 \tBatch: 136 \tLoss: 1.7082196474075317\n",
      "Epoch: 0 \tBatch: 137 \tLoss: 1.71538245677948\n",
      "Epoch: 0 \tBatch: 138 \tLoss: 1.6765080690383911\n",
      "Epoch: 0 \tBatch: 139 \tLoss: 1.6938586235046387\n",
      "Epoch: 0 \tBatch: 140 \tLoss: 1.742751121520996\n",
      "Epoch: 0 \tBatch: 141 \tLoss: 1.7018351554870605\n",
      "Epoch: 0 \tBatch: 142 \tLoss: 1.6938190460205078\n",
      "Epoch: 0 \tBatch: 143 \tLoss: 1.778734564781189\n",
      "Epoch: 0 \tBatch: 144 \tLoss: 1.6437532901763916\n",
      "Epoch: 0 \tBatch: 145 \tLoss: 1.7075870037078857\n",
      "Epoch: 0 \tBatch: 146 \tLoss: 1.7208224534988403\n",
      "Epoch: 0 \tBatch: 147 \tLoss: 1.6862579584121704\n",
      "Epoch: 0 \tBatch: 148 \tLoss: 1.7038826942443848\n",
      "Epoch: 0 \tBatch: 149 \tLoss: 1.722738265991211\n",
      "Epoch: 0 \tBatch: 150 \tLoss: 1.7304668426513672\n",
      "Epoch: 0 \tBatch: 151 \tLoss: 1.6789095401763916\n",
      "Epoch: 0 \tBatch: 152 \tLoss: 1.7354803085327148\n",
      "Epoch: 0 \tBatch: 153 \tLoss: 1.6962367296218872\n",
      "Epoch: 0 \tBatch: 154 \tLoss: 1.6905637979507446\n",
      "Epoch: 0 \tBatch: 155 \tLoss: 1.7215996980667114\n",
      "Epoch: 0 \tBatch: 156 \tLoss: 1.757737159729004\n",
      "Epoch: 0 \tBatch: 157 \tLoss: 1.748354196548462\n",
      "Epoch: 0 \tBatch: 158 \tLoss: 1.7515718936920166\n",
      "Epoch: 0 \tBatch: 159 \tLoss: 1.6437007188796997\n",
      "Epoch: 0 \tBatch: 160 \tLoss: 1.7537752389907837\n",
      "Epoch: 0 \tBatch: 161 \tLoss: 1.6778347492218018\n",
      "Epoch: 0 \tBatch: 162 \tLoss: 1.7054798603057861\n",
      "Epoch: 0 \tBatch: 163 \tLoss: 1.7165913581848145\n",
      "Epoch: 0 \tBatch: 164 \tLoss: 1.7093783617019653\n",
      "Epoch: 0 \tBatch: 165 \tLoss: 1.7459222078323364\n",
      "Epoch: 0 \tBatch: 166 \tLoss: 1.6864752769470215\n",
      "Epoch: 0 \tBatch: 167 \tLoss: 1.6829726696014404\n",
      "Epoch: 0 \tBatch: 168 \tLoss: 1.6660706996917725\n",
      "Epoch: 0 \tBatch: 169 \tLoss: 1.7331733703613281\n",
      "Epoch: 0 \tBatch: 170 \tLoss: 1.6804102659225464\n",
      "Epoch: 0 \tBatch: 171 \tLoss: 1.6771453619003296\n",
      "Epoch: 0 \tBatch: 172 \tLoss: 1.7357357740402222\n",
      "Epoch: 0 \tBatch: 173 \tLoss: 1.6384453773498535\n",
      "Epoch: 0 \tBatch: 174 \tLoss: 1.6715420484542847\n",
      "Epoch: 0 \tBatch: 175 \tLoss: 1.6938209533691406\n",
      "Epoch: 0 \tBatch: 176 \tLoss: 1.7137420177459717\n",
      "Epoch: 0 \tBatch: 177 \tLoss: 1.723244547843933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 178 \tLoss: 1.713692545890808\n",
      "Epoch: 0 \tBatch: 179 \tLoss: 1.659597635269165\n",
      "Epoch: 0 \tBatch: 180 \tLoss: 1.6314600706100464\n",
      "Epoch: 0 \tBatch: 181 \tLoss: 1.6624349355697632\n",
      "Epoch: 0 \tBatch: 182 \tLoss: 1.6955089569091797\n",
      "Epoch: 0 \tBatch: 183 \tLoss: 1.6627719402313232\n",
      "Epoch: 0 \tBatch: 184 \tLoss: 1.6435942649841309\n",
      "Epoch: 0 \tBatch: 185 \tLoss: 1.626241683959961\n",
      "Epoch: 0 \tBatch: 186 \tLoss: 1.6766678094863892\n",
      "Epoch: 0 \tBatch: 187 \tLoss: 1.6728447675704956\n",
      "Epoch: 0 \tBatch: 188 \tLoss: 1.6727240085601807\n",
      "Epoch: 0 \tBatch: 189 \tLoss: 1.600998878479004\n",
      "Epoch: 0 \tBatch: 190 \tLoss: 1.650092363357544\n",
      "Epoch: 0 \tBatch: 191 \tLoss: 1.6419106721878052\n",
      "Epoch: 0 \tBatch: 192 \tLoss: 1.6307591199874878\n",
      "Epoch: 0 \tBatch: 193 \tLoss: 1.594023585319519\n",
      "Epoch: 0 \tBatch: 194 \tLoss: 1.6163080930709839\n",
      "Epoch: 0 \tBatch: 195 \tLoss: 1.6156433820724487\n",
      "Epoch: 0 \tBatch: 196 \tLoss: 1.585378646850586\n",
      "Epoch: 0 \tBatch: 197 \tLoss: 1.6832269430160522\n",
      "Epoch: 0 \tBatch: 198 \tLoss: 1.6018619537353516\n",
      "Epoch: 0 \tBatch: 199 \tLoss: 1.6153634786605835\n",
      "Epoch: 0 \tBatch: 200 \tLoss: 1.6196343898773193\n",
      "Epoch: 0 \tBatch: 201 \tLoss: 1.6311991214752197\n",
      "Epoch: 0 \tBatch: 202 \tLoss: 1.6249827146530151\n",
      "Epoch: 0 \tBatch: 203 \tLoss: 1.6261861324310303\n",
      "Epoch: 0 \tBatch: 204 \tLoss: 1.6529898643493652\n",
      "Epoch: 0 \tBatch: 205 \tLoss: 1.604150652885437\n",
      "Epoch: 0 \tBatch: 206 \tLoss: 1.6132296323776245\n",
      "Epoch: 0 \tBatch: 207 \tLoss: 1.6330230236053467\n",
      "Epoch: 0 \tBatch: 208 \tLoss: 1.5847910642623901\n",
      "Epoch: 0 \tBatch: 209 \tLoss: 1.640401005744934\n",
      "Epoch: 0 \tBatch: 210 \tLoss: 1.6210914850234985\n",
      "Epoch: 0 \tBatch: 211 \tLoss: 1.5963242053985596\n",
      "Epoch: 0 \tBatch: 212 \tLoss: 1.657163143157959\n",
      "Epoch: 0 \tBatch: 213 \tLoss: 1.568935513496399\n",
      "Epoch: 0 \tBatch: 214 \tLoss: 1.7086371183395386\n",
      "Epoch: 0 \tBatch: 215 \tLoss: 1.5854891538619995\n",
      "Epoch: 0 \tBatch: 216 \tLoss: 1.6285291910171509\n",
      "Epoch: 0 \tBatch: 217 \tLoss: 1.637923240661621\n",
      "Epoch: 0 \tBatch: 218 \tLoss: 1.591157078742981\n",
      "Epoch: 0 \tBatch: 219 \tLoss: 1.6105562448501587\n",
      "Epoch: 0 \tBatch: 220 \tLoss: 1.6216015815734863\n",
      "Epoch: 0 \tBatch: 221 \tLoss: 1.6369253396987915\n",
      "Epoch: 0 \tBatch: 222 \tLoss: 1.6261870861053467\n",
      "Epoch: 0 \tBatch: 223 \tLoss: 1.6768821477890015\n",
      "Epoch: 0 \tBatch: 224 \tLoss: 1.5990790128707886\n",
      "Epoch: 0 \tBatch: 225 \tLoss: 1.67689049243927\n",
      "Epoch: 0 \tBatch: 226 \tLoss: 1.59675931930542\n",
      "Epoch: 0 \tBatch: 227 \tLoss: 1.6018186807632446\n",
      "Epoch: 0 \tBatch: 228 \tLoss: 1.6013188362121582\n",
      "Epoch: 0 \tBatch: 229 \tLoss: 1.5871526002883911\n",
      "Epoch: 0 \tBatch: 230 \tLoss: 1.687415599822998\n",
      "Epoch: 0 \tBatch: 231 \tLoss: 1.6295058727264404\n",
      "Epoch: 0 \tBatch: 232 \tLoss: 1.5877423286437988\n",
      "Epoch: 0 \tBatch: 233 \tLoss: 1.625180721282959\n",
      "Epoch: 0 \tBatch: 234 \tLoss: 1.6733896732330322\n",
      "Epoch: 0 \tBatch: 235 \tLoss: 1.654576301574707\n",
      "Epoch: 0 \tBatch: 236 \tLoss: 1.6575376987457275\n",
      "Epoch: 0 \tBatch: 237 \tLoss: 1.6465259790420532\n",
      "Epoch: 0 \tBatch: 238 \tLoss: 1.722851037979126\n",
      "Epoch: 0 \tBatch: 239 \tLoss: 1.6607284545898438\n",
      "Epoch: 0 \tBatch: 240 \tLoss: 1.6264307498931885\n",
      "Epoch: 0 \tBatch: 241 \tLoss: 1.5913281440734863\n",
      "Epoch: 0 \tBatch: 242 \tLoss: 1.6181572675704956\n",
      "Epoch: 0 \tBatch: 243 \tLoss: 1.6408216953277588\n",
      "Epoch: 0 \tBatch: 244 \tLoss: 1.629579782485962\n",
      "Epoch: 0 \tBatch: 245 \tLoss: 1.6257699728012085\n",
      "Epoch: 0 \tBatch: 246 \tLoss: 1.696302890777588\n",
      "Epoch: 0 \tBatch: 247 \tLoss: 1.628936767578125\n",
      "Epoch: 0 \tBatch: 248 \tLoss: 1.6078846454620361\n",
      "Epoch: 0 \tBatch: 249 \tLoss: 1.6777819395065308\n",
      "Epoch: 0 \tBatch: 250 \tLoss: 1.607123851776123\n",
      "Epoch: 0 \tBatch: 251 \tLoss: 1.5876837968826294\n",
      "Epoch: 0 \tBatch: 252 \tLoss: 1.588541030883789\n",
      "Epoch: 0 \tBatch: 253 \tLoss: 1.5887600183486938\n",
      "Epoch: 0 \tBatch: 254 \tLoss: 1.6060644388198853\n",
      "Epoch: 0 \tBatch: 255 \tLoss: 1.6754446029663086\n",
      "Epoch: 0 \tBatch: 256 \tLoss: 1.6312355995178223\n",
      "Epoch: 0 \tBatch: 257 \tLoss: 1.6168657541275024\n",
      "Epoch: 0 \tBatch: 258 \tLoss: 1.6013058423995972\n",
      "Epoch: 0 \tBatch: 259 \tLoss: 1.5947246551513672\n",
      "Epoch: 0 \tBatch: 260 \tLoss: 1.6769471168518066\n",
      "Epoch: 0 \tBatch: 261 \tLoss: 1.5960882902145386\n",
      "Epoch: 0 \tBatch: 262 \tLoss: 1.6315172910690308\n",
      "Epoch: 0 \tBatch: 263 \tLoss: 1.6381638050079346\n",
      "Epoch: 0 \tBatch: 264 \tLoss: 1.674713373184204\n",
      "Epoch: 0 \tBatch: 265 \tLoss: 1.570986032485962\n",
      "Epoch: 0 \tBatch: 266 \tLoss: 1.5856562852859497\n",
      "Epoch: 0 \tBatch: 267 \tLoss: 1.672349452972412\n",
      "Epoch: 0 \tBatch: 268 \tLoss: 1.5867323875427246\n",
      "Epoch: 0 \tBatch: 269 \tLoss: 1.5471340417861938\n",
      "Epoch: 0 \tBatch: 270 \tLoss: 1.6085326671600342\n",
      "Epoch: 0 \tBatch: 271 \tLoss: 1.6541298627853394\n",
      "Epoch: 0 \tBatch: 272 \tLoss: 1.6352572441101074\n",
      "Epoch: 0 \tBatch: 273 \tLoss: 1.5494898557662964\n",
      "Epoch: 0 \tBatch: 274 \tLoss: 1.6017296314239502\n",
      "Epoch: 0 \tBatch: 275 \tLoss: 1.6245216131210327\n",
      "Epoch: 0 \tBatch: 276 \tLoss: 1.6097679138183594\n",
      "Epoch: 0 \tBatch: 277 \tLoss: 1.6106725931167603\n",
      "Epoch: 0 \tBatch: 278 \tLoss: 1.6274458169937134\n",
      "Epoch: 0 \tBatch: 279 \tLoss: 1.5820767879486084\n",
      "Epoch: 0 \tBatch: 280 \tLoss: 1.6168253421783447\n",
      "Epoch: 0 \tBatch: 281 \tLoss: 1.6833313703536987\n",
      "Epoch: 0 \tBatch: 282 \tLoss: 1.5858865976333618\n",
      "Epoch: 0 \tBatch: 283 \tLoss: 1.5988374948501587\n",
      "Epoch: 0 \tBatch: 284 \tLoss: 1.6230356693267822\n",
      "Epoch: 0 \tBatch: 285 \tLoss: 1.601441740989685\n",
      "Epoch: 0 \tBatch: 286 \tLoss: 1.6484675407409668\n",
      "Epoch: 0 \tBatch: 287 \tLoss: 1.6069802045822144\n",
      "Epoch: 0 \tBatch: 288 \tLoss: 1.6091524362564087\n",
      "Epoch: 0 \tBatch: 289 \tLoss: 1.6079334020614624\n",
      "Epoch: 0 \tBatch: 290 \tLoss: 1.6055575609207153\n",
      "Epoch: 0 \tBatch: 291 \tLoss: 1.651026725769043\n",
      "Epoch: 0 \tBatch: 292 \tLoss: 1.6135989427566528\n",
      "Epoch: 0 \tBatch: 293 \tLoss: 1.596282958984375\n",
      "Epoch: 0 \tBatch: 294 \tLoss: 1.590567708015442\n",
      "Epoch: 0 \tBatch: 295 \tLoss: 1.6107728481292725\n",
      "Epoch: 0 \tBatch: 296 \tLoss: 1.557700276374817\n",
      "Epoch: 0 \tBatch: 297 \tLoss: 1.573563814163208\n",
      "Epoch: 0 \tBatch: 298 \tLoss: 1.5580989122390747\n",
      "Epoch: 0 \tBatch: 299 \tLoss: 1.606291651725769\n",
      "Epoch: 0 \tBatch: 300 \tLoss: 1.615978479385376\n",
      "Epoch: 0 \tBatch: 301 \tLoss: 1.5524777173995972\n",
      "Epoch: 0 \tBatch: 302 \tLoss: 1.6080738306045532\n",
      "Epoch: 0 \tBatch: 303 \tLoss: 1.6031891107559204\n",
      "Epoch: 0 \tBatch: 304 \tLoss: 1.5856740474700928\n",
      "Epoch: 0 \tBatch: 305 \tLoss: 1.6389827728271484\n",
      "Epoch: 0 \tBatch: 306 \tLoss: 1.6198519468307495\n",
      "Epoch: 0 \tBatch: 307 \tLoss: 1.5374380350112915\n",
      "Epoch: 0 \tBatch: 308 \tLoss: 1.606773853302002\n",
      "Epoch: 0 \tBatch: 309 \tLoss: 1.550808072090149\n",
      "Epoch: 0 \tBatch: 310 \tLoss: 1.6468281745910645\n",
      "Epoch: 0 \tBatch: 311 \tLoss: 1.6054658889770508\n",
      "Epoch: 0 \tBatch: 312 \tLoss: 1.5799565315246582\n",
      "Epoch: 0 \tBatch: 313 \tLoss: 1.6105937957763672\n",
      "Epoch: 0 \tBatch: 314 \tLoss: 1.6234809160232544\n",
      "Epoch: 0 \tBatch: 315 \tLoss: 1.571608543395996\n",
      "Epoch: 0 \tBatch: 316 \tLoss: 1.5546491146087646\n",
      "Epoch: 0 \tBatch: 317 \tLoss: 1.5545827150344849\n",
      "Epoch: 0 \tBatch: 318 \tLoss: 1.5710560083389282\n",
      "Epoch: 0 \tBatch: 319 \tLoss: 1.6177963018417358\n",
      "Epoch: 0 \tBatch: 320 \tLoss: 1.5916720628738403\n",
      "Epoch: 0 \tBatch: 321 \tLoss: 1.5802360773086548\n",
      "Epoch: 0 \tBatch: 322 \tLoss: 1.5790547132492065\n",
      "Epoch: 0 \tBatch: 323 \tLoss: 1.631485939025879\n",
      "Epoch: 0 \tBatch: 324 \tLoss: 1.6217528581619263\n",
      "Epoch: 0 \tBatch: 325 \tLoss: 1.6107381582260132\n",
      "Epoch: 0 \tBatch: 326 \tLoss: 1.5929960012435913\n",
      "Epoch: 0 \tBatch: 327 \tLoss: 1.6403813362121582\n",
      "Epoch: 0 \tBatch: 328 \tLoss: 1.5503442287445068\n",
      "Epoch: 0 \tBatch: 329 \tLoss: 1.6063605546951294\n",
      "Epoch: 0 \tBatch: 330 \tLoss: 1.5568253993988037\n",
      "Epoch: 0 \tBatch: 331 \tLoss: 1.6068683862686157\n",
      "Epoch: 0 \tBatch: 332 \tLoss: 1.611145257949829\n",
      "Epoch: 0 \tBatch: 333 \tLoss: 1.5870646238327026\n",
      "Epoch: 0 \tBatch: 334 \tLoss: 1.6032016277313232\n",
      "Epoch: 0 \tBatch: 335 \tLoss: 1.5955461263656616\n",
      "Epoch: 0 \tBatch: 336 \tLoss: 1.604895830154419\n",
      "Epoch: 0 \tBatch: 337 \tLoss: 1.5858213901519775\n",
      "Epoch: 0 \tBatch: 338 \tLoss: 1.5990139245986938\n",
      "Epoch: 0 \tBatch: 339 \tLoss: 1.532057285308838\n",
      "Epoch: 0 \tBatch: 340 \tLoss: 1.5026626586914062\n",
      "Epoch: 0 \tBatch: 341 \tLoss: 1.5050795078277588\n",
      "Epoch: 0 \tBatch: 342 \tLoss: 1.6226879358291626\n",
      "Epoch: 0 \tBatch: 343 \tLoss: 1.600345492362976\n",
      "Epoch: 0 \tBatch: 344 \tLoss: 1.551767110824585\n",
      "Epoch: 0 \tBatch: 345 \tLoss: 1.5303083658218384\n",
      "Epoch: 0 \tBatch: 346 \tLoss: 1.530536413192749\n",
      "Epoch: 0 \tBatch: 347 \tLoss: 1.5485122203826904\n",
      "Epoch: 0 \tBatch: 348 \tLoss: 1.550240397453308\n",
      "Epoch: 0 \tBatch: 349 \tLoss: 1.5364972352981567\n",
      "Epoch: 0 \tBatch: 350 \tLoss: 1.5009288787841797\n",
      "Epoch: 0 \tBatch: 351 \tLoss: 1.528814435005188\n",
      "Epoch: 0 \tBatch: 352 \tLoss: 1.5511212348937988\n",
      "Epoch: 0 \tBatch: 353 \tLoss: 1.5214983224868774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 354 \tLoss: 1.5138620138168335\n",
      "Epoch: 0 \tBatch: 355 \tLoss: 1.526913046836853\n",
      "Epoch: 0 \tBatch: 356 \tLoss: 1.5491578578948975\n",
      "Epoch: 0 \tBatch: 357 \tLoss: 1.5334943532943726\n",
      "Epoch: 0 \tBatch: 358 \tLoss: 1.5285252332687378\n",
      "Epoch: 0 \tBatch: 359 \tLoss: 1.5229631662368774\n",
      "Epoch: 0 \tBatch: 360 \tLoss: 1.526686668395996\n",
      "Epoch: 0 \tBatch: 361 \tLoss: 1.5401378870010376\n",
      "Epoch: 0 \tBatch: 362 \tLoss: 1.5259085893630981\n",
      "Epoch: 0 \tBatch: 363 \tLoss: 1.5432422161102295\n",
      "Epoch: 0 \tBatch: 364 \tLoss: 1.5665717124938965\n",
      "Epoch: 0 \tBatch: 365 \tLoss: 1.541891098022461\n",
      "Epoch: 0 \tBatch: 366 \tLoss: 1.4990313053131104\n",
      "Epoch: 0 \tBatch: 367 \tLoss: 1.5506958961486816\n",
      "Epoch: 0 \tBatch: 368 \tLoss: 1.5198251008987427\n",
      "Epoch: 0 \tBatch: 369 \tLoss: 1.5446444749832153\n",
      "Epoch: 0 \tBatch: 370 \tLoss: 1.517829179763794\n",
      "Epoch: 0 \tBatch: 371 \tLoss: 1.4970484972000122\n",
      "Epoch: 0 \tBatch: 372 \tLoss: 1.5144068002700806\n",
      "Epoch: 0 \tBatch: 373 \tLoss: 1.5275789499282837\n",
      "Epoch: 0 \tBatch: 374 \tLoss: 1.49545156955719\n",
      "Epoch: 0 \tBatch: 375 \tLoss: 1.5053958892822266\n",
      "Epoch: 0 \tBatch: 376 \tLoss: 1.5464694499969482\n",
      "Epoch: 0 \tBatch: 377 \tLoss: 1.5430934429168701\n",
      "Epoch: 0 \tBatch: 378 \tLoss: 1.514984130859375\n",
      "Epoch: 0 \tBatch: 379 \tLoss: 1.5040658712387085\n",
      "Epoch: 0 \tBatch: 380 \tLoss: 1.5212446451187134\n",
      "Epoch: 0 \tBatch: 381 \tLoss: 1.538170337677002\n",
      "Epoch: 0 \tBatch: 382 \tLoss: 1.5353838205337524\n",
      "Epoch: 0 \tBatch: 383 \tLoss: 1.5276544094085693\n",
      "Epoch: 0 \tBatch: 384 \tLoss: 1.4826743602752686\n",
      "Epoch: 0 \tBatch: 385 \tLoss: 1.5213412046432495\n",
      "Epoch: 0 \tBatch: 386 \tLoss: 1.529096007347107\n",
      "Epoch: 0 \tBatch: 387 \tLoss: 1.5136783123016357\n",
      "Epoch: 0 \tBatch: 388 \tLoss: 1.5022006034851074\n",
      "Epoch: 0 \tBatch: 389 \tLoss: 1.5008560419082642\n",
      "Epoch: 0 \tBatch: 390 \tLoss: 1.5133813619613647\n",
      "Epoch: 1 \tBatch: 0 \tLoss: 1.505846381187439\n",
      "Epoch: 1 \tBatch: 1 \tLoss: 1.5487830638885498\n",
      "Epoch: 1 \tBatch: 2 \tLoss: 1.5149458646774292\n",
      "Epoch: 1 \tBatch: 3 \tLoss: 1.5132217407226562\n",
      "Epoch: 1 \tBatch: 4 \tLoss: 1.515345811843872\n",
      "Epoch: 1 \tBatch: 5 \tLoss: 1.4964522123336792\n",
      "Epoch: 1 \tBatch: 6 \tLoss: 1.5217331647872925\n",
      "Epoch: 1 \tBatch: 7 \tLoss: 1.4919419288635254\n",
      "Epoch: 1 \tBatch: 8 \tLoss: 1.5185468196868896\n",
      "Epoch: 1 \tBatch: 9 \tLoss: 1.5180169343948364\n",
      "Epoch: 1 \tBatch: 10 \tLoss: 1.4952268600463867\n",
      "Epoch: 1 \tBatch: 11 \tLoss: 1.5049132108688354\n",
      "Epoch: 1 \tBatch: 12 \tLoss: 1.5263028144836426\n",
      "Epoch: 1 \tBatch: 13 \tLoss: 1.491647481918335\n",
      "Epoch: 1 \tBatch: 14 \tLoss: 1.5057992935180664\n",
      "Epoch: 1 \tBatch: 15 \tLoss: 1.4699116945266724\n",
      "Epoch: 1 \tBatch: 16 \tLoss: 1.5112698078155518\n",
      "Epoch: 1 \tBatch: 17 \tLoss: 1.505484700202942\n",
      "Epoch: 1 \tBatch: 18 \tLoss: 1.4992094039916992\n",
      "Epoch: 1 \tBatch: 19 \tLoss: 1.4966553449630737\n",
      "Epoch: 1 \tBatch: 20 \tLoss: 1.512296438217163\n",
      "Epoch: 1 \tBatch: 21 \tLoss: 1.5162793397903442\n",
      "Epoch: 1 \tBatch: 22 \tLoss: 1.4896639585494995\n",
      "Epoch: 1 \tBatch: 23 \tLoss: 1.5113155841827393\n",
      "Epoch: 1 \tBatch: 24 \tLoss: 1.5133596658706665\n",
      "Epoch: 1 \tBatch: 25 \tLoss: 1.5235897302627563\n",
      "Epoch: 1 \tBatch: 26 \tLoss: 1.5134062767028809\n",
      "Epoch: 1 \tBatch: 27 \tLoss: 1.493586778640747\n",
      "Epoch: 1 \tBatch: 28 \tLoss: 1.513107419013977\n",
      "Epoch: 1 \tBatch: 29 \tLoss: 1.5443726778030396\n",
      "Epoch: 1 \tBatch: 30 \tLoss: 1.5038634538650513\n",
      "Epoch: 1 \tBatch: 31 \tLoss: 1.4843111038208008\n",
      "Epoch: 1 \tBatch: 32 \tLoss: 1.4921698570251465\n",
      "Epoch: 1 \tBatch: 33 \tLoss: 1.499035358428955\n",
      "Epoch: 1 \tBatch: 34 \tLoss: 1.4913924932479858\n",
      "Epoch: 1 \tBatch: 35 \tLoss: 1.543546438217163\n",
      "Epoch: 1 \tBatch: 36 \tLoss: 1.496368169784546\n",
      "Epoch: 1 \tBatch: 37 \tLoss: 1.5321869850158691\n",
      "Epoch: 1 \tBatch: 38 \tLoss: 1.505279779434204\n",
      "Epoch: 1 \tBatch: 39 \tLoss: 1.4887832403182983\n",
      "Epoch: 1 \tBatch: 40 \tLoss: 1.5051385164260864\n",
      "Epoch: 1 \tBatch: 41 \tLoss: 1.508510947227478\n",
      "Epoch: 1 \tBatch: 42 \tLoss: 1.4971764087677002\n",
      "Epoch: 1 \tBatch: 43 \tLoss: 1.4955158233642578\n",
      "Epoch: 1 \tBatch: 44 \tLoss: 1.4979019165039062\n",
      "Epoch: 1 \tBatch: 45 \tLoss: 1.4942073822021484\n",
      "Epoch: 1 \tBatch: 46 \tLoss: 1.5128889083862305\n",
      "Epoch: 1 \tBatch: 47 \tLoss: 1.5008046627044678\n",
      "Epoch: 1 \tBatch: 48 \tLoss: 1.4991633892059326\n",
      "Epoch: 1 \tBatch: 49 \tLoss: 1.5212090015411377\n",
      "Epoch: 1 \tBatch: 50 \tLoss: 1.4958640336990356\n",
      "Epoch: 1 \tBatch: 51 \tLoss: 1.4919018745422363\n",
      "Epoch: 1 \tBatch: 52 \tLoss: 1.4770158529281616\n",
      "Epoch: 1 \tBatch: 53 \tLoss: 1.495419979095459\n",
      "Epoch: 1 \tBatch: 54 \tLoss: 1.5097943544387817\n",
      "Epoch: 1 \tBatch: 55 \tLoss: 1.4882643222808838\n",
      "Epoch: 1 \tBatch: 56 \tLoss: 1.507550597190857\n",
      "Epoch: 1 \tBatch: 57 \tLoss: 1.5171531438827515\n",
      "Epoch: 1 \tBatch: 58 \tLoss: 1.4996907711029053\n",
      "Epoch: 1 \tBatch: 59 \tLoss: 1.4744595289230347\n",
      "Epoch: 1 \tBatch: 60 \tLoss: 1.4977667331695557\n",
      "Epoch: 1 \tBatch: 61 \tLoss: 1.486316442489624\n",
      "Epoch: 1 \tBatch: 62 \tLoss: 1.5077537298202515\n",
      "Epoch: 1 \tBatch: 63 \tLoss: 1.5117346048355103\n",
      "Epoch: 1 \tBatch: 64 \tLoss: 1.4971809387207031\n",
      "Epoch: 1 \tBatch: 65 \tLoss: 1.508863925933838\n",
      "Epoch: 1 \tBatch: 66 \tLoss: 1.496127963066101\n",
      "Epoch: 1 \tBatch: 67 \tLoss: 1.5050984621047974\n",
      "Epoch: 1 \tBatch: 68 \tLoss: 1.4892865419387817\n",
      "Epoch: 1 \tBatch: 69 \tLoss: 1.5323752164840698\n",
      "Epoch: 1 \tBatch: 70 \tLoss: 1.494469165802002\n",
      "Epoch: 1 \tBatch: 71 \tLoss: 1.543881893157959\n",
      "Epoch: 1 \tBatch: 72 \tLoss: 1.4722870588302612\n",
      "Epoch: 1 \tBatch: 73 \tLoss: 1.5315617322921753\n",
      "Epoch: 1 \tBatch: 74 \tLoss: 1.4973312616348267\n",
      "Epoch: 1 \tBatch: 75 \tLoss: 1.5218980312347412\n",
      "Epoch: 1 \tBatch: 76 \tLoss: 1.5154608488082886\n",
      "Epoch: 1 \tBatch: 77 \tLoss: 1.5037226676940918\n",
      "Epoch: 1 \tBatch: 78 \tLoss: 1.4884923696517944\n",
      "Epoch: 1 \tBatch: 79 \tLoss: 1.508470058441162\n",
      "Epoch: 1 \tBatch: 80 \tLoss: 1.4917641878128052\n",
      "Epoch: 1 \tBatch: 81 \tLoss: 1.4889402389526367\n",
      "Epoch: 1 \tBatch: 82 \tLoss: 1.4920858144760132\n",
      "Epoch: 1 \tBatch: 83 \tLoss: 1.5158944129943848\n",
      "Epoch: 1 \tBatch: 84 \tLoss: 1.5049608945846558\n",
      "Epoch: 1 \tBatch: 85 \tLoss: 1.47355318069458\n",
      "Epoch: 1 \tBatch: 86 \tLoss: 1.4957633018493652\n",
      "Epoch: 1 \tBatch: 87 \tLoss: 1.5226794481277466\n",
      "Epoch: 1 \tBatch: 88 \tLoss: 1.4819161891937256\n",
      "Epoch: 1 \tBatch: 89 \tLoss: 1.485978603363037\n",
      "Epoch: 1 \tBatch: 90 \tLoss: 1.4929085969924927\n",
      "Epoch: 1 \tBatch: 91 \tLoss: 1.5094506740570068\n",
      "Epoch: 1 \tBatch: 92 \tLoss: 1.5184428691864014\n",
      "Epoch: 1 \tBatch: 93 \tLoss: 1.496511459350586\n",
      "Epoch: 1 \tBatch: 94 \tLoss: 1.503211498260498\n",
      "Epoch: 1 \tBatch: 95 \tLoss: 1.5027676820755005\n",
      "Epoch: 1 \tBatch: 96 \tLoss: 1.4949747323989868\n",
      "Epoch: 1 \tBatch: 97 \tLoss: 1.490260124206543\n",
      "Epoch: 1 \tBatch: 98 \tLoss: 1.4959518909454346\n",
      "Epoch: 1 \tBatch: 99 \tLoss: 1.474177360534668\n",
      "Epoch: 1 \tBatch: 100 \tLoss: 1.5058777332305908\n",
      "Epoch: 1 \tBatch: 101 \tLoss: 1.4983463287353516\n",
      "Epoch: 1 \tBatch: 102 \tLoss: 1.4835366010665894\n",
      "Epoch: 1 \tBatch: 103 \tLoss: 1.4727047681808472\n",
      "Epoch: 1 \tBatch: 104 \tLoss: 1.5276976823806763\n",
      "Epoch: 1 \tBatch: 105 \tLoss: 1.4907711744308472\n",
      "Epoch: 1 \tBatch: 106 \tLoss: 1.5316451787948608\n",
      "Epoch: 1 \tBatch: 107 \tLoss: 1.4964897632598877\n",
      "Epoch: 1 \tBatch: 108 \tLoss: 1.4968639612197876\n",
      "Epoch: 1 \tBatch: 109 \tLoss: 1.5070514678955078\n",
      "Epoch: 1 \tBatch: 110 \tLoss: 1.503546953201294\n",
      "Epoch: 1 \tBatch: 111 \tLoss: 1.5027214288711548\n",
      "Epoch: 1 \tBatch: 112 \tLoss: 1.4878214597702026\n",
      "Epoch: 1 \tBatch: 113 \tLoss: 1.5010392665863037\n",
      "Epoch: 1 \tBatch: 114 \tLoss: 1.5034469366073608\n",
      "Epoch: 1 \tBatch: 115 \tLoss: 1.5109089612960815\n",
      "Epoch: 1 \tBatch: 116 \tLoss: 1.474013328552246\n",
      "Epoch: 1 \tBatch: 117 \tLoss: 1.4796024560928345\n",
      "Epoch: 1 \tBatch: 118 \tLoss: 1.480467677116394\n",
      "Epoch: 1 \tBatch: 119 \tLoss: 1.5031896829605103\n",
      "Epoch: 1 \tBatch: 120 \tLoss: 1.4997363090515137\n",
      "Epoch: 1 \tBatch: 121 \tLoss: 1.499597191810608\n",
      "Epoch: 1 \tBatch: 122 \tLoss: 1.548553466796875\n",
      "Epoch: 1 \tBatch: 123 \tLoss: 1.522213101387024\n",
      "Epoch: 1 \tBatch: 124 \tLoss: 1.499320149421692\n",
      "Epoch: 1 \tBatch: 125 \tLoss: 1.4876635074615479\n",
      "Epoch: 1 \tBatch: 126 \tLoss: 1.4797476530075073\n",
      "Epoch: 1 \tBatch: 127 \tLoss: 1.5059757232666016\n",
      "Epoch: 1 \tBatch: 128 \tLoss: 1.4954442977905273\n",
      "Epoch: 1 \tBatch: 129 \tLoss: 1.4907704591751099\n",
      "Epoch: 1 \tBatch: 130 \tLoss: 1.4849461317062378\n",
      "Epoch: 1 \tBatch: 131 \tLoss: 1.504656195640564\n",
      "Epoch: 1 \tBatch: 132 \tLoss: 1.476703405380249\n",
      "Epoch: 1 \tBatch: 133 \tLoss: 1.4857267141342163\n",
      "Epoch: 1 \tBatch: 134 \tLoss: 1.4850246906280518\n",
      "Epoch: 1 \tBatch: 135 \tLoss: 1.5219485759735107\n",
      "Epoch: 1 \tBatch: 136 \tLoss: 1.5198107957839966\n",
      "Epoch: 1 \tBatch: 137 \tLoss: 1.4933305978775024\n",
      "Epoch: 1 \tBatch: 138 \tLoss: 1.5073484182357788\n",
      "Epoch: 1 \tBatch: 139 \tLoss: 1.5044317245483398\n",
      "Epoch: 1 \tBatch: 140 \tLoss: 1.5052958726882935\n",
      "Epoch: 1 \tBatch: 141 \tLoss: 1.4837450981140137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 142 \tLoss: 1.4909154176712036\n",
      "Epoch: 1 \tBatch: 143 \tLoss: 1.5024807453155518\n",
      "Epoch: 1 \tBatch: 144 \tLoss: 1.4870294332504272\n",
      "Epoch: 1 \tBatch: 145 \tLoss: 1.5017542839050293\n",
      "Epoch: 1 \tBatch: 146 \tLoss: 1.4903665781021118\n",
      "Epoch: 1 \tBatch: 147 \tLoss: 1.4887731075286865\n",
      "Epoch: 1 \tBatch: 148 \tLoss: 1.487028956413269\n",
      "Epoch: 1 \tBatch: 149 \tLoss: 1.4851371049880981\n",
      "Epoch: 1 \tBatch: 150 \tLoss: 1.5047266483306885\n",
      "Epoch: 1 \tBatch: 151 \tLoss: 1.4883495569229126\n",
      "Epoch: 1 \tBatch: 152 \tLoss: 1.4879975318908691\n",
      "Epoch: 1 \tBatch: 153 \tLoss: 1.5069971084594727\n",
      "Epoch: 1 \tBatch: 154 \tLoss: 1.5098211765289307\n",
      "Epoch: 1 \tBatch: 155 \tLoss: 1.4982470273971558\n",
      "Epoch: 1 \tBatch: 156 \tLoss: 1.4945515394210815\n",
      "Epoch: 1 \tBatch: 157 \tLoss: 1.5062065124511719\n",
      "Epoch: 1 \tBatch: 158 \tLoss: 1.486595869064331\n",
      "Epoch: 1 \tBatch: 159 \tLoss: 1.4981462955474854\n",
      "Epoch: 1 \tBatch: 160 \tLoss: 1.4888592958450317\n",
      "Epoch: 1 \tBatch: 161 \tLoss: 1.491750955581665\n",
      "Epoch: 1 \tBatch: 162 \tLoss: 1.501566767692566\n",
      "Epoch: 1 \tBatch: 163 \tLoss: 1.5149328708648682\n",
      "Epoch: 1 \tBatch: 164 \tLoss: 1.473357915878296\n",
      "Epoch: 1 \tBatch: 165 \tLoss: 1.486964464187622\n",
      "Epoch: 1 \tBatch: 166 \tLoss: 1.526117205619812\n",
      "Epoch: 1 \tBatch: 167 \tLoss: 1.512035846710205\n",
      "Epoch: 1 \tBatch: 168 \tLoss: 1.4874013662338257\n",
      "Epoch: 1 \tBatch: 169 \tLoss: 1.481095314025879\n",
      "Epoch: 1 \tBatch: 170 \tLoss: 1.4816312789916992\n",
      "Epoch: 1 \tBatch: 171 \tLoss: 1.5173063278198242\n",
      "Epoch: 1 \tBatch: 172 \tLoss: 1.491523027420044\n",
      "Epoch: 1 \tBatch: 173 \tLoss: 1.487304449081421\n",
      "Epoch: 1 \tBatch: 174 \tLoss: 1.5014454126358032\n",
      "Epoch: 1 \tBatch: 175 \tLoss: 1.5090243816375732\n",
      "Epoch: 1 \tBatch: 176 \tLoss: 1.4966349601745605\n",
      "Epoch: 1 \tBatch: 177 \tLoss: 1.4856044054031372\n",
      "Epoch: 1 \tBatch: 178 \tLoss: 1.495335578918457\n",
      "Epoch: 1 \tBatch: 179 \tLoss: 1.4845584630966187\n",
      "Epoch: 1 \tBatch: 180 \tLoss: 1.494570255279541\n",
      "Epoch: 1 \tBatch: 181 \tLoss: 1.5014666318893433\n",
      "Epoch: 1 \tBatch: 182 \tLoss: 1.4893826246261597\n",
      "Epoch: 1 \tBatch: 183 \tLoss: 1.4875283241271973\n",
      "Epoch: 1 \tBatch: 184 \tLoss: 1.4879870414733887\n",
      "Epoch: 1 \tBatch: 185 \tLoss: 1.4842599630355835\n",
      "Epoch: 1 \tBatch: 186 \tLoss: 1.5088142156600952\n",
      "Epoch: 1 \tBatch: 187 \tLoss: 1.4801427125930786\n",
      "Epoch: 1 \tBatch: 188 \tLoss: 1.4987802505493164\n",
      "Epoch: 1 \tBatch: 189 \tLoss: 1.4822722673416138\n",
      "Epoch: 1 \tBatch: 190 \tLoss: 1.4990733861923218\n",
      "Epoch: 1 \tBatch: 191 \tLoss: 1.5152767896652222\n",
      "Epoch: 1 \tBatch: 192 \tLoss: 1.4925696849822998\n",
      "Epoch: 1 \tBatch: 193 \tLoss: 1.498830795288086\n",
      "Epoch: 1 \tBatch: 194 \tLoss: 1.4987937211990356\n",
      "Epoch: 1 \tBatch: 195 \tLoss: 1.4836426973342896\n",
      "Epoch: 1 \tBatch: 196 \tLoss: 1.4881153106689453\n",
      "Epoch: 1 \tBatch: 197 \tLoss: 1.486550211906433\n",
      "Epoch: 1 \tBatch: 198 \tLoss: 1.4904807806015015\n",
      "Epoch: 1 \tBatch: 199 \tLoss: 1.4903699159622192\n",
      "Epoch: 1 \tBatch: 200 \tLoss: 1.5003401041030884\n",
      "Epoch: 1 \tBatch: 201 \tLoss: 1.475948452949524\n",
      "Epoch: 1 \tBatch: 202 \tLoss: 1.4843868017196655\n",
      "Epoch: 1 \tBatch: 203 \tLoss: 1.5094436407089233\n",
      "Epoch: 1 \tBatch: 204 \tLoss: 1.5098121166229248\n",
      "Epoch: 1 \tBatch: 205 \tLoss: 1.4975943565368652\n",
      "Epoch: 1 \tBatch: 206 \tLoss: 1.4888067245483398\n",
      "Epoch: 1 \tBatch: 207 \tLoss: 1.498589277267456\n",
      "Epoch: 1 \tBatch: 208 \tLoss: 1.4911476373672485\n",
      "Epoch: 1 \tBatch: 209 \tLoss: 1.5411155223846436\n",
      "Epoch: 1 \tBatch: 210 \tLoss: 1.5078102350234985\n",
      "Epoch: 1 \tBatch: 211 \tLoss: 1.5037078857421875\n",
      "Epoch: 1 \tBatch: 212 \tLoss: 1.4980027675628662\n",
      "Epoch: 1 \tBatch: 213 \tLoss: 1.5024789571762085\n",
      "Epoch: 1 \tBatch: 214 \tLoss: 1.4827507734298706\n",
      "Epoch: 1 \tBatch: 215 \tLoss: 1.4769046306610107\n",
      "Epoch: 1 \tBatch: 216 \tLoss: 1.4726238250732422\n",
      "Epoch: 1 \tBatch: 217 \tLoss: 1.4887703657150269\n",
      "Epoch: 1 \tBatch: 218 \tLoss: 1.489190936088562\n",
      "Epoch: 1 \tBatch: 219 \tLoss: 1.4918044805526733\n",
      "Epoch: 1 \tBatch: 220 \tLoss: 1.484615445137024\n",
      "Epoch: 1 \tBatch: 221 \tLoss: 1.4963151216506958\n",
      "Epoch: 1 \tBatch: 222 \tLoss: 1.5205334424972534\n",
      "Epoch: 1 \tBatch: 223 \tLoss: 1.4678764343261719\n",
      "Epoch: 1 \tBatch: 224 \tLoss: 1.4975183010101318\n",
      "Epoch: 1 \tBatch: 225 \tLoss: 1.4976271390914917\n",
      "Epoch: 1 \tBatch: 226 \tLoss: 1.499849557876587\n",
      "Epoch: 1 \tBatch: 227 \tLoss: 1.534784197807312\n",
      "Epoch: 1 \tBatch: 228 \tLoss: 1.4957300424575806\n",
      "Epoch: 1 \tBatch: 229 \tLoss: 1.4775810241699219\n",
      "Epoch: 1 \tBatch: 230 \tLoss: 1.4790719747543335\n",
      "Epoch: 1 \tBatch: 231 \tLoss: 1.4921964406967163\n",
      "Epoch: 1 \tBatch: 232 \tLoss: 1.4997522830963135\n",
      "Epoch: 1 \tBatch: 233 \tLoss: 1.4785127639770508\n",
      "Epoch: 1 \tBatch: 234 \tLoss: 1.491809368133545\n",
      "Epoch: 1 \tBatch: 235 \tLoss: 1.4934414625167847\n",
      "Epoch: 1 \tBatch: 236 \tLoss: 1.4962307214736938\n",
      "Epoch: 1 \tBatch: 237 \tLoss: 1.5172580480575562\n",
      "Epoch: 1 \tBatch: 238 \tLoss: 1.4827221632003784\n",
      "Epoch: 1 \tBatch: 239 \tLoss: 1.4725464582443237\n",
      "Epoch: 1 \tBatch: 240 \tLoss: 1.497889518737793\n",
      "Epoch: 1 \tBatch: 241 \tLoss: 1.4933665990829468\n",
      "Epoch: 1 \tBatch: 242 \tLoss: 1.507857322692871\n",
      "Epoch: 1 \tBatch: 243 \tLoss: 1.495370626449585\n",
      "Epoch: 1 \tBatch: 244 \tLoss: 1.5010018348693848\n",
      "Epoch: 1 \tBatch: 245 \tLoss: 1.4754942655563354\n",
      "Epoch: 1 \tBatch: 246 \tLoss: 1.5242509841918945\n",
      "Epoch: 1 \tBatch: 247 \tLoss: 1.4854633808135986\n",
      "Epoch: 1 \tBatch: 248 \tLoss: 1.4798029661178589\n",
      "Epoch: 1 \tBatch: 249 \tLoss: 1.5107932090759277\n",
      "Epoch: 1 \tBatch: 250 \tLoss: 1.4968807697296143\n",
      "Epoch: 1 \tBatch: 251 \tLoss: 1.499377965927124\n",
      "Epoch: 1 \tBatch: 252 \tLoss: 1.4990581274032593\n",
      "Epoch: 1 \tBatch: 253 \tLoss: 1.4953241348266602\n",
      "Epoch: 1 \tBatch: 254 \tLoss: 1.4992262125015259\n",
      "Epoch: 1 \tBatch: 255 \tLoss: 1.4908270835876465\n",
      "Epoch: 1 \tBatch: 256 \tLoss: 1.513550043106079\n",
      "Epoch: 1 \tBatch: 257 \tLoss: 1.5107085704803467\n",
      "Epoch: 1 \tBatch: 258 \tLoss: 1.5013236999511719\n",
      "Epoch: 1 \tBatch: 259 \tLoss: 1.4984790086746216\n",
      "Epoch: 1 \tBatch: 260 \tLoss: 1.5042468309402466\n",
      "Epoch: 1 \tBatch: 261 \tLoss: 1.506162405014038\n",
      "Epoch: 1 \tBatch: 262 \tLoss: 1.485278844833374\n",
      "Epoch: 1 \tBatch: 263 \tLoss: 1.4862602949142456\n",
      "Epoch: 1 \tBatch: 264 \tLoss: 1.5069103240966797\n",
      "Epoch: 1 \tBatch: 265 \tLoss: 1.488348364830017\n",
      "Epoch: 1 \tBatch: 266 \tLoss: 1.5068763494491577\n",
      "Epoch: 1 \tBatch: 267 \tLoss: 1.4800647497177124\n",
      "Epoch: 1 \tBatch: 268 \tLoss: 1.4929980039596558\n",
      "Epoch: 1 \tBatch: 269 \tLoss: 1.5228586196899414\n",
      "Epoch: 1 \tBatch: 270 \tLoss: 1.486221194267273\n",
      "Epoch: 1 \tBatch: 271 \tLoss: 1.4956815242767334\n",
      "Epoch: 1 \tBatch: 272 \tLoss: 1.4690957069396973\n",
      "Epoch: 1 \tBatch: 273 \tLoss: 1.4883259534835815\n",
      "Epoch: 1 \tBatch: 274 \tLoss: 1.4976667165756226\n",
      "Epoch: 1 \tBatch: 275 \tLoss: 1.5239346027374268\n",
      "Epoch: 1 \tBatch: 276 \tLoss: 1.4935508966445923\n",
      "Epoch: 1 \tBatch: 277 \tLoss: 1.5020943880081177\n",
      "Epoch: 1 \tBatch: 278 \tLoss: 1.511864185333252\n",
      "Epoch: 1 \tBatch: 279 \tLoss: 1.4651070833206177\n",
      "Epoch: 1 \tBatch: 280 \tLoss: 1.473530888557434\n",
      "Epoch: 1 \tBatch: 281 \tLoss: 1.5098998546600342\n",
      "Epoch: 1 \tBatch: 282 \tLoss: 1.4937185049057007\n",
      "Epoch: 1 \tBatch: 283 \tLoss: 1.4747202396392822\n",
      "Epoch: 1 \tBatch: 284 \tLoss: 1.49876070022583\n",
      "Epoch: 1 \tBatch: 285 \tLoss: 1.479753017425537\n",
      "Epoch: 1 \tBatch: 286 \tLoss: 1.489763617515564\n",
      "Epoch: 1 \tBatch: 287 \tLoss: 1.5015970468521118\n",
      "Epoch: 1 \tBatch: 288 \tLoss: 1.4920158386230469\n",
      "Epoch: 1 \tBatch: 289 \tLoss: 1.4770601987838745\n",
      "Epoch: 1 \tBatch: 290 \tLoss: 1.5099061727523804\n",
      "Epoch: 1 \tBatch: 291 \tLoss: 1.4958370923995972\n",
      "Epoch: 1 \tBatch: 292 \tLoss: 1.4868184328079224\n",
      "Epoch: 1 \tBatch: 293 \tLoss: 1.5050727128982544\n",
      "Epoch: 1 \tBatch: 294 \tLoss: 1.4896727800369263\n",
      "Epoch: 1 \tBatch: 295 \tLoss: 1.5011264085769653\n",
      "Epoch: 1 \tBatch: 296 \tLoss: 1.5060501098632812\n",
      "Epoch: 1 \tBatch: 297 \tLoss: 1.5066437721252441\n",
      "Epoch: 1 \tBatch: 298 \tLoss: 1.4765753746032715\n",
      "Epoch: 1 \tBatch: 299 \tLoss: 1.479957103729248\n",
      "Epoch: 1 \tBatch: 300 \tLoss: 1.49661386013031\n",
      "Epoch: 1 \tBatch: 301 \tLoss: 1.496537208557129\n",
      "Epoch: 1 \tBatch: 302 \tLoss: 1.5071759223937988\n",
      "Epoch: 1 \tBatch: 303 \tLoss: 1.5086126327514648\n",
      "Epoch: 1 \tBatch: 304 \tLoss: 1.4908417463302612\n",
      "Epoch: 1 \tBatch: 305 \tLoss: 1.480289101600647\n",
      "Epoch: 1 \tBatch: 306 \tLoss: 1.4936563968658447\n",
      "Epoch: 1 \tBatch: 307 \tLoss: 1.4703258275985718\n",
      "Epoch: 1 \tBatch: 308 \tLoss: 1.4900760650634766\n",
      "Epoch: 1 \tBatch: 309 \tLoss: 1.485098123550415\n",
      "Epoch: 1 \tBatch: 310 \tLoss: 1.4914071559906006\n",
      "Epoch: 1 \tBatch: 311 \tLoss: 1.4992172718048096\n",
      "Epoch: 1 \tBatch: 312 \tLoss: 1.5198187828063965\n",
      "Epoch: 1 \tBatch: 313 \tLoss: 1.4700381755828857\n",
      "Epoch: 1 \tBatch: 314 \tLoss: 1.485456943511963\n",
      "Epoch: 1 \tBatch: 315 \tLoss: 1.4845993518829346\n",
      "Epoch: 1 \tBatch: 316 \tLoss: 1.5015654563903809\n",
      "Epoch: 1 \tBatch: 317 \tLoss: 1.4796829223632812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 318 \tLoss: 1.4836969375610352\n",
      "Epoch: 1 \tBatch: 319 \tLoss: 1.4910567998886108\n",
      "Epoch: 1 \tBatch: 320 \tLoss: 1.4973526000976562\n",
      "Epoch: 1 \tBatch: 321 \tLoss: 1.4881224632263184\n",
      "Epoch: 1 \tBatch: 322 \tLoss: 1.4786945581436157\n",
      "Epoch: 1 \tBatch: 323 \tLoss: 1.4934654235839844\n",
      "Epoch: 1 \tBatch: 324 \tLoss: 1.4971363544464111\n",
      "Epoch: 1 \tBatch: 325 \tLoss: 1.4865063428878784\n",
      "Epoch: 1 \tBatch: 326 \tLoss: 1.4940310716629028\n",
      "Epoch: 1 \tBatch: 327 \tLoss: 1.4854793548583984\n",
      "Epoch: 1 \tBatch: 328 \tLoss: 1.487905740737915\n",
      "Epoch: 1 \tBatch: 329 \tLoss: 1.490767478942871\n",
      "Epoch: 1 \tBatch: 330 \tLoss: 1.5188592672348022\n",
      "Epoch: 1 \tBatch: 331 \tLoss: 1.4850367307662964\n",
      "Epoch: 1 \tBatch: 332 \tLoss: 1.5025171041488647\n",
      "Epoch: 1 \tBatch: 333 \tLoss: 1.4938249588012695\n",
      "Epoch: 1 \tBatch: 334 \tLoss: 1.4817391633987427\n",
      "Epoch: 1 \tBatch: 335 \tLoss: 1.4735995531082153\n",
      "Epoch: 1 \tBatch: 336 \tLoss: 1.4946237802505493\n",
      "Epoch: 1 \tBatch: 337 \tLoss: 1.4716764688491821\n",
      "Epoch: 1 \tBatch: 338 \tLoss: 1.495974063873291\n",
      "Epoch: 1 \tBatch: 339 \tLoss: 1.4920423030853271\n",
      "Epoch: 1 \tBatch: 340 \tLoss: 1.4966777563095093\n",
      "Epoch: 1 \tBatch: 341 \tLoss: 1.4862337112426758\n",
      "Epoch: 1 \tBatch: 342 \tLoss: 1.4908571243286133\n",
      "Epoch: 1 \tBatch: 343 \tLoss: 1.4934848546981812\n",
      "Epoch: 1 \tBatch: 344 \tLoss: 1.4891486167907715\n",
      "Epoch: 1 \tBatch: 345 \tLoss: 1.4917057752609253\n",
      "Epoch: 1 \tBatch: 346 \tLoss: 1.4921082258224487\n",
      "Epoch: 1 \tBatch: 347 \tLoss: 1.498884916305542\n",
      "Epoch: 1 \tBatch: 348 \tLoss: 1.4782683849334717\n",
      "Epoch: 1 \tBatch: 349 \tLoss: 1.5380494594573975\n",
      "Epoch: 1 \tBatch: 350 \tLoss: 1.4887381792068481\n",
      "Epoch: 1 \tBatch: 351 \tLoss: 1.4860674142837524\n",
      "Epoch: 1 \tBatch: 352 \tLoss: 1.4907300472259521\n",
      "Epoch: 1 \tBatch: 353 \tLoss: 1.4691520929336548\n",
      "Epoch: 1 \tBatch: 354 \tLoss: 1.4995555877685547\n",
      "Epoch: 1 \tBatch: 355 \tLoss: 1.5089699029922485\n",
      "Epoch: 1 \tBatch: 356 \tLoss: 1.4946882724761963\n",
      "Epoch: 1 \tBatch: 357 \tLoss: 1.51998770236969\n",
      "Epoch: 1 \tBatch: 358 \tLoss: 1.4855667352676392\n",
      "Epoch: 1 \tBatch: 359 \tLoss: 1.5054149627685547\n",
      "Epoch: 1 \tBatch: 360 \tLoss: 1.5092564821243286\n",
      "Epoch: 1 \tBatch: 361 \tLoss: 1.47330904006958\n",
      "Epoch: 1 \tBatch: 362 \tLoss: 1.4854646921157837\n",
      "Epoch: 1 \tBatch: 363 \tLoss: 1.513084888458252\n",
      "Epoch: 1 \tBatch: 364 \tLoss: 1.4997254610061646\n",
      "Epoch: 1 \tBatch: 365 \tLoss: 1.489967942237854\n",
      "Epoch: 1 \tBatch: 366 \tLoss: 1.4847596883773804\n",
      "Epoch: 1 \tBatch: 367 \tLoss: 1.4965643882751465\n",
      "Epoch: 1 \tBatch: 368 \tLoss: 1.494506597518921\n",
      "Epoch: 1 \tBatch: 369 \tLoss: 1.4871290922164917\n",
      "Epoch: 1 \tBatch: 370 \tLoss: 1.4823681116104126\n",
      "Epoch: 1 \tBatch: 371 \tLoss: 1.4733250141143799\n",
      "Epoch: 1 \tBatch: 372 \tLoss: 1.4944947957992554\n",
      "Epoch: 1 \tBatch: 373 \tLoss: 1.4830178022384644\n",
      "Epoch: 1 \tBatch: 374 \tLoss: 1.5074925422668457\n",
      "Epoch: 1 \tBatch: 375 \tLoss: 1.4861253499984741\n",
      "Epoch: 1 \tBatch: 376 \tLoss: 1.4901190996170044\n",
      "Epoch: 1 \tBatch: 377 \tLoss: 1.5075057744979858\n",
      "Epoch: 1 \tBatch: 378 \tLoss: 1.4997260570526123\n",
      "Epoch: 1 \tBatch: 379 \tLoss: 1.4845571517944336\n",
      "Epoch: 1 \tBatch: 380 \tLoss: 1.4906277656555176\n",
      "Epoch: 1 \tBatch: 381 \tLoss: 1.4814518690109253\n",
      "Epoch: 1 \tBatch: 382 \tLoss: 1.4792027473449707\n",
      "Epoch: 1 \tBatch: 383 \tLoss: 1.4774196147918701\n",
      "Epoch: 1 \tBatch: 384 \tLoss: 1.5087380409240723\n",
      "Epoch: 1 \tBatch: 385 \tLoss: 1.4691975116729736\n",
      "Epoch: 1 \tBatch: 386 \tLoss: 1.4745936393737793\n",
      "Epoch: 1 \tBatch: 387 \tLoss: 1.4987369775772095\n",
      "Epoch: 1 \tBatch: 388 \tLoss: 1.4903838634490967\n",
      "Epoch: 1 \tBatch: 389 \tLoss: 1.4918503761291504\n",
      "Epoch: 1 \tBatch: 390 \tLoss: 1.470414400100708\n",
      "Epoch: 2 \tBatch: 0 \tLoss: 1.4914674758911133\n",
      "Epoch: 2 \tBatch: 1 \tLoss: 1.5078024864196777\n",
      "Epoch: 2 \tBatch: 2 \tLoss: 1.4829074144363403\n",
      "Epoch: 2 \tBatch: 3 \tLoss: 1.4866735935211182\n",
      "Epoch: 2 \tBatch: 4 \tLoss: 1.4694955348968506\n",
      "Epoch: 2 \tBatch: 5 \tLoss: 1.4835253953933716\n",
      "Epoch: 2 \tBatch: 6 \tLoss: 1.491568922996521\n",
      "Epoch: 2 \tBatch: 7 \tLoss: 1.4923232793807983\n",
      "Epoch: 2 \tBatch: 8 \tLoss: 1.469266414642334\n",
      "Epoch: 2 \tBatch: 9 \tLoss: 1.482616901397705\n",
      "Epoch: 2 \tBatch: 10 \tLoss: 1.4996567964553833\n",
      "Epoch: 2 \tBatch: 11 \tLoss: 1.4980157613754272\n",
      "Epoch: 2 \tBatch: 12 \tLoss: 1.4665117263793945\n",
      "Epoch: 2 \tBatch: 13 \tLoss: 1.481693148612976\n",
      "Epoch: 2 \tBatch: 14 \tLoss: 1.511486530303955\n",
      "Epoch: 2 \tBatch: 15 \tLoss: 1.486203908920288\n",
      "Epoch: 2 \tBatch: 16 \tLoss: 1.4859123229980469\n",
      "Epoch: 2 \tBatch: 17 \tLoss: 1.4806768894195557\n",
      "Epoch: 2 \tBatch: 18 \tLoss: 1.4950579404830933\n",
      "Epoch: 2 \tBatch: 19 \tLoss: 1.4796457290649414\n",
      "Epoch: 2 \tBatch: 20 \tLoss: 1.4715487957000732\n",
      "Epoch: 2 \tBatch: 21 \tLoss: 1.4770604372024536\n",
      "Epoch: 2 \tBatch: 22 \tLoss: 1.501686692237854\n",
      "Epoch: 2 \tBatch: 23 \tLoss: 1.475759506225586\n",
      "Epoch: 2 \tBatch: 24 \tLoss: 1.4929133653640747\n",
      "Epoch: 2 \tBatch: 25 \tLoss: 1.4728918075561523\n",
      "Epoch: 2 \tBatch: 26 \tLoss: 1.4861072301864624\n",
      "Epoch: 2 \tBatch: 27 \tLoss: 1.4950511455535889\n",
      "Epoch: 2 \tBatch: 28 \tLoss: 1.477346420288086\n",
      "Epoch: 2 \tBatch: 29 \tLoss: 1.4871999025344849\n",
      "Epoch: 2 \tBatch: 30 \tLoss: 1.476253628730774\n",
      "Epoch: 2 \tBatch: 31 \tLoss: 1.4714804887771606\n",
      "Epoch: 2 \tBatch: 32 \tLoss: 1.4750727415084839\n",
      "Epoch: 2 \tBatch: 33 \tLoss: 1.4756848812103271\n",
      "Epoch: 2 \tBatch: 34 \tLoss: 1.4930853843688965\n",
      "Epoch: 2 \tBatch: 35 \tLoss: 1.4761457443237305\n",
      "Epoch: 2 \tBatch: 36 \tLoss: 1.4791979789733887\n",
      "Epoch: 2 \tBatch: 37 \tLoss: 1.5062488317489624\n",
      "Epoch: 2 \tBatch: 38 \tLoss: 1.4952689409255981\n",
      "Epoch: 2 \tBatch: 39 \tLoss: 1.4779683351516724\n",
      "Epoch: 2 \tBatch: 40 \tLoss: 1.478589415550232\n",
      "Epoch: 2 \tBatch: 41 \tLoss: 1.4752075672149658\n",
      "Epoch: 2 \tBatch: 42 \tLoss: 1.4833041429519653\n",
      "Epoch: 2 \tBatch: 43 \tLoss: 1.5026737451553345\n",
      "Epoch: 2 \tBatch: 44 \tLoss: 1.474317193031311\n",
      "Epoch: 2 \tBatch: 45 \tLoss: 1.4912117719650269\n",
      "Epoch: 2 \tBatch: 46 \tLoss: 1.4705464839935303\n",
      "Epoch: 2 \tBatch: 47 \tLoss: 1.4776723384857178\n",
      "Epoch: 2 \tBatch: 48 \tLoss: 1.479727864265442\n",
      "Epoch: 2 \tBatch: 49 \tLoss: 1.4786182641983032\n",
      "Epoch: 2 \tBatch: 50 \tLoss: 1.4703664779663086\n",
      "Epoch: 2 \tBatch: 51 \tLoss: 1.4929906129837036\n",
      "Epoch: 2 \tBatch: 52 \tLoss: 1.484002709388733\n",
      "Epoch: 2 \tBatch: 53 \tLoss: 1.4793200492858887\n",
      "Epoch: 2 \tBatch: 54 \tLoss: 1.479990839958191\n",
      "Epoch: 2 \tBatch: 55 \tLoss: 1.47986900806427\n",
      "Epoch: 2 \tBatch: 56 \tLoss: 1.4866771697998047\n",
      "Epoch: 2 \tBatch: 57 \tLoss: 1.5148544311523438\n",
      "Epoch: 2 \tBatch: 58 \tLoss: 1.489517092704773\n",
      "Epoch: 2 \tBatch: 59 \tLoss: 1.4884132146835327\n",
      "Epoch: 2 \tBatch: 60 \tLoss: 1.4729810953140259\n",
      "Epoch: 2 \tBatch: 61 \tLoss: 1.466224193572998\n",
      "Epoch: 2 \tBatch: 62 \tLoss: 1.482224941253662\n",
      "Epoch: 2 \tBatch: 63 \tLoss: 1.4902229309082031\n",
      "Epoch: 2 \tBatch: 64 \tLoss: 1.489567518234253\n",
      "Epoch: 2 \tBatch: 65 \tLoss: 1.4895764589309692\n",
      "Epoch: 2 \tBatch: 66 \tLoss: 1.4826960563659668\n",
      "Epoch: 2 \tBatch: 67 \tLoss: 1.5022964477539062\n",
      "Epoch: 2 \tBatch: 68 \tLoss: 1.4664448499679565\n",
      "Epoch: 2 \tBatch: 69 \tLoss: 1.493044137954712\n",
      "Epoch: 2 \tBatch: 70 \tLoss: 1.4906914234161377\n",
      "Epoch: 2 \tBatch: 71 \tLoss: 1.5088144540786743\n",
      "Epoch: 2 \tBatch: 72 \tLoss: 1.4784718751907349\n",
      "Epoch: 2 \tBatch: 73 \tLoss: 1.4697129726409912\n",
      "Epoch: 2 \tBatch: 74 \tLoss: 1.4752302169799805\n",
      "Epoch: 2 \tBatch: 75 \tLoss: 1.4629840850830078\n",
      "Epoch: 2 \tBatch: 76 \tLoss: 1.4788333177566528\n",
      "Epoch: 2 \tBatch: 77 \tLoss: 1.4741551876068115\n",
      "Epoch: 2 \tBatch: 78 \tLoss: 1.4848891496658325\n",
      "Epoch: 2 \tBatch: 79 \tLoss: 1.502183437347412\n",
      "Epoch: 2 \tBatch: 80 \tLoss: 1.4738502502441406\n",
      "Epoch: 2 \tBatch: 81 \tLoss: 1.4994275569915771\n",
      "Epoch: 2 \tBatch: 82 \tLoss: 1.5038480758666992\n",
      "Epoch: 2 \tBatch: 83 \tLoss: 1.4993212223052979\n",
      "Epoch: 2 \tBatch: 84 \tLoss: 1.527730941772461\n",
      "Epoch: 2 \tBatch: 85 \tLoss: 1.4855693578720093\n",
      "Epoch: 2 \tBatch: 86 \tLoss: 1.4966785907745361\n",
      "Epoch: 2 \tBatch: 87 \tLoss: 1.4860464334487915\n",
      "Epoch: 2 \tBatch: 88 \tLoss: 1.483973741531372\n",
      "Epoch: 2 \tBatch: 89 \tLoss: 1.4977240562438965\n",
      "Epoch: 2 \tBatch: 90 \tLoss: 1.4949617385864258\n",
      "Epoch: 2 \tBatch: 91 \tLoss: 1.4723570346832275\n",
      "Epoch: 2 \tBatch: 92 \tLoss: 1.4759775400161743\n",
      "Epoch: 2 \tBatch: 93 \tLoss: 1.4989181756973267\n",
      "Epoch: 2 \tBatch: 94 \tLoss: 1.5069305896759033\n",
      "Epoch: 2 \tBatch: 95 \tLoss: 1.4760979413986206\n",
      "Epoch: 2 \tBatch: 96 \tLoss: 1.4910746812820435\n",
      "Epoch: 2 \tBatch: 97 \tLoss: 1.4758046865463257\n",
      "Epoch: 2 \tBatch: 98 \tLoss: 1.4853707551956177\n",
      "Epoch: 2 \tBatch: 99 \tLoss: 1.4820902347564697\n",
      "Epoch: 2 \tBatch: 100 \tLoss: 1.4675227403640747\n",
      "Epoch: 2 \tBatch: 101 \tLoss: 1.4695051908493042\n",
      "Epoch: 2 \tBatch: 102 \tLoss: 1.5130343437194824\n",
      "Epoch: 2 \tBatch: 103 \tLoss: 1.4713422060012817\n",
      "Epoch: 2 \tBatch: 104 \tLoss: 1.4915759563446045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 105 \tLoss: 1.4739305973052979\n",
      "Epoch: 2 \tBatch: 106 \tLoss: 1.522164225578308\n",
      "Epoch: 2 \tBatch: 107 \tLoss: 1.475660800933838\n",
      "Epoch: 2 \tBatch: 108 \tLoss: 1.473942518234253\n",
      "Epoch: 2 \tBatch: 109 \tLoss: 1.497756004333496\n",
      "Epoch: 2 \tBatch: 110 \tLoss: 1.5055688619613647\n",
      "Epoch: 2 \tBatch: 111 \tLoss: 1.5114598274230957\n",
      "Epoch: 2 \tBatch: 112 \tLoss: 1.4865056276321411\n",
      "Epoch: 2 \tBatch: 113 \tLoss: 1.4813361167907715\n",
      "Epoch: 2 \tBatch: 114 \tLoss: 1.487593173980713\n",
      "Epoch: 2 \tBatch: 115 \tLoss: 1.4871447086334229\n",
      "Epoch: 2 \tBatch: 116 \tLoss: 1.5078458786010742\n",
      "Epoch: 2 \tBatch: 117 \tLoss: 1.4962399005889893\n",
      "Epoch: 2 \tBatch: 118 \tLoss: 1.503102421760559\n",
      "Epoch: 2 \tBatch: 119 \tLoss: 1.495141863822937\n",
      "Epoch: 2 \tBatch: 120 \tLoss: 1.5114622116088867\n",
      "Epoch: 2 \tBatch: 121 \tLoss: 1.480154275894165\n",
      "Epoch: 2 \tBatch: 122 \tLoss: 1.5034759044647217\n",
      "Epoch: 2 \tBatch: 123 \tLoss: 1.5150431394577026\n",
      "Epoch: 2 \tBatch: 124 \tLoss: 1.4940211772918701\n",
      "Epoch: 2 \tBatch: 125 \tLoss: 1.5002007484436035\n",
      "Epoch: 2 \tBatch: 126 \tLoss: 1.4805094003677368\n",
      "Epoch: 2 \tBatch: 127 \tLoss: 1.4713780879974365\n",
      "Epoch: 2 \tBatch: 128 \tLoss: 1.4630671739578247\n",
      "Epoch: 2 \tBatch: 129 \tLoss: 1.4668686389923096\n",
      "Epoch: 2 \tBatch: 130 \tLoss: 1.498095989227295\n",
      "Epoch: 2 \tBatch: 131 \tLoss: 1.4935556650161743\n",
      "Epoch: 2 \tBatch: 132 \tLoss: 1.4968444108963013\n",
      "Epoch: 2 \tBatch: 133 \tLoss: 1.4906039237976074\n",
      "Epoch: 2 \tBatch: 134 \tLoss: 1.470150113105774\n",
      "Epoch: 2 \tBatch: 135 \tLoss: 1.4807156324386597\n",
      "Epoch: 2 \tBatch: 136 \tLoss: 1.502927303314209\n",
      "Epoch: 2 \tBatch: 137 \tLoss: 1.485959768295288\n",
      "Epoch: 2 \tBatch: 138 \tLoss: 1.5181080102920532\n",
      "Epoch: 2 \tBatch: 139 \tLoss: 1.49484384059906\n",
      "Epoch: 2 \tBatch: 140 \tLoss: 1.504364252090454\n",
      "Epoch: 2 \tBatch: 141 \tLoss: 1.4759211540222168\n",
      "Epoch: 2 \tBatch: 142 \tLoss: 1.4916664361953735\n",
      "Epoch: 2 \tBatch: 143 \tLoss: 1.4923263788223267\n",
      "Epoch: 2 \tBatch: 144 \tLoss: 1.4826520681381226\n",
      "Epoch: 2 \tBatch: 145 \tLoss: 1.4836090803146362\n",
      "Epoch: 2 \tBatch: 146 \tLoss: 1.4830230474472046\n",
      "Epoch: 2 \tBatch: 147 \tLoss: 1.4795223474502563\n",
      "Epoch: 2 \tBatch: 148 \tLoss: 1.4797768592834473\n",
      "Epoch: 2 \tBatch: 149 \tLoss: 1.4956393241882324\n",
      "Epoch: 2 \tBatch: 150 \tLoss: 1.4846633672714233\n",
      "Epoch: 2 \tBatch: 151 \tLoss: 1.4871959686279297\n",
      "Epoch: 2 \tBatch: 152 \tLoss: 1.4903795719146729\n",
      "Epoch: 2 \tBatch: 153 \tLoss: 1.499332070350647\n",
      "Epoch: 2 \tBatch: 154 \tLoss: 1.4798469543457031\n",
      "Epoch: 2 \tBatch: 155 \tLoss: 1.4752527475357056\n",
      "Epoch: 2 \tBatch: 156 \tLoss: 1.4716025590896606\n",
      "Epoch: 2 \tBatch: 157 \tLoss: 1.5053831338882446\n",
      "Epoch: 2 \tBatch: 158 \tLoss: 1.4949871301651\n",
      "Epoch: 2 \tBatch: 159 \tLoss: 1.474345326423645\n",
      "Epoch: 2 \tBatch: 160 \tLoss: 1.493105173110962\n",
      "Epoch: 2 \tBatch: 161 \tLoss: 1.4872530698776245\n",
      "Epoch: 2 \tBatch: 162 \tLoss: 1.5152508020401\n",
      "Epoch: 2 \tBatch: 163 \tLoss: 1.486497163772583\n",
      "Epoch: 2 \tBatch: 164 \tLoss: 1.484974980354309\n",
      "Epoch: 2 \tBatch: 165 \tLoss: 1.4980405569076538\n",
      "Epoch: 2 \tBatch: 166 \tLoss: 1.480945348739624\n",
      "Epoch: 2 \tBatch: 167 \tLoss: 1.4688392877578735\n",
      "Epoch: 2 \tBatch: 168 \tLoss: 1.469765067100525\n",
      "Epoch: 2 \tBatch: 169 \tLoss: 1.4833154678344727\n",
      "Epoch: 2 \tBatch: 170 \tLoss: 1.486513614654541\n",
      "Epoch: 2 \tBatch: 171 \tLoss: 1.494024634361267\n",
      "Epoch: 2 \tBatch: 172 \tLoss: 1.4840703010559082\n",
      "Epoch: 2 \tBatch: 173 \tLoss: 1.5188316106796265\n",
      "Epoch: 2 \tBatch: 174 \tLoss: 1.4994895458221436\n",
      "Epoch: 2 \tBatch: 175 \tLoss: 1.4683696031570435\n",
      "Epoch: 2 \tBatch: 176 \tLoss: 1.4786107540130615\n",
      "Epoch: 2 \tBatch: 177 \tLoss: 1.4854590892791748\n",
      "Epoch: 2 \tBatch: 178 \tLoss: 1.493774652481079\n",
      "Epoch: 2 \tBatch: 179 \tLoss: 1.4902245998382568\n",
      "Epoch: 2 \tBatch: 180 \tLoss: 1.4772391319274902\n",
      "Epoch: 2 \tBatch: 181 \tLoss: 1.464667797088623\n",
      "Epoch: 2 \tBatch: 182 \tLoss: 1.4954301118850708\n",
      "Epoch: 2 \tBatch: 183 \tLoss: 1.483850121498108\n",
      "Epoch: 2 \tBatch: 184 \tLoss: 1.4911030530929565\n",
      "Epoch: 2 \tBatch: 185 \tLoss: 1.4795151948928833\n",
      "Epoch: 2 \tBatch: 186 \tLoss: 1.4954437017440796\n",
      "Epoch: 2 \tBatch: 187 \tLoss: 1.5030763149261475\n",
      "Epoch: 2 \tBatch: 188 \tLoss: 1.479085922241211\n",
      "Epoch: 2 \tBatch: 189 \tLoss: 1.4807630777359009\n",
      "Epoch: 2 \tBatch: 190 \tLoss: 1.5443896055221558\n",
      "Epoch: 2 \tBatch: 191 \tLoss: 1.463107943534851\n",
      "Epoch: 2 \tBatch: 192 \tLoss: 1.4928804636001587\n",
      "Epoch: 2 \tBatch: 193 \tLoss: 1.4756081104278564\n",
      "Epoch: 2 \tBatch: 194 \tLoss: 1.4803071022033691\n",
      "Epoch: 2 \tBatch: 195 \tLoss: 1.5020021200180054\n",
      "Epoch: 2 \tBatch: 196 \tLoss: 1.4674906730651855\n",
      "Epoch: 2 \tBatch: 197 \tLoss: 1.4805268049240112\n",
      "Epoch: 2 \tBatch: 198 \tLoss: 1.4951796531677246\n",
      "Epoch: 2 \tBatch: 199 \tLoss: 1.4625111818313599\n",
      "Epoch: 2 \tBatch: 200 \tLoss: 1.491223931312561\n",
      "Epoch: 2 \tBatch: 201 \tLoss: 1.4806448221206665\n",
      "Epoch: 2 \tBatch: 202 \tLoss: 1.4753382205963135\n",
      "Epoch: 2 \tBatch: 203 \tLoss: 1.4805949926376343\n",
      "Epoch: 2 \tBatch: 204 \tLoss: 1.4907689094543457\n",
      "Epoch: 2 \tBatch: 205 \tLoss: 1.4782148599624634\n",
      "Epoch: 2 \tBatch: 206 \tLoss: 1.4644582271575928\n",
      "Epoch: 2 \tBatch: 207 \tLoss: 1.471921443939209\n",
      "Epoch: 2 \tBatch: 208 \tLoss: 1.4942700862884521\n",
      "Epoch: 2 \tBatch: 209 \tLoss: 1.4881397485733032\n",
      "Epoch: 2 \tBatch: 210 \tLoss: 1.499975323677063\n",
      "Epoch: 2 \tBatch: 211 \tLoss: 1.486340045928955\n",
      "Epoch: 2 \tBatch: 212 \tLoss: 1.4881287813186646\n",
      "Epoch: 2 \tBatch: 213 \tLoss: 1.47504460811615\n",
      "Epoch: 2 \tBatch: 214 \tLoss: 1.4802149534225464\n",
      "Epoch: 2 \tBatch: 215 \tLoss: 1.4845035076141357\n",
      "Epoch: 2 \tBatch: 216 \tLoss: 1.4951603412628174\n",
      "Epoch: 2 \tBatch: 217 \tLoss: 1.4704551696777344\n",
      "Epoch: 2 \tBatch: 218 \tLoss: 1.5124642848968506\n",
      "Epoch: 2 \tBatch: 219 \tLoss: 1.4854954481124878\n",
      "Epoch: 2 \tBatch: 220 \tLoss: 1.4760501384735107\n",
      "Epoch: 2 \tBatch: 221 \tLoss: 1.492168664932251\n",
      "Epoch: 2 \tBatch: 222 \tLoss: 1.4841293096542358\n",
      "Epoch: 2 \tBatch: 223 \tLoss: 1.479845643043518\n",
      "Epoch: 2 \tBatch: 224 \tLoss: 1.491005301475525\n",
      "Epoch: 2 \tBatch: 225 \tLoss: 1.4907066822052002\n",
      "Epoch: 2 \tBatch: 226 \tLoss: 1.4810982942581177\n",
      "Epoch: 2 \tBatch: 227 \tLoss: 1.5144436359405518\n",
      "Epoch: 2 \tBatch: 228 \tLoss: 1.4718413352966309\n",
      "Epoch: 2 \tBatch: 229 \tLoss: 1.4781442880630493\n",
      "Epoch: 2 \tBatch: 230 \tLoss: 1.484519600868225\n",
      "Epoch: 2 \tBatch: 231 \tLoss: 1.5166798830032349\n",
      "Epoch: 2 \tBatch: 232 \tLoss: 1.4998315572738647\n",
      "Epoch: 2 \tBatch: 233 \tLoss: 1.4853122234344482\n",
      "Epoch: 2 \tBatch: 234 \tLoss: 1.4881974458694458\n",
      "Epoch: 2 \tBatch: 235 \tLoss: 1.4694699048995972\n",
      "Epoch: 2 \tBatch: 236 \tLoss: 1.483782410621643\n",
      "Epoch: 2 \tBatch: 237 \tLoss: 1.4763444662094116\n",
      "Epoch: 2 \tBatch: 238 \tLoss: 1.4895509481430054\n",
      "Epoch: 2 \tBatch: 239 \tLoss: 1.4747587442398071\n",
      "Epoch: 2 \tBatch: 240 \tLoss: 1.4849263429641724\n",
      "Epoch: 2 \tBatch: 241 \tLoss: 1.4674692153930664\n",
      "Epoch: 2 \tBatch: 242 \tLoss: 1.484200119972229\n",
      "Epoch: 2 \tBatch: 243 \tLoss: 1.5068334341049194\n",
      "Epoch: 2 \tBatch: 244 \tLoss: 1.4810247421264648\n",
      "Epoch: 2 \tBatch: 245 \tLoss: 1.4754843711853027\n",
      "Epoch: 2 \tBatch: 246 \tLoss: 1.474769115447998\n",
      "Epoch: 2 \tBatch: 247 \tLoss: 1.4787070751190186\n",
      "Epoch: 2 \tBatch: 248 \tLoss: 1.5066756010055542\n",
      "Epoch: 2 \tBatch: 249 \tLoss: 1.4806172847747803\n",
      "Epoch: 2 \tBatch: 250 \tLoss: 1.4937732219696045\n",
      "Epoch: 2 \tBatch: 251 \tLoss: 1.5196571350097656\n",
      "Epoch: 2 \tBatch: 252 \tLoss: 1.4686908721923828\n",
      "Epoch: 2 \tBatch: 253 \tLoss: 1.495392084121704\n",
      "Epoch: 2 \tBatch: 254 \tLoss: 1.4633852243423462\n",
      "Epoch: 2 \tBatch: 255 \tLoss: 1.4843635559082031\n",
      "Epoch: 2 \tBatch: 256 \tLoss: 1.48077392578125\n",
      "Epoch: 2 \tBatch: 257 \tLoss: 1.4771240949630737\n",
      "Epoch: 2 \tBatch: 258 \tLoss: 1.4797765016555786\n",
      "Epoch: 2 \tBatch: 259 \tLoss: 1.496265172958374\n",
      "Epoch: 2 \tBatch: 260 \tLoss: 1.4912497997283936\n",
      "Epoch: 2 \tBatch: 261 \tLoss: 1.4840161800384521\n",
      "Epoch: 2 \tBatch: 262 \tLoss: 1.471293568611145\n",
      "Epoch: 2 \tBatch: 263 \tLoss: 1.4922550916671753\n",
      "Epoch: 2 \tBatch: 264 \tLoss: 1.4775761365890503\n",
      "Epoch: 2 \tBatch: 265 \tLoss: 1.495082974433899\n",
      "Epoch: 2 \tBatch: 266 \tLoss: 1.4748955965042114\n",
      "Epoch: 2 \tBatch: 267 \tLoss: 1.4864981174468994\n",
      "Epoch: 2 \tBatch: 268 \tLoss: 1.4824680089950562\n",
      "Epoch: 2 \tBatch: 269 \tLoss: 1.4886410236358643\n",
      "Epoch: 2 \tBatch: 270 \tLoss: 1.494886040687561\n",
      "Epoch: 2 \tBatch: 271 \tLoss: 1.4806123971939087\n",
      "Epoch: 2 \tBatch: 272 \tLoss: 1.5062040090560913\n",
      "Epoch: 2 \tBatch: 273 \tLoss: 1.4784457683563232\n",
      "Epoch: 2 \tBatch: 274 \tLoss: 1.472020149230957\n",
      "Epoch: 2 \tBatch: 275 \tLoss: 1.4871289730072021\n",
      "Epoch: 2 \tBatch: 276 \tLoss: 1.4850393533706665\n",
      "Epoch: 2 \tBatch: 277 \tLoss: 1.4890382289886475\n",
      "Epoch: 2 \tBatch: 278 \tLoss: 1.493766188621521\n",
      "Epoch: 2 \tBatch: 279 \tLoss: 1.4816169738769531\n",
      "Epoch: 2 \tBatch: 280 \tLoss: 1.4994521141052246\n",
      "Epoch: 2 \tBatch: 281 \tLoss: 1.4770134687423706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 282 \tLoss: 1.4744807481765747\n",
      "Epoch: 2 \tBatch: 283 \tLoss: 1.4844876527786255\n",
      "Epoch: 2 \tBatch: 284 \tLoss: 1.5128531455993652\n",
      "Epoch: 2 \tBatch: 285 \tLoss: 1.4857547283172607\n",
      "Epoch: 2 \tBatch: 286 \tLoss: 1.4735124111175537\n",
      "Epoch: 2 \tBatch: 287 \tLoss: 1.4958380460739136\n",
      "Epoch: 2 \tBatch: 288 \tLoss: 1.4623615741729736\n",
      "Epoch: 2 \tBatch: 289 \tLoss: 1.4788663387298584\n",
      "Epoch: 2 \tBatch: 290 \tLoss: 1.4878300428390503\n",
      "Epoch: 2 \tBatch: 291 \tLoss: 1.4813759326934814\n",
      "Epoch: 2 \tBatch: 292 \tLoss: 1.478175401687622\n",
      "Epoch: 2 \tBatch: 293 \tLoss: 1.4789577722549438\n",
      "Epoch: 2 \tBatch: 294 \tLoss: 1.5076608657836914\n",
      "Epoch: 2 \tBatch: 295 \tLoss: 1.4853709936141968\n",
      "Epoch: 2 \tBatch: 296 \tLoss: 1.5028611421585083\n",
      "Epoch: 2 \tBatch: 297 \tLoss: 1.486277461051941\n",
      "Epoch: 2 \tBatch: 298 \tLoss: 1.4708186388015747\n",
      "Epoch: 2 \tBatch: 299 \tLoss: 1.4931976795196533\n",
      "Epoch: 2 \tBatch: 300 \tLoss: 1.4871666431427002\n",
      "Epoch: 2 \tBatch: 301 \tLoss: 1.4710073471069336\n",
      "Epoch: 2 \tBatch: 302 \tLoss: 1.501527190208435\n",
      "Epoch: 2 \tBatch: 303 \tLoss: 1.4806195497512817\n",
      "Epoch: 2 \tBatch: 304 \tLoss: 1.4699122905731201\n",
      "Epoch: 2 \tBatch: 305 \tLoss: 1.4661201238632202\n",
      "Epoch: 2 \tBatch: 306 \tLoss: 1.4997295141220093\n",
      "Epoch: 2 \tBatch: 307 \tLoss: 1.478750228881836\n",
      "Epoch: 2 \tBatch: 308 \tLoss: 1.4639265537261963\n",
      "Epoch: 2 \tBatch: 309 \tLoss: 1.47663152217865\n",
      "Epoch: 2 \tBatch: 310 \tLoss: 1.5065559148788452\n",
      "Epoch: 2 \tBatch: 311 \tLoss: 1.4901340007781982\n",
      "Epoch: 2 \tBatch: 312 \tLoss: 1.5040090084075928\n",
      "Epoch: 2 \tBatch: 313 \tLoss: 1.4844083786010742\n",
      "Epoch: 2 \tBatch: 314 \tLoss: 1.466436743736267\n",
      "Epoch: 2 \tBatch: 315 \tLoss: 1.4811969995498657\n",
      "Epoch: 2 \tBatch: 316 \tLoss: 1.4817198514938354\n",
      "Epoch: 2 \tBatch: 317 \tLoss: 1.4673510789871216\n",
      "Epoch: 2 \tBatch: 318 \tLoss: 1.4719414710998535\n",
      "Epoch: 2 \tBatch: 319 \tLoss: 1.482196569442749\n",
      "Epoch: 2 \tBatch: 320 \tLoss: 1.5014976263046265\n",
      "Epoch: 2 \tBatch: 321 \tLoss: 1.4747684001922607\n",
      "Epoch: 2 \tBatch: 322 \tLoss: 1.5218415260314941\n",
      "Epoch: 2 \tBatch: 323 \tLoss: 1.5095282793045044\n",
      "Epoch: 2 \tBatch: 324 \tLoss: 1.4821590185165405\n",
      "Epoch: 2 \tBatch: 325 \tLoss: 1.4967877864837646\n",
      "Epoch: 2 \tBatch: 326 \tLoss: 1.4717031717300415\n",
      "Epoch: 2 \tBatch: 327 \tLoss: 1.476027011871338\n",
      "Epoch: 2 \tBatch: 328 \tLoss: 1.4671424627304077\n",
      "Epoch: 2 \tBatch: 329 \tLoss: 1.4863518476486206\n",
      "Epoch: 2 \tBatch: 330 \tLoss: 1.467382550239563\n",
      "Epoch: 2 \tBatch: 331 \tLoss: 1.485318660736084\n",
      "Epoch: 2 \tBatch: 332 \tLoss: 1.499627947807312\n",
      "Epoch: 2 \tBatch: 333 \tLoss: 1.467698335647583\n",
      "Epoch: 2 \tBatch: 334 \tLoss: 1.479968786239624\n",
      "Epoch: 2 \tBatch: 335 \tLoss: 1.4675743579864502\n",
      "Epoch: 2 \tBatch: 336 \tLoss: 1.4959594011306763\n",
      "Epoch: 2 \tBatch: 337 \tLoss: 1.4879298210144043\n",
      "Epoch: 2 \tBatch: 338 \tLoss: 1.5022767782211304\n",
      "Epoch: 2 \tBatch: 339 \tLoss: 1.4761536121368408\n",
      "Epoch: 2 \tBatch: 340 \tLoss: 1.4838969707489014\n",
      "Epoch: 2 \tBatch: 341 \tLoss: 1.4905157089233398\n",
      "Epoch: 2 \tBatch: 342 \tLoss: 1.487747311592102\n",
      "Epoch: 2 \tBatch: 343 \tLoss: 1.4765138626098633\n",
      "Epoch: 2 \tBatch: 344 \tLoss: 1.4721238613128662\n",
      "Epoch: 2 \tBatch: 345 \tLoss: 1.4971345663070679\n",
      "Epoch: 2 \tBatch: 346 \tLoss: 1.4689080715179443\n",
      "Epoch: 2 \tBatch: 347 \tLoss: 1.4788020849227905\n",
      "Epoch: 2 \tBatch: 348 \tLoss: 1.4817460775375366\n",
      "Epoch: 2 \tBatch: 349 \tLoss: 1.4983234405517578\n",
      "Epoch: 2 \tBatch: 350 \tLoss: 1.4853525161743164\n",
      "Epoch: 2 \tBatch: 351 \tLoss: 1.4742294549942017\n",
      "Epoch: 2 \tBatch: 352 \tLoss: 1.4891722202301025\n",
      "Epoch: 2 \tBatch: 353 \tLoss: 1.4887404441833496\n",
      "Epoch: 2 \tBatch: 354 \tLoss: 1.4803460836410522\n",
      "Epoch: 2 \tBatch: 355 \tLoss: 1.4743860960006714\n",
      "Epoch: 2 \tBatch: 356 \tLoss: 1.504928708076477\n",
      "Epoch: 2 \tBatch: 357 \tLoss: 1.476861596107483\n",
      "Epoch: 2 \tBatch: 358 \tLoss: 1.4795945882797241\n",
      "Epoch: 2 \tBatch: 359 \tLoss: 1.4764257669448853\n",
      "Epoch: 2 \tBatch: 360 \tLoss: 1.4723758697509766\n",
      "Epoch: 2 \tBatch: 361 \tLoss: 1.4955668449401855\n",
      "Epoch: 2 \tBatch: 362 \tLoss: 1.4825341701507568\n",
      "Epoch: 2 \tBatch: 363 \tLoss: 1.4828226566314697\n",
      "Epoch: 2 \tBatch: 364 \tLoss: 1.5025341510772705\n",
      "Epoch: 2 \tBatch: 365 \tLoss: 1.4924544095993042\n",
      "Epoch: 2 \tBatch: 366 \tLoss: 1.4753150939941406\n",
      "Epoch: 2 \tBatch: 367 \tLoss: 1.4731018543243408\n",
      "Epoch: 2 \tBatch: 368 \tLoss: 1.4754695892333984\n",
      "Epoch: 2 \tBatch: 369 \tLoss: 1.4894708395004272\n",
      "Epoch: 2 \tBatch: 370 \tLoss: 1.4889109134674072\n",
      "Epoch: 2 \tBatch: 371 \tLoss: 1.4711662530899048\n",
      "Epoch: 2 \tBatch: 372 \tLoss: 1.4925593137741089\n",
      "Epoch: 2 \tBatch: 373 \tLoss: 1.4871740341186523\n",
      "Epoch: 2 \tBatch: 374 \tLoss: 1.4927903413772583\n",
      "Epoch: 2 \tBatch: 375 \tLoss: 1.474329948425293\n",
      "Epoch: 2 \tBatch: 376 \tLoss: 1.4847931861877441\n",
      "Epoch: 2 \tBatch: 377 \tLoss: 1.4874848127365112\n",
      "Epoch: 2 \tBatch: 378 \tLoss: 1.4918930530548096\n",
      "Epoch: 2 \tBatch: 379 \tLoss: 1.4900755882263184\n",
      "Epoch: 2 \tBatch: 380 \tLoss: 1.4860072135925293\n",
      "Epoch: 2 \tBatch: 381 \tLoss: 1.46676766872406\n",
      "Epoch: 2 \tBatch: 382 \tLoss: 1.4886196851730347\n",
      "Epoch: 2 \tBatch: 383 \tLoss: 1.4859824180603027\n",
      "Epoch: 2 \tBatch: 384 \tLoss: 1.4919373989105225\n",
      "Epoch: 2 \tBatch: 385 \tLoss: 1.4800763130187988\n",
      "Epoch: 2 \tBatch: 386 \tLoss: 1.4902682304382324\n",
      "Epoch: 2 \tBatch: 387 \tLoss: 1.4652739763259888\n",
      "Epoch: 2 \tBatch: 388 \tLoss: 1.486943006515503\n",
      "Epoch: 2 \tBatch: 389 \tLoss: 1.4772852659225464\n",
      "Epoch: 2 \tBatch: 390 \tLoss: 1.486575722694397\n",
      "Epoch: 3 \tBatch: 0 \tLoss: 1.4759260416030884\n",
      "Epoch: 3 \tBatch: 1 \tLoss: 1.477419376373291\n",
      "Epoch: 3 \tBatch: 2 \tLoss: 1.4753309488296509\n",
      "Epoch: 3 \tBatch: 3 \tLoss: 1.4686814546585083\n",
      "Epoch: 3 \tBatch: 4 \tLoss: 1.4708539247512817\n",
      "Epoch: 3 \tBatch: 5 \tLoss: 1.4848486185073853\n",
      "Epoch: 3 \tBatch: 6 \tLoss: 1.5005183219909668\n",
      "Epoch: 3 \tBatch: 7 \tLoss: 1.479876160621643\n",
      "Epoch: 3 \tBatch: 8 \tLoss: 1.4761240482330322\n",
      "Epoch: 3 \tBatch: 9 \tLoss: 1.4680814743041992\n",
      "Epoch: 3 \tBatch: 10 \tLoss: 1.4841309785842896\n",
      "Epoch: 3 \tBatch: 11 \tLoss: 1.4804905652999878\n",
      "Epoch: 3 \tBatch: 12 \tLoss: 1.4823678731918335\n",
      "Epoch: 3 \tBatch: 13 \tLoss: 1.483231544494629\n",
      "Epoch: 3 \tBatch: 14 \tLoss: 1.4986639022827148\n",
      "Epoch: 3 \tBatch: 15 \tLoss: 1.4922103881835938\n",
      "Epoch: 3 \tBatch: 16 \tLoss: 1.4879419803619385\n",
      "Epoch: 3 \tBatch: 17 \tLoss: 1.4699773788452148\n",
      "Epoch: 3 \tBatch: 18 \tLoss: 1.4848735332489014\n",
      "Epoch: 3 \tBatch: 19 \tLoss: 1.4960508346557617\n",
      "Epoch: 3 \tBatch: 20 \tLoss: 1.465647578239441\n",
      "Epoch: 3 \tBatch: 21 \tLoss: 1.4772437810897827\n",
      "Epoch: 3 \tBatch: 22 \tLoss: 1.4697251319885254\n",
      "Epoch: 3 \tBatch: 23 \tLoss: 1.488664984703064\n",
      "Epoch: 3 \tBatch: 24 \tLoss: 1.4733874797821045\n",
      "Epoch: 3 \tBatch: 25 \tLoss: 1.4757285118103027\n",
      "Epoch: 3 \tBatch: 26 \tLoss: 1.4696156978607178\n",
      "Epoch: 3 \tBatch: 27 \tLoss: 1.480614185333252\n",
      "Epoch: 3 \tBatch: 28 \tLoss: 1.4917210340499878\n",
      "Epoch: 3 \tBatch: 29 \tLoss: 1.4959797859191895\n",
      "Epoch: 3 \tBatch: 30 \tLoss: 1.4965386390686035\n",
      "Epoch: 3 \tBatch: 31 \tLoss: 1.4639421701431274\n",
      "Epoch: 3 \tBatch: 32 \tLoss: 1.4652692079544067\n",
      "Epoch: 3 \tBatch: 33 \tLoss: 1.4748135805130005\n",
      "Epoch: 3 \tBatch: 34 \tLoss: 1.487923502922058\n",
      "Epoch: 3 \tBatch: 35 \tLoss: 1.4735002517700195\n",
      "Epoch: 3 \tBatch: 36 \tLoss: 1.4831204414367676\n",
      "Epoch: 3 \tBatch: 37 \tLoss: 1.4765182733535767\n",
      "Epoch: 3 \tBatch: 38 \tLoss: 1.4646241664886475\n",
      "Epoch: 3 \tBatch: 39 \tLoss: 1.4703201055526733\n",
      "Epoch: 3 \tBatch: 40 \tLoss: 1.485198736190796\n",
      "Epoch: 3 \tBatch: 41 \tLoss: 1.4808447360992432\n",
      "Epoch: 3 \tBatch: 42 \tLoss: 1.4796781539916992\n",
      "Epoch: 3 \tBatch: 43 \tLoss: 1.475732445716858\n",
      "Epoch: 3 \tBatch: 44 \tLoss: 1.4762864112854004\n",
      "Epoch: 3 \tBatch: 45 \tLoss: 1.4889944791793823\n",
      "Epoch: 3 \tBatch: 46 \tLoss: 1.486070990562439\n",
      "Epoch: 3 \tBatch: 47 \tLoss: 1.4670860767364502\n",
      "Epoch: 3 \tBatch: 48 \tLoss: 1.4852399826049805\n",
      "Epoch: 3 \tBatch: 49 \tLoss: 1.4889556169509888\n",
      "Epoch: 3 \tBatch: 50 \tLoss: 1.5100347995758057\n",
      "Epoch: 3 \tBatch: 51 \tLoss: 1.4815473556518555\n",
      "Epoch: 3 \tBatch: 52 \tLoss: 1.478875756263733\n",
      "Epoch: 3 \tBatch: 53 \tLoss: 1.469386339187622\n",
      "Epoch: 3 \tBatch: 54 \tLoss: 1.4694136381149292\n",
      "Epoch: 3 \tBatch: 55 \tLoss: 1.471061110496521\n",
      "Epoch: 3 \tBatch: 56 \tLoss: 1.467720627784729\n",
      "Epoch: 3 \tBatch: 57 \tLoss: 1.4736294746398926\n",
      "Epoch: 3 \tBatch: 58 \tLoss: 1.4826992750167847\n",
      "Epoch: 3 \tBatch: 59 \tLoss: 1.4732681512832642\n",
      "Epoch: 3 \tBatch: 60 \tLoss: 1.4980125427246094\n",
      "Epoch: 3 \tBatch: 61 \tLoss: 1.474382996559143\n",
      "Epoch: 3 \tBatch: 62 \tLoss: 1.468332290649414\n",
      "Epoch: 3 \tBatch: 63 \tLoss: 1.484857439994812\n",
      "Epoch: 3 \tBatch: 64 \tLoss: 1.4617977142333984\n",
      "Epoch: 3 \tBatch: 65 \tLoss: 1.4784562587738037\n",
      "Epoch: 3 \tBatch: 66 \tLoss: 1.4919780492782593\n",
      "Epoch: 3 \tBatch: 67 \tLoss: 1.4800307750701904\n",
      "Epoch: 3 \tBatch: 68 \tLoss: 1.4791228771209717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 69 \tLoss: 1.477824330329895\n",
      "Epoch: 3 \tBatch: 70 \tLoss: 1.478187918663025\n",
      "Epoch: 3 \tBatch: 71 \tLoss: 1.4727979898452759\n",
      "Epoch: 3 \tBatch: 72 \tLoss: 1.485686182975769\n",
      "Epoch: 3 \tBatch: 73 \tLoss: 1.4933199882507324\n",
      "Epoch: 3 \tBatch: 74 \tLoss: 1.4779150485992432\n",
      "Epoch: 3 \tBatch: 75 \tLoss: 1.470446228981018\n",
      "Epoch: 3 \tBatch: 76 \tLoss: 1.4866588115692139\n",
      "Epoch: 3 \tBatch: 77 \tLoss: 1.477696418762207\n",
      "Epoch: 3 \tBatch: 78 \tLoss: 1.4884039163589478\n",
      "Epoch: 3 \tBatch: 79 \tLoss: 1.479145884513855\n",
      "Epoch: 3 \tBatch: 80 \tLoss: 1.4676254987716675\n",
      "Epoch: 3 \tBatch: 81 \tLoss: 1.5092432498931885\n",
      "Epoch: 3 \tBatch: 82 \tLoss: 1.5031131505966187\n",
      "Epoch: 3 \tBatch: 83 \tLoss: 1.498722791671753\n",
      "Epoch: 3 \tBatch: 84 \tLoss: 1.5119317770004272\n",
      "Epoch: 3 \tBatch: 85 \tLoss: 1.4697399139404297\n",
      "Epoch: 3 \tBatch: 86 \tLoss: 1.4870290756225586\n",
      "Epoch: 3 \tBatch: 87 \tLoss: 1.4810526371002197\n",
      "Epoch: 3 \tBatch: 88 \tLoss: 1.4791297912597656\n",
      "Epoch: 3 \tBatch: 89 \tLoss: 1.504901647567749\n",
      "Epoch: 3 \tBatch: 90 \tLoss: 1.468544363975525\n",
      "Epoch: 3 \tBatch: 91 \tLoss: 1.514980673789978\n",
      "Epoch: 3 \tBatch: 92 \tLoss: 1.4821971654891968\n",
      "Epoch: 3 \tBatch: 93 \tLoss: 1.4620187282562256\n",
      "Epoch: 3 \tBatch: 94 \tLoss: 1.474532961845398\n",
      "Epoch: 3 \tBatch: 95 \tLoss: 1.4767978191375732\n",
      "Epoch: 3 \tBatch: 96 \tLoss: 1.4860857725143433\n",
      "Epoch: 3 \tBatch: 97 \tLoss: 1.471114993095398\n",
      "Epoch: 3 \tBatch: 98 \tLoss: 1.4785268306732178\n",
      "Epoch: 3 \tBatch: 99 \tLoss: 1.4703495502471924\n",
      "Epoch: 3 \tBatch: 100 \tLoss: 1.4770522117614746\n",
      "Epoch: 3 \tBatch: 101 \tLoss: 1.469236135482788\n",
      "Epoch: 3 \tBatch: 102 \tLoss: 1.4644465446472168\n",
      "Epoch: 3 \tBatch: 103 \tLoss: 1.477349877357483\n",
      "Epoch: 3 \tBatch: 104 \tLoss: 1.4905091524124146\n",
      "Epoch: 3 \tBatch: 105 \tLoss: 1.4890707731246948\n",
      "Epoch: 3 \tBatch: 106 \tLoss: 1.4721788167953491\n",
      "Epoch: 3 \tBatch: 107 \tLoss: 1.4903918504714966\n",
      "Epoch: 3 \tBatch: 108 \tLoss: 1.4934715032577515\n",
      "Epoch: 3 \tBatch: 109 \tLoss: 1.489351749420166\n",
      "Epoch: 3 \tBatch: 110 \tLoss: 1.481850266456604\n",
      "Epoch: 3 \tBatch: 111 \tLoss: 1.4764416217803955\n",
      "Epoch: 3 \tBatch: 112 \tLoss: 1.4812012910842896\n",
      "Epoch: 3 \tBatch: 113 \tLoss: 1.4847201108932495\n",
      "Epoch: 3 \tBatch: 114 \tLoss: 1.4794626235961914\n",
      "Epoch: 3 \tBatch: 115 \tLoss: 1.5078624486923218\n",
      "Epoch: 3 \tBatch: 116 \tLoss: 1.472427248954773\n",
      "Epoch: 3 \tBatch: 117 \tLoss: 1.5042247772216797\n",
      "Epoch: 3 \tBatch: 118 \tLoss: 1.4711397886276245\n",
      "Epoch: 3 \tBatch: 119 \tLoss: 1.4861414432525635\n",
      "Epoch: 3 \tBatch: 120 \tLoss: 1.4667326211929321\n",
      "Epoch: 3 \tBatch: 121 \tLoss: 1.489620566368103\n",
      "Epoch: 3 \tBatch: 122 \tLoss: 1.475904107093811\n",
      "Epoch: 3 \tBatch: 123 \tLoss: 1.4739372730255127\n",
      "Epoch: 3 \tBatch: 124 \tLoss: 1.4809226989746094\n",
      "Epoch: 3 \tBatch: 125 \tLoss: 1.4849610328674316\n",
      "Epoch: 3 \tBatch: 126 \tLoss: 1.4693281650543213\n",
      "Epoch: 3 \tBatch: 127 \tLoss: 1.4728374481201172\n",
      "Epoch: 3 \tBatch: 128 \tLoss: 1.4788252115249634\n",
      "Epoch: 3 \tBatch: 129 \tLoss: 1.475001573562622\n",
      "Epoch: 3 \tBatch: 130 \tLoss: 1.4637081623077393\n",
      "Epoch: 3 \tBatch: 131 \tLoss: 1.470807671546936\n",
      "Epoch: 3 \tBatch: 132 \tLoss: 1.489161729812622\n",
      "Epoch: 3 \tBatch: 133 \tLoss: 1.4836431741714478\n",
      "Epoch: 3 \tBatch: 134 \tLoss: 1.4840309619903564\n",
      "Epoch: 3 \tBatch: 135 \tLoss: 1.4804261922836304\n",
      "Epoch: 3 \tBatch: 136 \tLoss: 1.4730299711227417\n",
      "Epoch: 3 \tBatch: 137 \tLoss: 1.4828566312789917\n",
      "Epoch: 3 \tBatch: 138 \tLoss: 1.488072156906128\n",
      "Epoch: 3 \tBatch: 139 \tLoss: 1.4805634021759033\n",
      "Epoch: 3 \tBatch: 140 \tLoss: 1.4843800067901611\n",
      "Epoch: 3 \tBatch: 141 \tLoss: 1.4727349281311035\n",
      "Epoch: 3 \tBatch: 142 \tLoss: 1.4758528470993042\n",
      "Epoch: 3 \tBatch: 143 \tLoss: 1.4779998064041138\n",
      "Epoch: 3 \tBatch: 144 \tLoss: 1.501366138458252\n",
      "Epoch: 3 \tBatch: 145 \tLoss: 1.4800795316696167\n",
      "Epoch: 3 \tBatch: 146 \tLoss: 1.4794113636016846\n",
      "Epoch: 3 \tBatch: 147 \tLoss: 1.4893312454223633\n",
      "Epoch: 3 \tBatch: 148 \tLoss: 1.4792693853378296\n",
      "Epoch: 3 \tBatch: 149 \tLoss: 1.472549319267273\n",
      "Epoch: 3 \tBatch: 150 \tLoss: 1.4945796728134155\n",
      "Epoch: 3 \tBatch: 151 \tLoss: 1.4728082418441772\n",
      "Epoch: 3 \tBatch: 152 \tLoss: 1.4776036739349365\n",
      "Epoch: 3 \tBatch: 153 \tLoss: 1.4961998462677002\n",
      "Epoch: 3 \tBatch: 154 \tLoss: 1.4773670434951782\n",
      "Epoch: 3 \tBatch: 155 \tLoss: 1.4766252040863037\n",
      "Epoch: 3 \tBatch: 156 \tLoss: 1.4864658117294312\n",
      "Epoch: 3 \tBatch: 157 \tLoss: 1.4676518440246582\n",
      "Epoch: 3 \tBatch: 158 \tLoss: 1.4767820835113525\n",
      "Epoch: 3 \tBatch: 159 \tLoss: 1.4883769750595093\n",
      "Epoch: 3 \tBatch: 160 \tLoss: 1.4746378660202026\n",
      "Epoch: 3 \tBatch: 161 \tLoss: 1.466807246208191\n",
      "Epoch: 3 \tBatch: 162 \tLoss: 1.4786462783813477\n",
      "Epoch: 3 \tBatch: 163 \tLoss: 1.4832674264907837\n",
      "Epoch: 3 \tBatch: 164 \tLoss: 1.4778023958206177\n",
      "Epoch: 3 \tBatch: 165 \tLoss: 1.485275387763977\n",
      "Epoch: 3 \tBatch: 166 \tLoss: 1.488174319267273\n",
      "Epoch: 3 \tBatch: 167 \tLoss: 1.4875671863555908\n",
      "Epoch: 3 \tBatch: 168 \tLoss: 1.4811533689498901\n",
      "Epoch: 3 \tBatch: 169 \tLoss: 1.477259635925293\n",
      "Epoch: 3 \tBatch: 170 \tLoss: 1.466044306755066\n",
      "Epoch: 3 \tBatch: 171 \tLoss: 1.4708176851272583\n",
      "Epoch: 3 \tBatch: 172 \tLoss: 1.4701391458511353\n",
      "Epoch: 3 \tBatch: 173 \tLoss: 1.47052800655365\n",
      "Epoch: 3 \tBatch: 174 \tLoss: 1.4710274934768677\n",
      "Epoch: 3 \tBatch: 175 \tLoss: 1.4699558019638062\n",
      "Epoch: 3 \tBatch: 176 \tLoss: 1.471718668937683\n",
      "Epoch: 3 \tBatch: 177 \tLoss: 1.485708236694336\n",
      "Epoch: 3 \tBatch: 178 \tLoss: 1.4784587621688843\n",
      "Epoch: 3 \tBatch: 179 \tLoss: 1.4935728311538696\n",
      "Epoch: 3 \tBatch: 180 \tLoss: 1.4907137155532837\n",
      "Epoch: 3 \tBatch: 181 \tLoss: 1.4809620380401611\n",
      "Epoch: 3 \tBatch: 182 \tLoss: 1.4874472618103027\n",
      "Epoch: 3 \tBatch: 183 \tLoss: 1.4990755319595337\n",
      "Epoch: 3 \tBatch: 184 \tLoss: 1.490706443786621\n",
      "Epoch: 3 \tBatch: 185 \tLoss: 1.4723833799362183\n",
      "Epoch: 3 \tBatch: 186 \tLoss: 1.494484782218933\n",
      "Epoch: 3 \tBatch: 187 \tLoss: 1.4654093980789185\n",
      "Epoch: 3 \tBatch: 188 \tLoss: 1.4782731533050537\n",
      "Epoch: 3 \tBatch: 189 \tLoss: 1.4947052001953125\n",
      "Epoch: 3 \tBatch: 190 \tLoss: 1.4837549924850464\n",
      "Epoch: 3 \tBatch: 191 \tLoss: 1.5169355869293213\n",
      "Epoch: 3 \tBatch: 192 \tLoss: 1.4872623682022095\n",
      "Epoch: 3 \tBatch: 193 \tLoss: 1.4636698961257935\n",
      "Epoch: 3 \tBatch: 194 \tLoss: 1.5025285482406616\n",
      "Epoch: 3 \tBatch: 195 \tLoss: 1.4694981575012207\n",
      "Epoch: 3 \tBatch: 196 \tLoss: 1.4882012605667114\n",
      "Epoch: 3 \tBatch: 197 \tLoss: 1.478683352470398\n",
      "Epoch: 3 \tBatch: 198 \tLoss: 1.4720591306686401\n",
      "Epoch: 3 \tBatch: 199 \tLoss: 1.4962645769119263\n",
      "Epoch: 3 \tBatch: 200 \tLoss: 1.4623793363571167\n",
      "Epoch: 3 \tBatch: 201 \tLoss: 1.4914108514785767\n",
      "Epoch: 3 \tBatch: 202 \tLoss: 1.4697797298431396\n",
      "Epoch: 3 \tBatch: 203 \tLoss: 1.4885673522949219\n",
      "Epoch: 3 \tBatch: 204 \tLoss: 1.4831775426864624\n",
      "Epoch: 3 \tBatch: 205 \tLoss: 1.4763336181640625\n",
      "Epoch: 3 \tBatch: 206 \tLoss: 1.4700021743774414\n",
      "Epoch: 3 \tBatch: 207 \tLoss: 1.461511492729187\n",
      "Epoch: 3 \tBatch: 208 \tLoss: 1.4930977821350098\n",
      "Epoch: 3 \tBatch: 209 \tLoss: 1.4874635934829712\n",
      "Epoch: 3 \tBatch: 210 \tLoss: 1.4876898527145386\n",
      "Epoch: 3 \tBatch: 211 \tLoss: 1.4783499240875244\n",
      "Epoch: 3 \tBatch: 212 \tLoss: 1.4863592386245728\n",
      "Epoch: 3 \tBatch: 213 \tLoss: 1.4850913286209106\n",
      "Epoch: 3 \tBatch: 214 \tLoss: 1.494758129119873\n",
      "Epoch: 3 \tBatch: 215 \tLoss: 1.4803215265274048\n",
      "Epoch: 3 \tBatch: 216 \tLoss: 1.46369206905365\n",
      "Epoch: 3 \tBatch: 217 \tLoss: 1.4849755764007568\n",
      "Epoch: 3 \tBatch: 218 \tLoss: 1.4744075536727905\n",
      "Epoch: 3 \tBatch: 219 \tLoss: 1.4727306365966797\n",
      "Epoch: 3 \tBatch: 220 \tLoss: 1.4782185554504395\n",
      "Epoch: 3 \tBatch: 221 \tLoss: 1.4805186986923218\n",
      "Epoch: 3 \tBatch: 222 \tLoss: 1.4866740703582764\n",
      "Epoch: 3 \tBatch: 223 \tLoss: 1.482666254043579\n",
      "Epoch: 3 \tBatch: 224 \tLoss: 1.4882761240005493\n",
      "Epoch: 3 \tBatch: 225 \tLoss: 1.4744030237197876\n",
      "Epoch: 3 \tBatch: 226 \tLoss: 1.473842740058899\n",
      "Epoch: 3 \tBatch: 227 \tLoss: 1.490012288093567\n",
      "Epoch: 3 \tBatch: 228 \tLoss: 1.487292766571045\n",
      "Epoch: 3 \tBatch: 229 \tLoss: 1.4805454015731812\n",
      "Epoch: 3 \tBatch: 230 \tLoss: 1.4824092388153076\n",
      "Epoch: 3 \tBatch: 231 \tLoss: 1.4828072786331177\n",
      "Epoch: 3 \tBatch: 232 \tLoss: 1.4762279987335205\n",
      "Epoch: 3 \tBatch: 233 \tLoss: 1.477325677871704\n",
      "Epoch: 3 \tBatch: 234 \tLoss: 1.4788182973861694\n",
      "Epoch: 3 \tBatch: 235 \tLoss: 1.4615603685379028\n",
      "Epoch: 3 \tBatch: 236 \tLoss: 1.4795739650726318\n",
      "Epoch: 3 \tBatch: 237 \tLoss: 1.4709744453430176\n",
      "Epoch: 3 \tBatch: 238 \tLoss: 1.4758085012435913\n",
      "Epoch: 3 \tBatch: 239 \tLoss: 1.4865864515304565\n",
      "Epoch: 3 \tBatch: 240 \tLoss: 1.4799227714538574\n",
      "Epoch: 3 \tBatch: 241 \tLoss: 1.4828903675079346\n",
      "Epoch: 3 \tBatch: 242 \tLoss: 1.4777369499206543\n",
      "Epoch: 3 \tBatch: 243 \tLoss: 1.4696521759033203\n",
      "Epoch: 3 \tBatch: 244 \tLoss: 1.469204068183899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 245 \tLoss: 1.48347806930542\n",
      "Epoch: 3 \tBatch: 246 \tLoss: 1.4796043634414673\n",
      "Epoch: 3 \tBatch: 247 \tLoss: 1.483127236366272\n",
      "Epoch: 3 \tBatch: 248 \tLoss: 1.4838881492614746\n",
      "Epoch: 3 \tBatch: 249 \tLoss: 1.4813151359558105\n",
      "Epoch: 3 \tBatch: 250 \tLoss: 1.4690868854522705\n",
      "Epoch: 3 \tBatch: 251 \tLoss: 1.4768258333206177\n",
      "Epoch: 3 \tBatch: 252 \tLoss: 1.497137427330017\n",
      "Epoch: 3 \tBatch: 253 \tLoss: 1.4840418100357056\n",
      "Epoch: 3 \tBatch: 254 \tLoss: 1.4796733856201172\n",
      "Epoch: 3 \tBatch: 255 \tLoss: 1.4800387620925903\n",
      "Epoch: 3 \tBatch: 256 \tLoss: 1.4723764657974243\n",
      "Epoch: 3 \tBatch: 257 \tLoss: 1.4755163192749023\n",
      "Epoch: 3 \tBatch: 258 \tLoss: 1.4764487743377686\n",
      "Epoch: 3 \tBatch: 259 \tLoss: 1.480393886566162\n",
      "Epoch: 3 \tBatch: 260 \tLoss: 1.4790101051330566\n",
      "Epoch: 3 \tBatch: 261 \tLoss: 1.4786453247070312\n",
      "Epoch: 3 \tBatch: 262 \tLoss: 1.4828565120697021\n",
      "Epoch: 3 \tBatch: 263 \tLoss: 1.4700095653533936\n",
      "Epoch: 3 \tBatch: 264 \tLoss: 1.4910869598388672\n",
      "Epoch: 3 \tBatch: 265 \tLoss: 1.462849497795105\n",
      "Epoch: 3 \tBatch: 266 \tLoss: 1.4875657558441162\n",
      "Epoch: 3 \tBatch: 267 \tLoss: 1.4681878089904785\n",
      "Epoch: 3 \tBatch: 268 \tLoss: 1.470088243484497\n",
      "Epoch: 3 \tBatch: 269 \tLoss: 1.4766631126403809\n",
      "Epoch: 3 \tBatch: 270 \tLoss: 1.47047758102417\n",
      "Epoch: 3 \tBatch: 271 \tLoss: 1.4731173515319824\n",
      "Epoch: 3 \tBatch: 272 \tLoss: 1.4981023073196411\n",
      "Epoch: 3 \tBatch: 273 \tLoss: 1.4782681465148926\n",
      "Epoch: 3 \tBatch: 274 \tLoss: 1.4786790609359741\n",
      "Epoch: 3 \tBatch: 275 \tLoss: 1.4845783710479736\n",
      "Epoch: 3 \tBatch: 276 \tLoss: 1.487540364265442\n",
      "Epoch: 3 \tBatch: 277 \tLoss: 1.4820547103881836\n",
      "Epoch: 3 \tBatch: 278 \tLoss: 1.48665452003479\n",
      "Epoch: 3 \tBatch: 279 \tLoss: 1.4901939630508423\n",
      "Epoch: 3 \tBatch: 280 \tLoss: 1.4878613948822021\n",
      "Epoch: 3 \tBatch: 281 \tLoss: 1.4952729940414429\n",
      "Epoch: 3 \tBatch: 282 \tLoss: 1.4789973497390747\n",
      "Epoch: 3 \tBatch: 283 \tLoss: 1.48801851272583\n",
      "Epoch: 3 \tBatch: 284 \tLoss: 1.4866377115249634\n",
      "Epoch: 3 \tBatch: 285 \tLoss: 1.4724971055984497\n",
      "Epoch: 3 \tBatch: 286 \tLoss: 1.4746347665786743\n",
      "Epoch: 3 \tBatch: 287 \tLoss: 1.4652893543243408\n",
      "Epoch: 3 \tBatch: 288 \tLoss: 1.480890154838562\n",
      "Epoch: 3 \tBatch: 289 \tLoss: 1.4698848724365234\n",
      "Epoch: 3 \tBatch: 290 \tLoss: 1.4967026710510254\n",
      "Epoch: 3 \tBatch: 291 \tLoss: 1.4873934984207153\n",
      "Epoch: 3 \tBatch: 292 \tLoss: 1.48348069190979\n",
      "Epoch: 3 \tBatch: 293 \tLoss: 1.4691420793533325\n",
      "Epoch: 3 \tBatch: 294 \tLoss: 1.4876766204833984\n",
      "Epoch: 3 \tBatch: 295 \tLoss: 1.4959138631820679\n",
      "Epoch: 3 \tBatch: 296 \tLoss: 1.4903678894042969\n",
      "Epoch: 3 \tBatch: 297 \tLoss: 1.4949402809143066\n",
      "Epoch: 3 \tBatch: 298 \tLoss: 1.500617265701294\n",
      "Epoch: 3 \tBatch: 299 \tLoss: 1.487291693687439\n",
      "Epoch: 3 \tBatch: 300 \tLoss: 1.473610758781433\n",
      "Epoch: 3 \tBatch: 301 \tLoss: 1.4743129014968872\n",
      "Epoch: 3 \tBatch: 302 \tLoss: 1.4693959951400757\n",
      "Epoch: 3 \tBatch: 303 \tLoss: 1.481321096420288\n",
      "Epoch: 3 \tBatch: 304 \tLoss: 1.4874117374420166\n",
      "Epoch: 3 \tBatch: 305 \tLoss: 1.4813587665557861\n",
      "Epoch: 3 \tBatch: 306 \tLoss: 1.4773155450820923\n",
      "Epoch: 3 \tBatch: 307 \tLoss: 1.4840105772018433\n",
      "Epoch: 3 \tBatch: 308 \tLoss: 1.475933313369751\n",
      "Epoch: 3 \tBatch: 309 \tLoss: 1.4937119483947754\n",
      "Epoch: 3 \tBatch: 310 \tLoss: 1.4719102382659912\n",
      "Epoch: 3 \tBatch: 311 \tLoss: 1.485878348350525\n",
      "Epoch: 3 \tBatch: 312 \tLoss: 1.4693375825881958\n",
      "Epoch: 3 \tBatch: 313 \tLoss: 1.4690920114517212\n",
      "Epoch: 3 \tBatch: 314 \tLoss: 1.482093334197998\n",
      "Epoch: 3 \tBatch: 315 \tLoss: 1.4615557193756104\n",
      "Epoch: 3 \tBatch: 316 \tLoss: 1.4907306432724\n",
      "Epoch: 3 \tBatch: 317 \tLoss: 1.471413493156433\n",
      "Epoch: 3 \tBatch: 318 \tLoss: 1.482763648033142\n",
      "Epoch: 3 \tBatch: 319 \tLoss: 1.4711958169937134\n",
      "Epoch: 3 \tBatch: 320 \tLoss: 1.473473072052002\n",
      "Epoch: 3 \tBatch: 321 \tLoss: 1.466477632522583\n",
      "Epoch: 3 \tBatch: 322 \tLoss: 1.4747166633605957\n",
      "Epoch: 3 \tBatch: 323 \tLoss: 1.4690853357315063\n",
      "Epoch: 3 \tBatch: 324 \tLoss: 1.4804474115371704\n",
      "Epoch: 3 \tBatch: 325 \tLoss: 1.476886510848999\n",
      "Epoch: 3 \tBatch: 326 \tLoss: 1.4667454957962036\n",
      "Epoch: 3 \tBatch: 327 \tLoss: 1.4951010942459106\n",
      "Epoch: 3 \tBatch: 328 \tLoss: 1.482120394706726\n",
      "Epoch: 3 \tBatch: 329 \tLoss: 1.50124990940094\n",
      "Epoch: 3 \tBatch: 330 \tLoss: 1.4751641750335693\n",
      "Epoch: 3 \tBatch: 331 \tLoss: 1.480886697769165\n",
      "Epoch: 3 \tBatch: 332 \tLoss: 1.4849239587783813\n",
      "Epoch: 3 \tBatch: 333 \tLoss: 1.4644520282745361\n",
      "Epoch: 3 \tBatch: 334 \tLoss: 1.4824779033660889\n",
      "Epoch: 3 \tBatch: 335 \tLoss: 1.4728446006774902\n",
      "Epoch: 3 \tBatch: 336 \tLoss: 1.4918508529663086\n",
      "Epoch: 3 \tBatch: 337 \tLoss: 1.4968663454055786\n",
      "Epoch: 3 \tBatch: 338 \tLoss: 1.4791431427001953\n",
      "Epoch: 3 \tBatch: 339 \tLoss: 1.5074539184570312\n",
      "Epoch: 3 \tBatch: 340 \tLoss: 1.4796326160430908\n",
      "Epoch: 3 \tBatch: 341 \tLoss: 1.4909844398498535\n",
      "Epoch: 3 \tBatch: 342 \tLoss: 1.478257179260254\n",
      "Epoch: 3 \tBatch: 343 \tLoss: 1.4653042554855347\n",
      "Epoch: 3 \tBatch: 344 \tLoss: 1.4862779378890991\n",
      "Epoch: 3 \tBatch: 345 \tLoss: 1.4617685079574585\n",
      "Epoch: 3 \tBatch: 346 \tLoss: 1.5061535835266113\n",
      "Epoch: 3 \tBatch: 347 \tLoss: 1.5141942501068115\n",
      "Epoch: 3 \tBatch: 348 \tLoss: 1.4951527118682861\n",
      "Epoch: 3 \tBatch: 349 \tLoss: 1.4642292261123657\n",
      "Epoch: 3 \tBatch: 350 \tLoss: 1.4818551540374756\n",
      "Epoch: 3 \tBatch: 351 \tLoss: 1.4648991823196411\n",
      "Epoch: 3 \tBatch: 352 \tLoss: 1.4883071184158325\n",
      "Epoch: 3 \tBatch: 353 \tLoss: 1.4714690446853638\n",
      "Epoch: 3 \tBatch: 354 \tLoss: 1.4704350233078003\n",
      "Epoch: 3 \tBatch: 355 \tLoss: 1.4786045551300049\n",
      "Epoch: 3 \tBatch: 356 \tLoss: 1.479748010635376\n",
      "Epoch: 3 \tBatch: 357 \tLoss: 1.4772433042526245\n",
      "Epoch: 3 \tBatch: 358 \tLoss: 1.4943238496780396\n",
      "Epoch: 3 \tBatch: 359 \tLoss: 1.4710304737091064\n",
      "Epoch: 3 \tBatch: 360 \tLoss: 1.480246901512146\n",
      "Epoch: 3 \tBatch: 361 \tLoss: 1.483060598373413\n",
      "Epoch: 3 \tBatch: 362 \tLoss: 1.5074812173843384\n",
      "Epoch: 3 \tBatch: 363 \tLoss: 1.474799633026123\n",
      "Epoch: 3 \tBatch: 364 \tLoss: 1.4761170148849487\n",
      "Epoch: 3 \tBatch: 365 \tLoss: 1.4919828176498413\n",
      "Epoch: 3 \tBatch: 366 \tLoss: 1.4728375673294067\n",
      "Epoch: 3 \tBatch: 367 \tLoss: 1.4854257106781006\n",
      "Epoch: 3 \tBatch: 368 \tLoss: 1.4794607162475586\n",
      "Epoch: 3 \tBatch: 369 \tLoss: 1.4836323261260986\n",
      "Epoch: 3 \tBatch: 370 \tLoss: 1.471103549003601\n",
      "Epoch: 3 \tBatch: 371 \tLoss: 1.481688141822815\n",
      "Epoch: 3 \tBatch: 372 \tLoss: 1.489417314529419\n",
      "Epoch: 3 \tBatch: 373 \tLoss: 1.4758113622665405\n",
      "Epoch: 3 \tBatch: 374 \tLoss: 1.5014444589614868\n",
      "Epoch: 3 \tBatch: 375 \tLoss: 1.4680306911468506\n",
      "Epoch: 3 \tBatch: 376 \tLoss: 1.4865365028381348\n",
      "Epoch: 3 \tBatch: 377 \tLoss: 1.4769740104675293\n",
      "Epoch: 3 \tBatch: 378 \tLoss: 1.485097050666809\n",
      "Epoch: 3 \tBatch: 379 \tLoss: 1.495646357536316\n",
      "Epoch: 3 \tBatch: 380 \tLoss: 1.4850716590881348\n",
      "Epoch: 3 \tBatch: 381 \tLoss: 1.4850738048553467\n",
      "Epoch: 3 \tBatch: 382 \tLoss: 1.4824007749557495\n",
      "Epoch: 3 \tBatch: 383 \tLoss: 1.4934704303741455\n",
      "Epoch: 3 \tBatch: 384 \tLoss: 1.4778470993041992\n",
      "Epoch: 3 \tBatch: 385 \tLoss: 1.477284550666809\n",
      "Epoch: 3 \tBatch: 386 \tLoss: 1.4974415302276611\n",
      "Epoch: 3 \tBatch: 387 \tLoss: 1.4734300374984741\n",
      "Epoch: 3 \tBatch: 388 \tLoss: 1.4809683561325073\n",
      "Epoch: 3 \tBatch: 389 \tLoss: 1.4644312858581543\n",
      "Epoch: 3 \tBatch: 390 \tLoss: 1.4878922700881958\n",
      "Epoch: 4 \tBatch: 0 \tLoss: 1.502578854560852\n",
      "Epoch: 4 \tBatch: 1 \tLoss: 1.4654676914215088\n",
      "Epoch: 4 \tBatch: 2 \tLoss: 1.4777913093566895\n",
      "Epoch: 4 \tBatch: 3 \tLoss: 1.4688117504119873\n",
      "Epoch: 4 \tBatch: 4 \tLoss: 1.4986176490783691\n",
      "Epoch: 4 \tBatch: 5 \tLoss: 1.4744657278060913\n",
      "Epoch: 4 \tBatch: 6 \tLoss: 1.492527961730957\n",
      "Epoch: 4 \tBatch: 7 \tLoss: 1.4717237949371338\n",
      "Epoch: 4 \tBatch: 8 \tLoss: 1.4618865251541138\n",
      "Epoch: 4 \tBatch: 9 \tLoss: 1.4701764583587646\n",
      "Epoch: 4 \tBatch: 10 \tLoss: 1.468703031539917\n",
      "Epoch: 4 \tBatch: 11 \tLoss: 1.481860876083374\n",
      "Epoch: 4 \tBatch: 12 \tLoss: 1.4787808656692505\n",
      "Epoch: 4 \tBatch: 13 \tLoss: 1.4791736602783203\n",
      "Epoch: 4 \tBatch: 14 \tLoss: 1.4716593027114868\n",
      "Epoch: 4 \tBatch: 15 \tLoss: 1.4948391914367676\n",
      "Epoch: 4 \tBatch: 16 \tLoss: 1.4702802896499634\n",
      "Epoch: 4 \tBatch: 17 \tLoss: 1.4978269338607788\n",
      "Epoch: 4 \tBatch: 18 \tLoss: 1.4637540578842163\n",
      "Epoch: 4 \tBatch: 19 \tLoss: 1.4837558269500732\n",
      "Epoch: 4 \tBatch: 20 \tLoss: 1.4975569248199463\n",
      "Epoch: 4 \tBatch: 21 \tLoss: 1.4717298746109009\n",
      "Epoch: 4 \tBatch: 22 \tLoss: 1.4732402563095093\n",
      "Epoch: 4 \tBatch: 23 \tLoss: 1.4724656343460083\n",
      "Epoch: 4 \tBatch: 24 \tLoss: 1.4894033670425415\n",
      "Epoch: 4 \tBatch: 25 \tLoss: 1.4850527048110962\n",
      "Epoch: 4 \tBatch: 26 \tLoss: 1.5072505474090576\n",
      "Epoch: 4 \tBatch: 27 \tLoss: 1.4832046031951904\n",
      "Epoch: 4 \tBatch: 28 \tLoss: 1.486878514289856\n",
      "Epoch: 4 \tBatch: 29 \tLoss: 1.4779049158096313\n",
      "Epoch: 4 \tBatch: 30 \tLoss: 1.4692438840866089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 31 \tLoss: 1.4971518516540527\n",
      "Epoch: 4 \tBatch: 32 \tLoss: 1.480244755744934\n",
      "Epoch: 4 \tBatch: 33 \tLoss: 1.4825767278671265\n",
      "Epoch: 4 \tBatch: 34 \tLoss: 1.4920202493667603\n",
      "Epoch: 4 \tBatch: 35 \tLoss: 1.4645146131515503\n",
      "Epoch: 4 \tBatch: 36 \tLoss: 1.4802498817443848\n",
      "Epoch: 4 \tBatch: 37 \tLoss: 1.4871838092803955\n",
      "Epoch: 4 \tBatch: 38 \tLoss: 1.4924182891845703\n",
      "Epoch: 4 \tBatch: 39 \tLoss: 1.4777690172195435\n",
      "Epoch: 4 \tBatch: 40 \tLoss: 1.4911178350448608\n",
      "Epoch: 4 \tBatch: 41 \tLoss: 1.4956393241882324\n",
      "Epoch: 4 \tBatch: 42 \tLoss: 1.4814172983169556\n",
      "Epoch: 4 \tBatch: 43 \tLoss: 1.4740039110183716\n",
      "Epoch: 4 \tBatch: 44 \tLoss: 1.4641828536987305\n",
      "Epoch: 4 \tBatch: 45 \tLoss: 1.4729701280593872\n",
      "Epoch: 4 \tBatch: 46 \tLoss: 1.4614589214324951\n",
      "Epoch: 4 \tBatch: 47 \tLoss: 1.4794505834579468\n",
      "Epoch: 4 \tBatch: 48 \tLoss: 1.4770183563232422\n",
      "Epoch: 4 \tBatch: 49 \tLoss: 1.471062183380127\n",
      "Epoch: 4 \tBatch: 50 \tLoss: 1.4740616083145142\n",
      "Epoch: 4 \tBatch: 51 \tLoss: 1.4640898704528809\n",
      "Epoch: 4 \tBatch: 52 \tLoss: 1.4645088911056519\n",
      "Epoch: 4 \tBatch: 53 \tLoss: 1.4866409301757812\n",
      "Epoch: 4 \tBatch: 54 \tLoss: 1.4703409671783447\n",
      "Epoch: 4 \tBatch: 55 \tLoss: 1.473083257675171\n",
      "Epoch: 4 \tBatch: 56 \tLoss: 1.4806348085403442\n",
      "Epoch: 4 \tBatch: 57 \tLoss: 1.4931814670562744\n",
      "Epoch: 4 \tBatch: 58 \tLoss: 1.4767338037490845\n",
      "Epoch: 4 \tBatch: 59 \tLoss: 1.474756121635437\n",
      "Epoch: 4 \tBatch: 60 \tLoss: 1.471204400062561\n",
      "Epoch: 4 \tBatch: 61 \tLoss: 1.47640061378479\n",
      "Epoch: 4 \tBatch: 62 \tLoss: 1.4644031524658203\n",
      "Epoch: 4 \tBatch: 63 \tLoss: 1.4693381786346436\n",
      "Epoch: 4 \tBatch: 64 \tLoss: 1.4803588390350342\n",
      "Epoch: 4 \tBatch: 65 \tLoss: 1.4707059860229492\n",
      "Epoch: 4 \tBatch: 66 \tLoss: 1.466964840888977\n",
      "Epoch: 4 \tBatch: 67 \tLoss: 1.474690556526184\n",
      "Epoch: 4 \tBatch: 68 \tLoss: 1.4783296585083008\n",
      "Epoch: 4 \tBatch: 69 \tLoss: 1.4699697494506836\n",
      "Epoch: 4 \tBatch: 70 \tLoss: 1.4776039123535156\n",
      "Epoch: 4 \tBatch: 71 \tLoss: 1.4735485315322876\n",
      "Epoch: 4 \tBatch: 72 \tLoss: 1.487352967262268\n",
      "Epoch: 4 \tBatch: 73 \tLoss: 1.4832600355148315\n",
      "Epoch: 4 \tBatch: 74 \tLoss: 1.4709198474884033\n",
      "Epoch: 4 \tBatch: 75 \tLoss: 1.4692773818969727\n",
      "Epoch: 4 \tBatch: 76 \tLoss: 1.4788618087768555\n",
      "Epoch: 4 \tBatch: 77 \tLoss: 1.472387433052063\n",
      "Epoch: 4 \tBatch: 78 \tLoss: 1.4952878952026367\n",
      "Epoch: 4 \tBatch: 79 \tLoss: 1.4728517532348633\n",
      "Epoch: 4 \tBatch: 80 \tLoss: 1.472652792930603\n",
      "Epoch: 4 \tBatch: 81 \tLoss: 1.4773763418197632\n",
      "Epoch: 4 \tBatch: 82 \tLoss: 1.469954490661621\n",
      "Epoch: 4 \tBatch: 83 \tLoss: 1.4693105220794678\n",
      "Epoch: 4 \tBatch: 84 \tLoss: 1.4932451248168945\n",
      "Epoch: 4 \tBatch: 85 \tLoss: 1.482051134109497\n",
      "Epoch: 4 \tBatch: 86 \tLoss: 1.4719196557998657\n",
      "Epoch: 4 \tBatch: 87 \tLoss: 1.493032455444336\n",
      "Epoch: 4 \tBatch: 88 \tLoss: 1.473395586013794\n",
      "Epoch: 4 \tBatch: 89 \tLoss: 1.4748977422714233\n",
      "Epoch: 4 \tBatch: 90 \tLoss: 1.4850811958312988\n",
      "Epoch: 4 \tBatch: 91 \tLoss: 1.4843512773513794\n",
      "Epoch: 4 \tBatch: 92 \tLoss: 1.487765908241272\n",
      "Epoch: 4 \tBatch: 93 \tLoss: 1.4810130596160889\n",
      "Epoch: 4 \tBatch: 94 \tLoss: 1.4700548648834229\n",
      "Epoch: 4 \tBatch: 95 \tLoss: 1.478894591331482\n",
      "Epoch: 4 \tBatch: 96 \tLoss: 1.466374158859253\n",
      "Epoch: 4 \tBatch: 97 \tLoss: 1.4630980491638184\n",
      "Epoch: 4 \tBatch: 98 \tLoss: 1.471781611442566\n",
      "Epoch: 4 \tBatch: 99 \tLoss: 1.4629217386245728\n",
      "Epoch: 4 \tBatch: 100 \tLoss: 1.4876083135604858\n",
      "Epoch: 4 \tBatch: 101 \tLoss: 1.4931526184082031\n",
      "Epoch: 4 \tBatch: 102 \tLoss: 1.4829387664794922\n",
      "Epoch: 4 \tBatch: 103 \tLoss: 1.474144458770752\n",
      "Epoch: 4 \tBatch: 104 \tLoss: 1.4778597354888916\n",
      "Epoch: 4 \tBatch: 105 \tLoss: 1.476302981376648\n",
      "Epoch: 4 \tBatch: 106 \tLoss: 1.4793245792388916\n",
      "Epoch: 4 \tBatch: 107 \tLoss: 1.4705519676208496\n",
      "Epoch: 4 \tBatch: 108 \tLoss: 1.4883410930633545\n",
      "Epoch: 4 \tBatch: 109 \tLoss: 1.4912710189819336\n",
      "Epoch: 4 \tBatch: 110 \tLoss: 1.4646624326705933\n",
      "Epoch: 4 \tBatch: 111 \tLoss: 1.4893066883087158\n",
      "Epoch: 4 \tBatch: 112 \tLoss: 1.4683992862701416\n",
      "Epoch: 4 \tBatch: 113 \tLoss: 1.4715523719787598\n",
      "Epoch: 4 \tBatch: 114 \tLoss: 1.473388671875\n",
      "Epoch: 4 \tBatch: 115 \tLoss: 1.4690481424331665\n",
      "Epoch: 4 \tBatch: 116 \tLoss: 1.4836171865463257\n",
      "Epoch: 4 \tBatch: 117 \tLoss: 1.4758378267288208\n",
      "Epoch: 4 \tBatch: 118 \tLoss: 1.4795446395874023\n",
      "Epoch: 4 \tBatch: 119 \tLoss: 1.4765678644180298\n",
      "Epoch: 4 \tBatch: 120 \tLoss: 1.4718797206878662\n",
      "Epoch: 4 \tBatch: 121 \tLoss: 1.484971284866333\n",
      "Epoch: 4 \tBatch: 122 \tLoss: 1.4729725122451782\n",
      "Epoch: 4 \tBatch: 123 \tLoss: 1.4846949577331543\n",
      "Epoch: 4 \tBatch: 124 \tLoss: 1.4870156049728394\n",
      "Epoch: 4 \tBatch: 125 \tLoss: 1.4894728660583496\n",
      "Epoch: 4 \tBatch: 126 \tLoss: 1.4633359909057617\n",
      "Epoch: 4 \tBatch: 127 \tLoss: 1.4701708555221558\n",
      "Epoch: 4 \tBatch: 128 \tLoss: 1.4943526983261108\n",
      "Epoch: 4 \tBatch: 129 \tLoss: 1.4886056184768677\n",
      "Epoch: 4 \tBatch: 130 \tLoss: 1.5016826391220093\n",
      "Epoch: 4 \tBatch: 131 \tLoss: 1.4744325876235962\n",
      "Epoch: 4 \tBatch: 132 \tLoss: 1.475979208946228\n",
      "Epoch: 4 \tBatch: 133 \tLoss: 1.4850456714630127\n",
      "Epoch: 4 \tBatch: 134 \tLoss: 1.468345046043396\n",
      "Epoch: 4 \tBatch: 135 \tLoss: 1.493346095085144\n",
      "Epoch: 4 \tBatch: 136 \tLoss: 1.4694464206695557\n",
      "Epoch: 4 \tBatch: 137 \tLoss: 1.4763996601104736\n",
      "Epoch: 4 \tBatch: 138 \tLoss: 1.4681307077407837\n",
      "Epoch: 4 \tBatch: 139 \tLoss: 1.4804646968841553\n",
      "Epoch: 4 \tBatch: 140 \tLoss: 1.486935019493103\n",
      "Epoch: 4 \tBatch: 141 \tLoss: 1.4651505947113037\n",
      "Epoch: 4 \tBatch: 142 \tLoss: 1.4768307209014893\n",
      "Epoch: 4 \tBatch: 143 \tLoss: 1.4765909910202026\n",
      "Epoch: 4 \tBatch: 144 \tLoss: 1.499593734741211\n",
      "Epoch: 4 \tBatch: 145 \tLoss: 1.463361382484436\n",
      "Epoch: 4 \tBatch: 146 \tLoss: 1.4652862548828125\n",
      "Epoch: 4 \tBatch: 147 \tLoss: 1.4999499320983887\n",
      "Epoch: 4 \tBatch: 148 \tLoss: 1.4845107793807983\n",
      "Epoch: 4 \tBatch: 149 \tLoss: 1.4653640985488892\n",
      "Epoch: 4 \tBatch: 150 \tLoss: 1.46498441696167\n",
      "Epoch: 4 \tBatch: 151 \tLoss: 1.4631803035736084\n",
      "Epoch: 4 \tBatch: 152 \tLoss: 1.5004546642303467\n",
      "Epoch: 4 \tBatch: 153 \tLoss: 1.4813350439071655\n",
      "Epoch: 4 \tBatch: 154 \tLoss: 1.4693747758865356\n",
      "Epoch: 4 \tBatch: 155 \tLoss: 1.4696805477142334\n",
      "Epoch: 4 \tBatch: 156 \tLoss: 1.4689120054244995\n",
      "Epoch: 4 \tBatch: 157 \tLoss: 1.4838660955429077\n",
      "Epoch: 4 \tBatch: 158 \tLoss: 1.4840351343154907\n",
      "Epoch: 4 \tBatch: 159 \tLoss: 1.4767152070999146\n",
      "Epoch: 4 \tBatch: 160 \tLoss: 1.4692959785461426\n",
      "Epoch: 4 \tBatch: 161 \tLoss: 1.4686030149459839\n",
      "Epoch: 4 \tBatch: 162 \tLoss: 1.493727207183838\n",
      "Epoch: 4 \tBatch: 163 \tLoss: 1.4736518859863281\n",
      "Epoch: 4 \tBatch: 164 \tLoss: 1.491931438446045\n",
      "Epoch: 4 \tBatch: 165 \tLoss: 1.5024245977401733\n",
      "Epoch: 4 \tBatch: 166 \tLoss: 1.4807831048965454\n",
      "Epoch: 4 \tBatch: 167 \tLoss: 1.4716511964797974\n",
      "Epoch: 4 \tBatch: 168 \tLoss: 1.493290901184082\n",
      "Epoch: 4 \tBatch: 169 \tLoss: 1.5038864612579346\n",
      "Epoch: 4 \tBatch: 170 \tLoss: 1.4818875789642334\n",
      "Epoch: 4 \tBatch: 171 \tLoss: 1.472675085067749\n",
      "Epoch: 4 \tBatch: 172 \tLoss: 1.4883712530136108\n",
      "Epoch: 4 \tBatch: 173 \tLoss: 1.489874005317688\n",
      "Epoch: 4 \tBatch: 174 \tLoss: 1.4953371286392212\n",
      "Epoch: 4 \tBatch: 175 \tLoss: 1.4612040519714355\n",
      "Epoch: 4 \tBatch: 176 \tLoss: 1.490582823753357\n",
      "Epoch: 4 \tBatch: 177 \tLoss: 1.4765530824661255\n",
      "Epoch: 4 \tBatch: 178 \tLoss: 1.485308289527893\n",
      "Epoch: 4 \tBatch: 179 \tLoss: 1.477187156677246\n",
      "Epoch: 4 \tBatch: 180 \tLoss: 1.4631246328353882\n",
      "Epoch: 4 \tBatch: 181 \tLoss: 1.4914720058441162\n",
      "Epoch: 4 \tBatch: 182 \tLoss: 1.4773253202438354\n",
      "Epoch: 4 \tBatch: 183 \tLoss: 1.4777573347091675\n",
      "Epoch: 4 \tBatch: 184 \tLoss: 1.4693068265914917\n",
      "Epoch: 4 \tBatch: 185 \tLoss: 1.4656877517700195\n",
      "Epoch: 4 \tBatch: 186 \tLoss: 1.4913846254348755\n",
      "Epoch: 4 \tBatch: 187 \tLoss: 1.4732297658920288\n",
      "Epoch: 4 \tBatch: 188 \tLoss: 1.4723495244979858\n",
      "Epoch: 4 \tBatch: 189 \tLoss: 1.4777840375900269\n",
      "Epoch: 4 \tBatch: 190 \tLoss: 1.463834524154663\n",
      "Epoch: 4 \tBatch: 191 \tLoss: 1.4616262912750244\n",
      "Epoch: 4 \tBatch: 192 \tLoss: 1.481554627418518\n",
      "Epoch: 4 \tBatch: 193 \tLoss: 1.4923176765441895\n",
      "Epoch: 4 \tBatch: 194 \tLoss: 1.4612590074539185\n",
      "Epoch: 4 \tBatch: 195 \tLoss: 1.4706145524978638\n",
      "Epoch: 4 \tBatch: 196 \tLoss: 1.4772313833236694\n",
      "Epoch: 4 \tBatch: 197 \tLoss: 1.472474455833435\n",
      "Epoch: 4 \tBatch: 198 \tLoss: 1.472557783126831\n",
      "Epoch: 4 \tBatch: 199 \tLoss: 1.482805848121643\n",
      "Epoch: 4 \tBatch: 200 \tLoss: 1.4684391021728516\n",
      "Epoch: 4 \tBatch: 201 \tLoss: 1.477306604385376\n",
      "Epoch: 4 \tBatch: 202 \tLoss: 1.4694091081619263\n",
      "Epoch: 4 \tBatch: 203 \tLoss: 1.4837844371795654\n",
      "Epoch: 4 \tBatch: 204 \tLoss: 1.4888907670974731\n",
      "Epoch: 4 \tBatch: 205 \tLoss: 1.467695713043213\n",
      "Epoch: 4 \tBatch: 206 \tLoss: 1.4784343242645264\n",
      "Epoch: 4 \tBatch: 207 \tLoss: 1.487701654434204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 208 \tLoss: 1.4676910638809204\n",
      "Epoch: 4 \tBatch: 209 \tLoss: 1.469804286956787\n",
      "Epoch: 4 \tBatch: 210 \tLoss: 1.4680519104003906\n",
      "Epoch: 4 \tBatch: 211 \tLoss: 1.4852250814437866\n",
      "Epoch: 4 \tBatch: 212 \tLoss: 1.4690947532653809\n",
      "Epoch: 4 \tBatch: 213 \tLoss: 1.4693022966384888\n",
      "Epoch: 4 \tBatch: 214 \tLoss: 1.4769607782363892\n",
      "Epoch: 4 \tBatch: 215 \tLoss: 1.4892330169677734\n",
      "Epoch: 4 \tBatch: 216 \tLoss: 1.4682893753051758\n",
      "Epoch: 4 \tBatch: 217 \tLoss: 1.4774184226989746\n",
      "Epoch: 4 \tBatch: 218 \tLoss: 1.4711676836013794\n",
      "Epoch: 4 \tBatch: 219 \tLoss: 1.476205587387085\n",
      "Epoch: 4 \tBatch: 220 \tLoss: 1.4791141748428345\n",
      "Epoch: 4 \tBatch: 221 \tLoss: 1.4823988676071167\n",
      "Epoch: 4 \tBatch: 222 \tLoss: 1.469348430633545\n",
      "Epoch: 4 \tBatch: 223 \tLoss: 1.4819896221160889\n",
      "Epoch: 4 \tBatch: 224 \tLoss: 1.4835714101791382\n",
      "Epoch: 4 \tBatch: 225 \tLoss: 1.466317892074585\n",
      "Epoch: 4 \tBatch: 226 \tLoss: 1.4758994579315186\n",
      "Epoch: 4 \tBatch: 227 \tLoss: 1.4865220785140991\n",
      "Epoch: 4 \tBatch: 228 \tLoss: 1.4702955484390259\n",
      "Epoch: 4 \tBatch: 229 \tLoss: 1.495287299156189\n",
      "Epoch: 4 \tBatch: 230 \tLoss: 1.4899861812591553\n",
      "Epoch: 4 \tBatch: 231 \tLoss: 1.4640599489212036\n",
      "Epoch: 4 \tBatch: 232 \tLoss: 1.4768532514572144\n",
      "Epoch: 4 \tBatch: 233 \tLoss: 1.4798072576522827\n",
      "Epoch: 4 \tBatch: 234 \tLoss: 1.4762922525405884\n",
      "Epoch: 4 \tBatch: 235 \tLoss: 1.4862103462219238\n",
      "Epoch: 4 \tBatch: 236 \tLoss: 1.475578784942627\n",
      "Epoch: 4 \tBatch: 237 \tLoss: 1.4714354276657104\n",
      "Epoch: 4 \tBatch: 238 \tLoss: 1.4810086488723755\n",
      "Epoch: 4 \tBatch: 239 \tLoss: 1.4659953117370605\n",
      "Epoch: 4 \tBatch: 240 \tLoss: 1.474231243133545\n",
      "Epoch: 4 \tBatch: 241 \tLoss: 1.4858628511428833\n",
      "Epoch: 4 \tBatch: 242 \tLoss: 1.48921537399292\n",
      "Epoch: 4 \tBatch: 243 \tLoss: 1.4778248071670532\n",
      "Epoch: 4 \tBatch: 244 \tLoss: 1.4635926485061646\n",
      "Epoch: 4 \tBatch: 245 \tLoss: 1.4811800718307495\n",
      "Epoch: 4 \tBatch: 246 \tLoss: 1.4693459272384644\n",
      "Epoch: 4 \tBatch: 247 \tLoss: 1.4782062768936157\n",
      "Epoch: 4 \tBatch: 248 \tLoss: 1.492998480796814\n",
      "Epoch: 4 \tBatch: 249 \tLoss: 1.479549527168274\n",
      "Epoch: 4 \tBatch: 250 \tLoss: 1.4795324802398682\n",
      "Epoch: 4 \tBatch: 251 \tLoss: 1.4649600982666016\n",
      "Epoch: 4 \tBatch: 252 \tLoss: 1.4712166786193848\n",
      "Epoch: 4 \tBatch: 253 \tLoss: 1.4862418174743652\n",
      "Epoch: 4 \tBatch: 254 \tLoss: 1.4856873750686646\n",
      "Epoch: 4 \tBatch: 255 \tLoss: 1.4766463041305542\n",
      "Epoch: 4 \tBatch: 256 \tLoss: 1.4720569849014282\n",
      "Epoch: 4 \tBatch: 257 \tLoss: 1.467748761177063\n",
      "Epoch: 4 \tBatch: 258 \tLoss: 1.4785431623458862\n",
      "Epoch: 4 \tBatch: 259 \tLoss: 1.4846835136413574\n",
      "Epoch: 4 \tBatch: 260 \tLoss: 1.499066948890686\n",
      "Epoch: 4 \tBatch: 261 \tLoss: 1.4724504947662354\n",
      "Epoch: 4 \tBatch: 262 \tLoss: 1.4793899059295654\n",
      "Epoch: 4 \tBatch: 263 \tLoss: 1.4676333665847778\n",
      "Epoch: 4 \tBatch: 264 \tLoss: 1.4947415590286255\n",
      "Epoch: 4 \tBatch: 265 \tLoss: 1.4691320657730103\n",
      "Epoch: 4 \tBatch: 266 \tLoss: 1.4753096103668213\n",
      "Epoch: 4 \tBatch: 267 \tLoss: 1.4801688194274902\n",
      "Epoch: 4 \tBatch: 268 \tLoss: 1.4711428880691528\n",
      "Epoch: 4 \tBatch: 269 \tLoss: 1.4851405620574951\n",
      "Epoch: 4 \tBatch: 270 \tLoss: 1.469935417175293\n",
      "Epoch: 4 \tBatch: 271 \tLoss: 1.4720193147659302\n",
      "Epoch: 4 \tBatch: 272 \tLoss: 1.4750041961669922\n",
      "Epoch: 4 \tBatch: 273 \tLoss: 1.484212040901184\n",
      "Epoch: 4 \tBatch: 274 \tLoss: 1.4786808490753174\n",
      "Epoch: 4 \tBatch: 275 \tLoss: 1.4751774072647095\n",
      "Epoch: 4 \tBatch: 276 \tLoss: 1.4932339191436768\n",
      "Epoch: 4 \tBatch: 277 \tLoss: 1.4643123149871826\n",
      "Epoch: 4 \tBatch: 278 \tLoss: 1.4763445854187012\n",
      "Epoch: 4 \tBatch: 279 \tLoss: 1.4790492057800293\n",
      "Epoch: 4 \tBatch: 280 \tLoss: 1.4862395524978638\n",
      "Epoch: 4 \tBatch: 281 \tLoss: 1.4751286506652832\n",
      "Epoch: 4 \tBatch: 282 \tLoss: 1.463303804397583\n",
      "Epoch: 4 \tBatch: 283 \tLoss: 1.4745023250579834\n",
      "Epoch: 4 \tBatch: 284 \tLoss: 1.4758464097976685\n",
      "Epoch: 4 \tBatch: 285 \tLoss: 1.4643077850341797\n",
      "Epoch: 4 \tBatch: 286 \tLoss: 1.4642572402954102\n",
      "Epoch: 4 \tBatch: 287 \tLoss: 1.4697890281677246\n",
      "Epoch: 4 \tBatch: 288 \tLoss: 1.4720964431762695\n",
      "Epoch: 4 \tBatch: 289 \tLoss: 1.4704145193099976\n",
      "Epoch: 4 \tBatch: 290 \tLoss: 1.4760408401489258\n",
      "Epoch: 4 \tBatch: 291 \tLoss: 1.4834665060043335\n",
      "Epoch: 4 \tBatch: 292 \tLoss: 1.4811105728149414\n",
      "Epoch: 4 \tBatch: 293 \tLoss: 1.4898319244384766\n",
      "Epoch: 4 \tBatch: 294 \tLoss: 1.474879503250122\n",
      "Epoch: 4 \tBatch: 295 \tLoss: 1.484451174736023\n",
      "Epoch: 4 \tBatch: 296 \tLoss: 1.481972098350525\n",
      "Epoch: 4 \tBatch: 297 \tLoss: 1.4726898670196533\n",
      "Epoch: 4 \tBatch: 298 \tLoss: 1.4654457569122314\n",
      "Epoch: 4 \tBatch: 299 \tLoss: 1.4807428121566772\n",
      "Epoch: 4 \tBatch: 300 \tLoss: 1.4714088439941406\n",
      "Epoch: 4 \tBatch: 301 \tLoss: 1.4720301628112793\n",
      "Epoch: 4 \tBatch: 302 \tLoss: 1.4732247591018677\n",
      "Epoch: 4 \tBatch: 303 \tLoss: 1.478980541229248\n",
      "Epoch: 4 \tBatch: 304 \tLoss: 1.4833406209945679\n",
      "Epoch: 4 \tBatch: 305 \tLoss: 1.4710633754730225\n",
      "Epoch: 4 \tBatch: 306 \tLoss: 1.4772530794143677\n",
      "Epoch: 4 \tBatch: 307 \tLoss: 1.4942280054092407\n",
      "Epoch: 4 \tBatch: 308 \tLoss: 1.4656932353973389\n",
      "Epoch: 4 \tBatch: 309 \tLoss: 1.4834717512130737\n",
      "Epoch: 4 \tBatch: 310 \tLoss: 1.4838932752609253\n",
      "Epoch: 4 \tBatch: 311 \tLoss: 1.4761598110198975\n",
      "Epoch: 4 \tBatch: 312 \tLoss: 1.470668911933899\n",
      "Epoch: 4 \tBatch: 313 \tLoss: 1.4968979358673096\n",
      "Epoch: 4 \tBatch: 314 \tLoss: 1.4794632196426392\n",
      "Epoch: 4 \tBatch: 315 \tLoss: 1.4812067747116089\n",
      "Epoch: 4 \tBatch: 316 \tLoss: 1.4844744205474854\n",
      "Epoch: 4 \tBatch: 317 \tLoss: 1.4786794185638428\n",
      "Epoch: 4 \tBatch: 318 \tLoss: 1.4706743955612183\n",
      "Epoch: 4 \tBatch: 319 \tLoss: 1.4802806377410889\n",
      "Epoch: 4 \tBatch: 320 \tLoss: 1.4786659479141235\n",
      "Epoch: 4 \tBatch: 321 \tLoss: 1.462391972541809\n",
      "Epoch: 4 \tBatch: 322 \tLoss: 1.463329553604126\n",
      "Epoch: 4 \tBatch: 323 \tLoss: 1.473761796951294\n",
      "Epoch: 4 \tBatch: 324 \tLoss: 1.4708044528961182\n",
      "Epoch: 4 \tBatch: 325 \tLoss: 1.483911156654358\n",
      "Epoch: 4 \tBatch: 326 \tLoss: 1.4688854217529297\n",
      "Epoch: 4 \tBatch: 327 \tLoss: 1.4833076000213623\n",
      "Epoch: 4 \tBatch: 328 \tLoss: 1.4652074575424194\n",
      "Epoch: 4 \tBatch: 329 \tLoss: 1.482593059539795\n",
      "Epoch: 4 \tBatch: 330 \tLoss: 1.4696322679519653\n",
      "Epoch: 4 \tBatch: 331 \tLoss: 1.48055100440979\n",
      "Epoch: 4 \tBatch: 332 \tLoss: 1.4651318788528442\n",
      "Epoch: 4 \tBatch: 333 \tLoss: 1.5045186281204224\n",
      "Epoch: 4 \tBatch: 334 \tLoss: 1.4768407344818115\n",
      "Epoch: 4 \tBatch: 335 \tLoss: 1.4743620157241821\n",
      "Epoch: 4 \tBatch: 336 \tLoss: 1.4977480173110962\n",
      "Epoch: 4 \tBatch: 337 \tLoss: 1.4856317043304443\n",
      "Epoch: 4 \tBatch: 338 \tLoss: 1.4940003156661987\n",
      "Epoch: 4 \tBatch: 339 \tLoss: 1.477160930633545\n",
      "Epoch: 4 \tBatch: 340 \tLoss: 1.4810715913772583\n",
      "Epoch: 4 \tBatch: 341 \tLoss: 1.486395239830017\n",
      "Epoch: 4 \tBatch: 342 \tLoss: 1.4823598861694336\n",
      "Epoch: 4 \tBatch: 343 \tLoss: 1.47518789768219\n",
      "Epoch: 4 \tBatch: 344 \tLoss: 1.4851380586624146\n",
      "Epoch: 4 \tBatch: 345 \tLoss: 1.480271339416504\n",
      "Epoch: 4 \tBatch: 346 \tLoss: 1.4950231313705444\n",
      "Epoch: 4 \tBatch: 347 \tLoss: 1.4845410585403442\n",
      "Epoch: 4 \tBatch: 348 \tLoss: 1.482966661453247\n",
      "Epoch: 4 \tBatch: 349 \tLoss: 1.4763104915618896\n",
      "Epoch: 4 \tBatch: 350 \tLoss: 1.4800782203674316\n",
      "Epoch: 4 \tBatch: 351 \tLoss: 1.486362099647522\n",
      "Epoch: 4 \tBatch: 352 \tLoss: 1.4695309400558472\n",
      "Epoch: 4 \tBatch: 353 \tLoss: 1.4718427658081055\n",
      "Epoch: 4 \tBatch: 354 \tLoss: 1.4705840349197388\n",
      "Epoch: 4 \tBatch: 355 \tLoss: 1.4747540950775146\n",
      "Epoch: 4 \tBatch: 356 \tLoss: 1.4652303457260132\n",
      "Epoch: 4 \tBatch: 357 \tLoss: 1.4805833101272583\n",
      "Epoch: 4 \tBatch: 358 \tLoss: 1.4695159196853638\n",
      "Epoch: 4 \tBatch: 359 \tLoss: 1.4777177572250366\n",
      "Epoch: 4 \tBatch: 360 \tLoss: 1.4772506952285767\n",
      "Epoch: 4 \tBatch: 361 \tLoss: 1.4885644912719727\n",
      "Epoch: 4 \tBatch: 362 \tLoss: 1.471606969833374\n",
      "Epoch: 4 \tBatch: 363 \tLoss: 1.4763665199279785\n",
      "Epoch: 4 \tBatch: 364 \tLoss: 1.4798569679260254\n",
      "Epoch: 4 \tBatch: 365 \tLoss: 1.4907020330429077\n",
      "Epoch: 4 \tBatch: 366 \tLoss: 1.4776617288589478\n",
      "Epoch: 4 \tBatch: 367 \tLoss: 1.4861984252929688\n",
      "Epoch: 4 \tBatch: 368 \tLoss: 1.4721479415893555\n",
      "Epoch: 4 \tBatch: 369 \tLoss: 1.4826594591140747\n",
      "Epoch: 4 \tBatch: 370 \tLoss: 1.4624576568603516\n",
      "Epoch: 4 \tBatch: 371 \tLoss: 1.4919378757476807\n",
      "Epoch: 4 \tBatch: 372 \tLoss: 1.475497841835022\n",
      "Epoch: 4 \tBatch: 373 \tLoss: 1.4752535820007324\n",
      "Epoch: 4 \tBatch: 374 \tLoss: 1.480958104133606\n",
      "Epoch: 4 \tBatch: 375 \tLoss: 1.4873138666152954\n",
      "Epoch: 4 \tBatch: 376 \tLoss: 1.4900380373001099\n",
      "Epoch: 4 \tBatch: 377 \tLoss: 1.5061681270599365\n",
      "Epoch: 4 \tBatch: 378 \tLoss: 1.4673740863800049\n",
      "Epoch: 4 \tBatch: 379 \tLoss: 1.4726380109786987\n",
      "Epoch: 4 \tBatch: 380 \tLoss: 1.4669290781021118\n",
      "Epoch: 4 \tBatch: 381 \tLoss: 1.4701249599456787\n",
      "Epoch: 4 \tBatch: 382 \tLoss: 1.4774037599563599\n",
      "Epoch: 4 \tBatch: 383 \tLoss: 1.4711600542068481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 384 \tLoss: 1.4767134189605713\n",
      "Epoch: 4 \tBatch: 385 \tLoss: 1.4843350648880005\n",
      "Epoch: 4 \tBatch: 386 \tLoss: 1.4709004163742065\n",
      "Epoch: 4 \tBatch: 387 \tLoss: 1.4697456359863281\n",
      "Epoch: 4 \tBatch: 388 \tLoss: 1.4896326065063477\n",
      "Epoch: 4 \tBatch: 389 \tLoss: 1.4858089685440063\n",
      "Epoch: 4 \tBatch: 390 \tLoss: 1.475142240524292\n",
      "Training Complete. Final loss = 1.475142240524292\n"
     ]
    }
   ],
   "source": [
    "def train(model, epochs, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())\n",
    "    \n",
    "train(cnn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.536\n",
      "Validation Accuracy: 98.22\n",
      "Test Accuracy: 98.15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct\n",
    "\n",
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's done\n",
    "You should now understand\n",
    "- the advantages of using CNNs vs vanilla neural networks\n",
    "- how an image is represented as data, including its channels\n",
    "- what convolution is in the context of machine learning\n",
    "- the new convolutional and pooling layers that we have used in this notebook\n",
    "\n",
    "## Next steps\n",
    "- [Custom Datasets](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Custom%20Datasets.ipynb)\n",
    "\n",
    "## Appendix\n",
    "- [Empirical Benchmarking of Fully Connected vs Convolutional Architecture on MNIST](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Empirical%20Benchmarking%20of%20Fully%20Connected%20vs%20Convolutional%20Architecture%20on%20MNIST.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
